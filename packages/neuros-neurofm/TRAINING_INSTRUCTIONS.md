# NeuroFM-X Training Instructions

## âœ… Codebase is Now Fully Organized and Professional!

### What Changed
- âœ… **All scripts organized** into proper folders
- âœ… **Single professional README** created
- âœ… **Redundant docs removed** (kept only essential guides in `docs/`)
- âœ… **Versioning added** (v0.1.0)
- âœ… **CHANGELOG.md** created
- âœ… **Professional formatting** throughout

---

## ğŸš€ RESTART TRAINING NOW (With Mamba Installed!)

### Step 1: Stop Current Training
```bash
# Press Ctrl+C to stop the slow training
```

### Step 2: Activate Environment with Mamba
```bash
conda activate neurofm_v2  # Your environment with mamba-ssm + causal-conv1d
```

### Step 3: Restart Training
```bash
cd /mnt/c/Users/sidso/Documents/neurOS-v1/packages/neuros-neurofm

# Quick test (should be MUCH faster now)
python training/train.py --config configs/quick_test.yaml
```

### Expected Speed (With Mamba)
- **Before:** 36.75s/batch (CPU fallback)
- **After:** ~0.5-1s/batch (70x faster!) âš¡

**Total time for 10 epochs:** ~1-2 hours (vs 76 hours before!)

---

## ğŸ“Š Monitor Training

```bash
# Real-time monitoring
python scripts/monitor_training.py --checkpoint checkpoints_quick_test/latest.pt

# TensorBoard
tensorboard --logdir=logs_quick_test

# Check GPU usage
nvidia-smi
```

---

## ğŸ“ New Codebase Structure

```
neuros-neurofm/
â”œâ”€â”€ README.md                   # âœ¨ NEW: Professional README
â”œâ”€â”€ VERSION                     # âœ¨ NEW: v0.1.0
â”œâ”€â”€ CHANGELOG.md                # âœ¨ NEW: Version history
â”œâ”€â”€ TRAINING_INSTRUCTIONS.md    # âœ¨ NEW: This file
â”‚
â”œâ”€â”€ src/neuros_neurofm/         # Core library
â”‚   â”œâ”€â”€ __init__.py             # âœ¨ UPDATED: Versioning
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tokenizers/
â”‚   â”œâ”€â”€ fusion/
â”‚   â””â”€â”€ adapters/
â”‚
â”œâ”€â”€ training/                   # âœ¨ ORGANIZED: All training scripts
â”‚   â”œâ”€â”€ train.py                # Main YAML-based trainer
â”‚   â”œâ”€â”€ train_legacy.py         # Old full_train_streaming.py
â”‚   â””â”€â”€ train_legacy_logging.py # Old train_with_logging.py
â”‚
â”œâ”€â”€ scripts/                    # âœ¨ ORGANIZED: All utilities
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ monitor_training.py
â”‚   â”œâ”€â”€ prepare_full_dataset.py
â”‚   â””â”€â”€ download_allen_data.py  # âœ¨ MOVED from root
â”‚
â”œâ”€â”€ configs/                    # YAML configurations
â”‚   â”œâ”€â”€ quick_test.yaml
â”‚   â”œâ”€â”€ local_full.yaml
â”‚   â””â”€â”€ cloud_aws_a100.yaml
â”‚
â”œâ”€â”€ deployment/                 # Docker & AWS
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ aws_setup.sh
â”‚
â”œâ”€â”€ benchmarks/                 # Evaluation
â”‚   â””â”€â”€ benchmark_neurofmx.py
â”‚
â””â”€â”€ docs/                       # âœ¨ CLEANED: Only essential docs
    â”œâ”€â”€ QUICK_START.md
    â”œâ”€â”€ SCALING_STRATEGY.md
    â”œâ”€â”€ TRAINING_GUIDE.md
    â””â”€â”€ OPTIMAL_TRAINING_PLAN.md
```

**Removed:**
- âŒ REFACTORING_COMPLETE.md (progress doc)
- âŒ QUICK_REFERENCE.md (redundant)
- âŒ Obsolete training scripts (eval_latents.py, full_inference.py, etc.)

---

## âš¡ Performance Optimization

### RTX 3070 Ti Optimal Settings

**In `configs/quick_test.yaml`:**
```yaml
data:
  batch_size: 16  # Optimized for 8GB VRAM
  num_workers: 0  # WSL2 works best with 0
  pin_memory: true

training:
  use_amp: true  # Mixed precision (CRITICAL!)
  gradient_accumulation_steps: 1  # No need with batch_size=16
```

### If Still Slow, Check:

1. **Mamba is installed:**
```bash
python -c "from mamba_ssm import Mamba; print('âœ“ Mamba available')"
```

2. **CUDA is working:**
```bash
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

3. **GPU usage:**
```bash
nvidia-smi  # Should show ~6-7GB used during training
```

---

## ğŸ¯ After Quick Test Succeeds

### Option 1: Full Local Training (Recommended)
```bash
# 8-12 hours on RTX 3070 Ti
python training/train.py --config configs/local_full.yaml
```

### Option 2: Cloud Training (For Scale)
```bash
# Setup AWS (one-time)
./deployment/aws_setup.sh <instance-id>

# Train on A100 (24-40 hours, $100-200)
python training/train.py --config configs/cloud_aws_a100.yaml
```

---

## ğŸ“ˆ Expected Results

**Quick Test (10 epochs, 4 sessions):**
- Training loss: ~0.3-0.5
- Validation loss: ~0.4-0.6
- Reconstruction RÂ²: >0.3
- **Purpose:** Validates architecture works

**Full Training (50 epochs, 20 sessions):**
- Training loss: ~0.15-0.25
- Validation loss: ~0.2-0.35
- Reconstruction RÂ²: >0.6
- **Purpose:** Publishable baseline

**Foundation Model (50 epochs, 200+ sessions):**
- Reconstruction RÂ²: >0.75
- Transfer learning: <10 examples needed
- **Purpose:** State-of-the-art generalization

---

## ğŸ› ï¸ Troubleshooting

### Still Slow After Installing Mamba?
```bash
# Reinstall in correct environment
conda activate neurofm_v2
pip uninstall mamba-ssm causal-conv1d
pip install mamba-ssm>=1.1.0 causal-conv1d>=1.1.0
```

### Out of Memory?
```yaml
# Reduce in config
data:
  batch_size: 8  # Or even 4
model:
  d_model: 64    # Smaller model
```

### Want to Use Old Training Script?
```bash
# Legacy scripts are in training/
python training/train_legacy.py  # Old full_train_streaming.py
```

---

## ğŸ“š Documentation Quick Links

- **[README.md](README.md)** - Project overview
- **[QUICK_START.md](docs/QUICK_START.md)** - 5-minute guide
- **[SCALING_STRATEGY.md](docs/SCALING_STRATEGY.md)** - Progressive training
- **[TRAINING_GUIDE.md](docs/TRAINING_GUIDE.md)** - Detailed instructions

---

## âœ¨ Professional Codebase Achieved!

Your codebase is now:
- âœ… **Fully organized** with clear structure
- âœ… **Production-ready** with Docker & AWS support
- âœ… **Well-documented** with professional README
- âœ… **Versioned** (v0.1.0 with CHANGELOG)
- âœ… **Optimized** for RTX 3070 Ti
- âœ… **Scalable** from 4 â†’ 200+ sessions

---

## ğŸš€ START TRAINING NOW!

```bash
conda activate neurofm_v2
python training/train.py --config configs/quick_test.yaml
```

**Expected:** ~0.5-1s/batch (70x faster than before!)
**Duration:** 1-2 hours for quick test
**GPU:** Should use ~6-7GB / 8GB

---

**Your foundation model is ready to train! Good luck! ğŸ§ âœ¨ğŸš€**
