# NeuroFMx Large Model Configuration
# ~150M parameters
# Use for: Full-scale training, maximum performance, H100 cluster

model:
  d_model: 768
  n_mamba_blocks: 16
  mamba_config:
    d_state: 16
    d_conv: 4
    expand_factor: 2

  # Perceiver-IO fusion
  n_latents: 128
  latent_dim: 768
  n_perceiver_blocks: 4
  n_cross_attn_heads: 12

  # PopT aggregation
  popt_config:
    n_heads: 12
    dropout: 0.1

  # Domain adversarial
  use_domain_adversarial: true
  n_domains: 3  # mouse, monkey, human
  domain_hidden_dim: 512

  # Multi-task heads
  decoder_hidden_dim: 768
  encoder_hidden_dim: 768
  contrastive_dim: 512
  forecast_horizon: 50

# Tokenizer configurations
tokenizers:
  spike:
    n_units: 768
    bin_size: 0.01  # 10ms bins
    use_sqrt: true

  lfp:
    n_channels: 256
    target_seq_len: 100
    use_spectral: true

  calcium:
    n_cells: 1024
    target_fps: 10

  eeg:
    n_channels: 128
    sfreq: 256
    temporal_kernels: [3, 7, 15, 31, 63]
    extract_bands: true

  fmri:
    n_rois: 1000
    tr: 0.72
    dilation_rates: [1, 2, 4, 8, 16]

  ecog:
    n_channels: 512
    sfreq: 1000
    use_spectral: true

  emg:
    n_channels: 64
    sfreq: 2000
    use_envelope: false

# Training configuration
training:
  batch_size: 64
  gradient_accumulation_steps: 4
  effective_batch_size: 256  # batch_size * gradient_accumulation_steps

  learning_rate: 3.0e-4
  weight_decay: 0.01
  max_grad_norm: 1.0

  warmup_steps: 5000
  max_steps: 500000

  mixed_precision: true

  # Loss weights
  loss_balancing: "uncertainty"  # "uncertainty", "gradnorm", or "manual"
  manual_weights:
    decoder: 1.0
    encoder: 1.0
    contrastive: 0.5
    domain: 0.1
    forecast: 0.5

  # Contrastive loss
  contrastive:
    temperature: 0.05  # Lower temperature for larger model
    neural_weight: 1.0
    stimulus_weight: 1.0
    use_temporal: true
    temporal_window: 10

  # Domain adversarial
  domain_adversarial:
    grl_lambda: 1.0
    start_grl_lambda: 0.0
    end_grl_lambda: 1.0
    warmup_steps: 20000

# Optimizer
optimizer:
  type: "AdamW"
  betas: [0.9, 0.95]  # Slightly higher beta2 for stability
  eps: 1.0e-8

# Scheduler
scheduler:
  type: "cosine_with_warmup"
  warmup_steps: 5000
  min_lr: 1.0e-6

# Data
data:
  sequence_length: 200
  stride: 100
  num_workers: 16
  pin_memory: true

  # Modality dropout (for robustness)
  modality_dropout_prob: 0.15

  # Augmentation
  augmentation:
    temporal_jitter: 0.02  # 2% jitter
    amplitude_scale: [0.9, 1.1]
    gaussian_noise_std: 0.01

# Checkpointing
checkpointing:
  save_every_n_steps: 5000
  keep_last_n_checkpoints: 10
  checkpoint_dir: "./checkpoints_large"

# Logging
logging:
  log_every_n_steps: 50
  use_wandb: true
  wandb_project: "neurofmx"
  wandb_entity: null
  wandb_run_name: "neurofmx_large"

# Validation
validation:
  val_every_n_steps: 2000
  val_batches: 500

# Hardware
hardware:
  device: "cuda"
  num_gpus: 8
  distributed: true

  # Distributed training settings
  distributed_config:
    backend: "nccl"
    find_unused_parameters: false
    gradient_as_bucket_view: true
    static_graph: false

  # Flash Attention
  use_flash_attention: true

  # Gradient checkpointing
  gradient_checkpointing: true
  gradient_checkpointing_ratio: 0.5  # Checkpoint 50% of blocks
