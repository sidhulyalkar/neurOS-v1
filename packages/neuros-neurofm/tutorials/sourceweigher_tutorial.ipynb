{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SourceWeigher Tutorial: Domain Adaptation for Neural Data\n",
    "\n",
    "This tutorial demonstrates how to use **SourceWeigher** for domain adaptation when training neural foundation models on multi-subject data.\n",
    "\n",
    "## Problem: Distribution Shift Across Subjects\n",
    "\n",
    "Neural recordings from different subjects (animals, humans, sessions) exhibit **domain shift**:\n",
    "- Different recording setups\n",
    "- Individual variability in neural responses\n",
    "- Different behavioral states\n",
    "\n",
    "SourceWeigher solves this by learning **mixture weights** that optimally combine source domains to match a target domain.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Source Domains**: Multiple datasets (e.g., subjects 1-10)\n",
    "2. **Target Domain**: New dataset to adapt to (e.g., subject 11)\n",
    "3. **Mixture Weights**: Learned weights π ∈ Δ^n (simplex) that combine sources\n",
    "4. **Three-Phase Training**:\n",
    "   - Phase 1: Pretrain on all sources\n",
    "   - Phase 2: Train with learned mixture weights\n",
    "   - Phase 3: Fine-tune on target\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install neuros-sourceweigher\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# SourceWeigher imports\n",
    "from neuros_sourceweigher import SourceWeigher\n",
    "\n",
    "# NeuroFMX imports\n",
    "from neuros_neurofm.models.neurofmx import NeuroFMX\n",
    "from neuros_neurofm.training.neurofmxx_trainer import NeuroFMXXTrainer\n",
    "from neuros_neurofm.training.curriculum import Curriculum, CurriculumConfig\n",
    "from neuros_neurofm.data.multi_subject_loader import MultiSubjectDataLoader\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Multi-Subject Neural Data\n",
    "\n",
    "We'll use the Allen Neuropixels dataset with recordings from multiple sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multi-subject data\n",
    "data_dir = Path('../data/allen_neuropixels/processed_sequences_full')\n",
    "\n",
    "# Get all session files\n",
    "session_files = sorted(data_dir.glob('session_*.npz'))\n",
    "print(f\"Found {len(session_files)} sessions\")\n",
    "\n",
    "# Load sessions\n",
    "sessions = []\n",
    "for session_file in session_files[:10]:  # Use first 10 for demo\n",
    "    data = np.load(session_file)\n",
    "    sessions.append({\n",
    "        'name': session_file.stem,\n",
    "        'spike_trains': torch.tensor(data['spike_trains'], dtype=torch.float32),\n",
    "        'lfp': torch.tensor(data.get('lfp', np.zeros((100, 512))), dtype=torch.float32),\n",
    "        'behavior': torch.tensor(data.get('behavior', np.zeros((100, 16))), dtype=torch.float32),\n",
    "    })\n",
    "\n",
    "print(f\"\\nLoaded {len(sessions)} sessions:\")\n",
    "for i, sess in enumerate(sessions[:3]):\n",
    "    print(f\"  Session {i}: {sess['name']}\")\n",
    "    print(f\"    Spike trains: {sess['spike_trains'].shape}\")\n",
    "    print(f\"    LFP: {sess['lfp'].shape}\")\n",
    "    print(f\"    Behavior: {sess['behavior'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Source and Target Domains\n",
    "\n",
    "We'll use the first 8 sessions as source domains and sessions 9-10 as target domains for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into source and target\n",
    "n_sources = 8\n",
    "source_sessions = sessions[:n_sources]\n",
    "target_sessions = sessions[n_sources:]\n",
    "\n",
    "print(f\"Source sessions: {n_sources}\")\n",
    "print(f\"Target sessions: {len(target_sessions)}\")\n",
    "\n",
    "# Prepare data loaders\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NeuralDataset(Dataset):\n",
    "    def __init__(self, sessions, seq_len=512):\n",
    "        self.sessions = sessions\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return sum(len(s['spike_trains']) for s in self.sessions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Find which session\n",
    "        for session in self.sessions:\n",
    "            if idx < len(session['spike_trains']):\n",
    "                return {\n",
    "                    'spike_trains': session['spike_trains'][idx],\n",
    "                    'lfp': session['lfp'][idx] if idx < len(session['lfp']) else torch.zeros(self.seq_len),\n",
    "                    'behavior': session['behavior'][idx] if idx < len(session['behavior']) else torch.zeros(16),\n",
    "                }\n",
    "            idx -= len(session['spike_trains'])\n",
    "\n",
    "# Create dataloaders\n",
    "source_dataset = NeuralDataset(source_sessions)\n",
    "target_dataset = NeuralDataset(target_sessions)\n",
    "\n",
    "source_loader = DataLoader(source_dataset, batch_size=32, shuffle=True)\n",
    "target_loader = DataLoader(target_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Source: {len(source_dataset)} samples\")\n",
    "print(f\"  Target: {len(target_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Domain Statistics\n",
    "\n",
    "SourceWeigher requires computing summary statistics (moments) for each domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_moments(sessions, max_samples=1000):\n",
    "    \"\"\"\n",
    "    Compute mean and covariance for each session.\n",
    "    \n",
    "    Returns moments as a (n_features,) vector containing:\n",
    "    - Mean of spike trains\n",
    "    - Variance of spike trains\n",
    "    - Mean of LFP power\n",
    "    \"\"\"\n",
    "    all_spikes = []\n",
    "    all_lfp = []\n",
    "    \n",
    "    for session in sessions:\n",
    "        # Sample up to max_samples\n",
    "        n_samples = min(len(session['spike_trains']), max_samples)\n",
    "        indices = np.random.choice(len(session['spike_trains']), n_samples, replace=False)\n",
    "        \n",
    "        spikes = session['spike_trains'][indices]\n",
    "        lfp = session['lfp'][indices] if len(session['lfp']) > 0 else torch.zeros((n_samples, 512))\n",
    "        \n",
    "        all_spikes.append(spikes)\n",
    "        all_lfp.append(lfp)\n",
    "    \n",
    "    # Concatenate\n",
    "    all_spikes = torch.cat(all_spikes, dim=0)\n",
    "    all_lfp = torch.cat(all_lfp, dim=0)\n",
    "    \n",
    "    # Compute moments\n",
    "    moments = np.array([\n",
    "        all_spikes.mean().item(),\n",
    "        all_spikes.std().item(),\n",
    "        all_lfp.mean().item(),\n",
    "        all_lfp.std().item(),\n",
    "        (all_spikes > 0).float().mean().item(),  # Sparsity\n",
    "    ])\n",
    "    \n",
    "    return moments\n",
    "\n",
    "# Compute moments for each source session\n",
    "source_moments = []\n",
    "for i, session in enumerate(source_sessions):\n",
    "    moments = compute_moments([session])\n",
    "    source_moments.append(moments)\n",
    "    print(f\"Source {i} moments: {moments}\")\n",
    "\n",
    "# Compute moments for target\n",
    "target_moments = compute_moments(target_sessions)\n",
    "print(f\"\\nTarget moments: {target_moments}\")\n",
    "\n",
    "# Stack source moments\n",
    "source_moments = np.array(source_moments)  # (n_sources, n_features)\n",
    "print(f\"\\nSource moments shape: {source_moments.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimate Mixture Weights with SourceWeigher\n",
    "\n",
    "Now we use SourceWeigher to find optimal mixture weights that combine the sources to match the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SourceWeigher\n",
    "weigher = SourceWeigher()\n",
    "\n",
    "# Estimate weights\n",
    "mixture_weights = weigher.estimate_weights(\n",
    "    source_moments=source_moments.T,  # (n_features, n_sources)\n",
    "    target_moments=target_moments,    # (n_features,)\n",
    ")\n",
    "\n",
    "print(f\"\\nEstimated mixture weights:\")\n",
    "for i, weight in enumerate(mixture_weights):\n",
    "    print(f\"  Source {i}: {weight:.4f}\")\n",
    "\n",
    "print(f\"\\nSum of weights: {mixture_weights.sum():.4f} (should be ~1.0)\")\n",
    "\n",
    "# Visualize weights\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(len(mixture_weights)), mixture_weights)\n",
    "plt.xlabel('Source Session')\n",
    "plt.ylabel('Mixture Weight')\n",
    "plt.title('SourceWeigher: Learned Domain Weights')\n",
    "plt.axhline(y=1/len(mixture_weights), color='r', linestyle='--', label='Uniform')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  Highest weight: Source {mixture_weights.argmax()} ({mixture_weights.max():.4f})\")\n",
    "print(f\"  This source is most similar to the target domain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize NeuroFMX Model\n",
    "\n",
    "Create the foundation model that will be trained with domain adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_config = {\n",
    "    'n_neurons': 256,\n",
    "    'd_model': 512,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 6,\n",
    "    'd_ff': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'max_seq_len': 512,\n",
    "}\n",
    "\n",
    "# Create model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = NeuroFMX(**model_config).to(device)\n",
    "\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Three-Phase Training with Domain Adaptation\n",
    "\n",
    "### Phase 1: Pretrain on All Sources (Uniform Weights)\n",
    "\n",
    "Train on all source domains with equal weighting to learn general features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure curriculum\n",
    "curriculum_config = CurriculumConfig(\n",
    "    num_pretrain_epochs=10,\n",
    "    num_weighted_epochs=20,\n",
    "    num_target_epochs=5,\n",
    "    warmup_steps=1000,\n",
    "    learning_rate=1e-4,\n",
    ")\n",
    "\n",
    "curriculum = Curriculum(\n",
    "    num_pretrain_epochs=curriculum_config.num_pretrain_epochs,\n",
    "    num_weighted_epochs=curriculum_config.num_weighted_epochs,\n",
    "    num_target_epochs=curriculum_config.num_target_epochs,\n",
    ")\n",
    "\n",
    "print(f\"Curriculum:\")\n",
    "print(f\"  Phase 1 (Pretrain): {curriculum_config.num_pretrain_epochs} epochs\")\n",
    "print(f\"  Phase 2 (Weighted): {curriculum_config.num_weighted_epochs} epochs\")\n",
    "print(f\"  Phase 3 (Target): {curriculum_config.num_target_epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with SourceWeigher\n",
    "trainer = NeuroFMXXTrainer(\n",
    "    model=model,\n",
    "    source_dataloaders=[source_loader],\n",
    "    target_dataloader=target_loader,\n",
    "    mixture_weights=mixture_weights,\n",
    "    curriculum=curriculum,\n",
    "    device=device,\n",
    "    log_dir='./logs/sourceweigher_tutorial',\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Pretrain\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: PRETRAIN ON ALL SOURCES (UNIFORM WEIGHTS)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "pretrain_losses = trainer.train_phase_1()\n",
    "\n",
    "# Plot pretraining loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(pretrain_losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Phase 1: Pretraining Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Domain-Weighted Training\n",
    "\n",
    "Continue training with learned mixture weights to adapt toward target domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: DOMAIN-WEIGHTED TRAINING\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "weighted_losses = trainer.train_phase_2()\n",
    "\n",
    "# Plot weighted training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(weighted_losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Phase 2: Domain-Weighted Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Target Fine-Tuning\n",
    "\n",
    "Fine-tune exclusively on target domain data for final adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 3: TARGET FINE-TUNING\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "finetune_losses = trainer.train_phase_3()\n",
    "\n",
    "# Plot fine-tuning loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(finetune_losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Phase 3: Target Fine-Tuning Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Domain Adaptation Performance\n",
    "\n",
    "Compare performance with and without domain adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on target domain\n",
    "model.eval()\n",
    "target_loss = 0.0\n",
    "n_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in target_loader:\n",
    "        spike_trains = batch['spike_trains'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(spike_trains)\n",
    "        loss = nn.MSELoss()(outputs, spike_trains)\n",
    "        \n",
    "        target_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "target_loss /= n_batches\n",
    "\n",
    "print(f\"\\nFinal Performance on Target Domain:\")\n",
    "print(f\"  Loss: {target_loss:.4f}\")\n",
    "print(f\"\\nDomain adaptation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress\n",
    "\n",
    "Plot all three phases together to see the full training trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all losses\n",
    "all_losses = np.concatenate([pretrain_losses, weighted_losses, finetune_losses])\n",
    "\n",
    "# Create phase boundaries\n",
    "phase1_end = len(pretrain_losses)\n",
    "phase2_end = phase1_end + len(weighted_losses)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Full training curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_losses, linewidth=2)\n",
    "plt.axvline(x=phase1_end, color='r', linestyle='--', alpha=0.5, label='Phase 1→2')\n",
    "plt.axvline(x=phase2_end, color='r', linestyle='--', alpha=0.5, label='Phase 2→3')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Three-Phase Training with SourceWeigher')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Phase comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "phase_means = [\n",
    "    np.mean(pretrain_losses[-100:]),\n",
    "    np.mean(weighted_losses[-100:]),\n",
    "    np.mean(finetune_losses[-100:]),\n",
    "]\n",
    "plt.bar(['Phase 1\\n(Pretrain)', 'Phase 2\\n(Weighted)', 'Phase 3\\n(Fine-tune)'], phase_means)\n",
    "plt.ylabel('Average Loss (last 100 steps)')\n",
    "plt.title('Performance by Training Phase')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPhase Performance (last 100 steps):\")\n",
    "print(f\"  Phase 1 (Pretrain): {phase_means[0]:.4f}\")\n",
    "print(f\"  Phase 2 (Weighted): {phase_means[1]:.4f}\")\n",
    "print(f\"  Phase 3 (Fine-tune): {phase_means[2]:.4f}\")\n",
    "print(f\"\\nImprovement: {(phase_means[0] - phase_means[2]) / phase_means[0] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare with Baseline (No Domain Adaptation)\n",
    "\n",
    "Train a baseline model without domain adaptation to quantify the benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE: TRAINING WITHOUT DOMAIN ADAPTATION\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Create baseline model\n",
    "baseline_model = NeuroFMX(**model_config).to(device)\n",
    "baseline_optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train on all sources with uniform weights\n",
    "baseline_losses = []\n",
    "baseline_model.train()\n",
    "\n",
    "for epoch in range(10):  # Same total epochs as Phase 1\n",
    "    for batch in source_loader:\n",
    "        spike_trains = batch['spike_trains'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = baseline_model(spike_trains)\n",
    "        loss = nn.MSELoss()(outputs, spike_trains)\n",
    "        \n",
    "        # Backward\n",
    "        baseline_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        baseline_optimizer.step()\n",
    "        \n",
    "        baseline_losses.append(loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/10, Loss: {np.mean(baseline_losses[-100:]):.4f}\")\n",
    "\n",
    "# Evaluate baseline on target\n",
    "baseline_model.eval()\n",
    "baseline_target_loss = 0.0\n",
    "n_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in target_loader:\n",
    "        spike_trains = batch['spike_trains'].to(device)\n",
    "        outputs = baseline_model(spike_trains)\n",
    "        loss = nn.MSELoss()(outputs, spike_trains)\n",
    "        baseline_target_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "baseline_target_loss /= n_batches\n",
    "\n",
    "print(f\"\\nBaseline Performance on Target: {baseline_target_loss:.4f}\")\n",
    "print(f\"SourceWeigher Performance on Target: {target_loss:.4f}\")\n",
    "print(f\"\\nImprovement: {(baseline_target_loss - target_loss) / baseline_target_loss * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Model and Weights\n",
    "\n",
    "Save the adapted model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create checkpoint directory\n",
    "checkpoint_dir = Path('./checkpoints/sourceweigher_tutorial')\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config,\n",
    "    'mixture_weights': mixture_weights,\n",
    "    'curriculum_config': curriculum_config,\n",
    "    'final_target_loss': target_loss,\n",
    "}, checkpoint_dir / 'neurofmx_sourceweigher.pt')\n",
    "\n",
    "print(f\"Model saved to {checkpoint_dir / 'neurofmx_sourceweigher.pt'}\")\n",
    "\n",
    "# Save mixture weights separately\n",
    "np.save(checkpoint_dir / 'mixture_weights.npy', mixture_weights)\n",
    "print(f\"Mixture weights saved to {checkpoint_dir / 'mixture_weights.npy'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **SourceWeigher** automatically learns mixture weights from domain statistics\n",
    "2. **Three-phase training** progressively adapts from general to domain-specific:\n",
    "   - Pretrain: Learn general features\n",
    "   - Weighted: Adapt toward target\n",
    "   - Fine-tune: Specialize on target\n",
    "3. **Domain adaptation** significantly improves performance on target domains\n",
    "4. **No manual hyperparameter tuning** needed for mixture weights\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- Handles distribution shift across subjects/sessions\n",
    "- Automatic weight estimation (no manual tuning)\n",
    "- Theoretically grounded (moment matching)\n",
    "- Works with any neural architecture\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with your own multi-subject datasets\n",
    "- Experiment with different moment statistics\n",
    "- Combine with other regularizers (fractal priors, SAE, etc.)\n",
    "- Use class-conditional weighting (NeuroFMXXXTrainer) for finer control\n",
    "\n",
    "---\n",
    "\n",
    "**Tutorial created with Claude Code**  \n",
    "For more information, see:\n",
    "- [SourceWeigher Documentation](../neuros-sourceweigher/README.md)\n",
    "- [SOURCEWEIGHER_INTEGRATION_PLAN.md](../SOURCEWEIGHER_INTEGRATION_PLAN.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
