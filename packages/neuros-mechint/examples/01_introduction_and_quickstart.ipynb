{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Mechanistic Interpretability\n",
    "\n",
    "**Welcome to neuros-mechint!** This notebook introduces you to the world of mechanistic interpretability—understanding not just *what* neural networks compute, but *how* they compute it.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- What mechanistic interpretability is and why it matters\n",
    "- The core concepts: features, circuits, and interventions\n",
    "- How to run your first interpretability analysis\n",
    "- An overview of all major techniques in the library\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python programming\n",
    "- Familiarity with PyTorch\n",
    "- Understanding of neural networks (MLPs, transformers)\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's make sure everything is installed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library if needed\n",
    "# !pip install neuros-mechint[viz]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available (optional)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Import neuros-mechint\n",
    "import neuros_mechint\n",
    "print(f\"neuros-mechint version: {neuros_mechint.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What is Mechanistic Interpretability?\n",
    "\n",
    "### The Black Box Problem\n",
    "\n",
    "Neural networks are powerful, but we often don't understand *how* they work internally:\n",
    "\n",
    "```\n",
    "Input → [??? Black Box ???] → Output\n",
    "```\n",
    "\n",
    "**Traditional interpretability** asks: \"What features does the model use?\"\n",
    "\n",
    "**Mechanistic interpretability** asks: \"How does the model transform inputs into outputs? What algorithms are implemented in the weights?\"\n",
    "\n",
    "### The Goal: Reading the \"Source Code\"\n",
    "\n",
    "Imagine if we could understand neural networks like this:\n",
    "\n",
    "```python\n",
    "def neural_network(input):\n",
    "    # Layer 1: Detect edges and textures\n",
    "    edges = detect_edges(input)\n",
    "    textures = detect_textures(input)\n",
    "    \n",
    "    # Layer 2: Combine into object parts\n",
    "    parts = combine_features(edges, textures)\n",
    "    \n",
    "    # Layer 3: Recognize complete objects\n",
    "    objects = recognize_objects(parts)\n",
    "    \n",
    "    return classify(objects)\n",
    "```\n",
    "\n",
    "This is the dream of mechanistic interpretability!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**For Neuroscience:**\n",
    "- Compare how artificial and biological networks solve the same problems\n",
    "- Test hypotheses about brain computation\n",
    "- Discover new computational principles\n",
    "\n",
    "**For AI Safety:**\n",
    "- Detect deceptive or dangerous behaviors\n",
    "- Verify alignment with human values\n",
    "- Understand failure modes\n",
    "\n",
    "**For Science:**\n",
    "- Understand emergent phenomena (e.g., in-context learning)\n",
    "- Discover universal computational motifs\n",
    "- Bridge levels of analysis (algorithms ↔ implementation)\n",
    "\n",
    "### Three Core Concepts\n",
    "\n",
    "1. **Features**: Interpretable units of representation\n",
    "   - Problem: Neurons are *polysemantic* (respond to many unrelated things)\n",
    "   - Solution: Sparse autoencoders to find *monosemantic* features\n",
    "\n",
    "2. **Circuits**: Connected groups of features that implement algorithms\n",
    "   - Like subroutines in code\n",
    "   - Example: An \"induction head\" circuit that completes patterns\n",
    "\n",
    "3. **Interventions**: Causally testing what components do\n",
    "   - *Activation patching*: Restore clean activations in corrupted inputs\n",
    "   - *Ablation*: Remove components to see what breaks\n",
    "   - *Steering*: Modify activations to change behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Your First Analysis - Understanding a Simple Transformer\n",
    "\n",
    "Let's analyze a tiny transformer to see mechanistic interpretability in action.\n",
    "\n",
    "### The Model: A Toy Transformer Layer\n",
    "\n",
    "We'll use a simple transformer encoder layer—small enough to run on a laptop, but complex enough to be interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple transformer layer\n",
    "d_model = 64      # Hidden dimension (small for laptop-friendly)\n",
    "nhead = 4         # Number of attention heads\n",
    "batch_size = 8\n",
    "seq_len = 12\n",
    "\n",
    "model = nn.TransformerEncoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Create some example input\n",
    "x = torch.randn(seq_len, batch_size, d_model).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Model input shape: {x.shape}\")\n",
    "print(f\"Model output shape: {output.shape}\")\n",
    "print(f\"\\nModel has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What Does This Transformer Actually Compute?\n",
    "\n",
    "Looking at the weights tells us almost nothing. Let's use mechanistic interpretability to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1: Finding Features with Sparse Autoencoders\n",
    "\n",
    "### The Problem: Polysemanticity\n",
    "\n",
    "Individual neurons often respond to many unrelated concepts. For example, a neuron might activate for:\n",
    "- The word \"apple\" (fruit)\n",
    "- The word \"Apple\" (company) \n",
    "- Red colors\n",
    "- Round shapes\n",
    "\n",
    "This makes neurons hard to interpret!\n",
    "\n",
    "### The Solution: Sparse Autoencoders (SAEs)\n",
    "\n",
    "SAEs learn an *overcomplete dictionary* of features:\n",
    "- More features than neurons (e.g., 4× expansion)\n",
    "- Each feature is *monosemantic* (represents one thing)\n",
    "- Activations are *sparse* (few features active at once)\n",
    "\n",
    "**Mathematical Setup:**\n",
    "\n",
    "Given neuron activations $h \\in \\mathbb{R}^{d}$, learn:\n",
    "- Encoder: $f(h) = \\text{ReLU}(W_e h + b_e) \\in \\mathbb{R}^{m}$ where $m >> d$\n",
    "- Decoder: $\\hat{h} = W_d f(h)$\n",
    "\n",
    "**Loss function:**\n",
    "$$\\mathcal{L} = \\|h - \\hat{h}\\|^2 + \\lambda \\|f(h)\\|_1$$\n",
    "\n",
    "- First term: Reconstruction quality\n",
    "- Second term: Sparsity penalty (encourages few active features)\n",
    "\n",
    "Let's train an SAE on our transformer's activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuros_mechint import SparseAutoencoder\n",
    "\n",
    "# Collect activations from the transformer's MLP layer\n",
    "activations_list = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Hook to capture intermediate activations\"\"\"\n",
    "    activations_list.append(output.detach().cpu())\n",
    "\n",
    "# Register hook on the first linear layer of the MLP\n",
    "hook_handle = model.linear1.register_forward_hook(hook_fn)\n",
    "\n",
    "# Generate more data to train the SAE\n",
    "print(\"Collecting activations...\")\n",
    "for _ in range(50):  # 50 batches\n",
    "    x_batch = torch.randn(seq_len, batch_size, d_model).to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(x_batch)\n",
    "\n",
    "# Remove hook\n",
    "hook_handle.remove()\n",
    "\n",
    "# Combine all activations\n",
    "activations = torch.cat(activations_list, dim=1)  # (seq_len, n_samples, features)\n",
    "activations = activations.reshape(-1, activations.shape[-1])  # Flatten\n",
    "\n",
    "print(f\"Collected {activations.shape[0]} activation vectors of dimension {activations.shape[1]}\")\n",
    "\n",
    "# Create and train SAE\n",
    "sae = SparseAutoencoder(\n",
    "    input_dim=activations.shape[1],\n",
    "    hidden_dim=activations.shape[1] * 4,  # 4x overcomplete\n",
    "    sparsity_coef=0.01,  # L1 penalty strength\n",
    "    tied_weights=False\n",
    ")\n",
    "\n",
    "print(\"\\nTraining SAE...\")\n",
    "losses = sae.train_on_activations(\n",
    "    activations,\n",
    "    num_epochs=100,\n",
    "    batch_size=128,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SAE Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Learned Features\n",
    "\n",
    "Now let's see what features the SAE discovered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature activations for a test batch\n",
    "test_activations = activations[:100]  # Take first 100 samples\n",
    "feature_acts = sae.get_feature_activations(test_activations)\n",
    "\n",
    "print(f\"Feature activation shape: {feature_acts.shape}\")\n",
    "print(f\"Average sparsity: {(feature_acts > 0).float().mean():.2%} (fraction of features active)\")\n",
    "\n",
    "# Visualize sparsity pattern\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Left: Activation heatmap\n",
    "im = axes[0].imshow(feature_acts[:50].T, aspect='auto', cmap='viridis', interpolation='none')\n",
    "axes[0].set_xlabel('Sample')\n",
    "axes[0].set_ylabel('Feature')\n",
    "axes[0].set_title('Feature Activations (50 samples)')\n",
    "plt.colorbar(im, ax=axes[0], label='Activation')\n",
    "\n",
    "# Right: Activation distribution\n",
    "axes[1].hist(feature_acts.flatten(), bins=50, edgecolor='black')\n",
    "axes[1].set_xlabel('Activation Value')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribution of Feature Activations')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most active features\n",
    "feature_frequency = (feature_acts > 0).float().mean(dim=0)\n",
    "top_features = feature_frequency.argsort(descending=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 most frequently active features:\")\n",
    "for i, feat_idx in enumerate(top_features):\n",
    "    freq = feature_frequency[feat_idx]\n",
    "    avg_activation = feature_acts[:, feat_idx][feature_acts[:, feat_idx] > 0].mean()\n",
    "    print(f\"  {i+1}. Feature {feat_idx}: Active {freq:.1%} of the time, avg activation {avg_activation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "Notice how the SAE achieves **sparse** activations—only a small fraction of features are active for any given input. This is exactly what we want! Each feature can now represent a specific concept rather than responding to many unrelated things.\n",
    "\n",
    "In the next notebooks, we'll learn how to:\n",
    "- Interpret what each feature represents\n",
    "- Build hierarchical concept dictionaries\n",
    "- Use features for causal interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2: Causal Circuit Discovery\n",
    "\n",
    "### The Question: Which Components are Causally Important?\n",
    "\n",
    "Just because a component activates doesn't mean it's *causing* the output. We need **causal interventions**.\n",
    "\n",
    "### Activation Patching: The Core Technique\n",
    "\n",
    "**Setup:**\n",
    "1. **Clean run**: Normal input → normal output ✓\n",
    "2. **Corrupted run**: Corrupted input → wrong output ✗\n",
    "3. **Patched run**: Corrupted input, but restore (\"patch\") clean activations at a specific layer\n",
    "\n",
    "**If patching fixes the output**, that component is causally important!\n",
    "\n",
    "**Recovery score**: $R = \\frac{\\text{loss(corrupted)} - \\text{loss(patched)}}{\\text{loss(corrupted)} - \\text{loss(clean)}}$\n",
    "- $R \\approx 1$: Component is critical\n",
    "- $R \\approx 0$: Component doesn't matter\n",
    "\n",
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuros_mechint.interventions import ActivationPatcher\n",
    "\n",
    "# Create clean and corrupted inputs\n",
    "clean_input = torch.randn(seq_len, 1, d_model).to(device)\n",
    "corrupted_input = torch.randn(seq_len, 1, d_model).to(device)  # Different random input\n",
    "\n",
    "# Define a simple loss (e.g., difference from clean output)\n",
    "with torch.no_grad():\n",
    "    clean_output = model(clean_input)\n",
    "\n",
    "def compute_loss(output):\n",
    "    \"\"\"Loss: distance from clean output\"\"\"\n",
    "    return torch.nn.functional.mse_loss(output, clean_output)\n",
    "\n",
    "# Test different components\n",
    "components_to_test = [\n",
    "    'self_attn',  # Attention sublayer\n",
    "    'linear1',    # First MLP layer\n",
    "    'linear2',    # Second MLP layer\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Testing causal importance of components...\\n\")\n",
    "\n",
    "for comp_name in components_to_test:\n",
    "    # Create patcher for this component\n",
    "    patcher = ActivationPatcher(\n",
    "        model=model,\n",
    "        layer_name=comp_name\n",
    "    )\n",
    "    \n",
    "    # Run patching experiment\n",
    "    result = patcher.patch(\n",
    "        clean_input=clean_input,\n",
    "        corrupted_input=corrupted_input,\n",
    "        loss_fn=compute_loss\n",
    "    )\n",
    "    \n",
    "    results[comp_name] = result\n",
    "    \n",
    "    print(f\"{comp_name}:\")\n",
    "    print(f\"  Clean loss: {result['clean_loss']:.4f}\")\n",
    "    print(f\"  Corrupted loss: {result['corrupted_loss']:.4f}\")\n",
    "    print(f\"  Patched loss: {result['patched_loss']:.4f}\")\n",
    "    print(f\"  Recovery score: {result['recovery_score']:.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Component Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract recovery scores\n",
    "component_names = list(results.keys())\n",
    "recovery_scores = [results[name]['recovery_score'] for name in component_names]\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(range(len(component_names)), recovery_scores, edgecolor='black')\n",
    "\n",
    "# Color bars by importance\n",
    "for i, bar in enumerate(bars):\n",
    "    score = recovery_scores[i]\n",
    "    if score > 0.7:\n",
    "        bar.set_color('darkgreen')\n",
    "    elif score > 0.3:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('lightcoral')\n",
    "\n",
    "plt.xticks(range(len(component_names)), component_names, rotation=45)\n",
    "plt.ylabel('Recovery Score')\n",
    "plt.title('Causal Importance of Transformer Components')\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Medium importance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Green: Highly important (recovery > 70%)\")\n",
    "print(\"Orange: Moderately important (30-70%)\")\n",
    "print(\"Red: Less important (< 30%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    "\n",
    "This simple experiment reveals which components are **causally necessary** for the computation:\n",
    "- High recovery score → Component is critical for this task\n",
    "- Low recovery score → Component has minimal causal impact\n",
    "\n",
    "This is more informative than just looking at activation magnitudes!\n",
    "\n",
    "In Notebook 03, we'll learn to:\n",
    "- Trace information flow through entire circuits\n",
    "- Build causal graphs\n",
    "- Discover minimal sufficient subnetworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Overview of All Techniques in the Library\n",
    "\n",
    "You've now seen two core techniques: **Sparse Autoencoders** for finding features and **Activation Patching** for discovering circuits. But there's so much more!\n",
    "\n",
    "### The Complete Toolkit\n",
    "\n",
    "#### 1. Feature Discovery & Analysis\n",
    "- **Sparse Autoencoders** (SAEs): Decompose polysemantic neurons\n",
    "- **Hierarchical SAEs**: Multi-level concept hierarchies\n",
    "- **Feature Attribution**: Integrated Gradients, DeepLIFT, SHAP\n",
    "- **Concept Dictionaries**: Semantic labeling of features\n",
    "\n",
    "#### 2. Causal Interventions\n",
    "- **Activation Patching**: Test component importance\n",
    "- **Ablation Studies**: Systematic component removal\n",
    "- **Path Analysis**: Trace information flow\n",
    "- **Counterfactuals**: What-if analysis\n",
    "\n",
    "#### 3. Circuit Extraction\n",
    "- **Latent RNN Models**: Minimal explanatory circuits\n",
    "- **DUNL**: Demixed sparse coding for mixed selectivity\n",
    "- **Feature Visualization**: What do neurons/features respond to?\n",
    "- **Motif Detection**: Find recurring computational patterns\n",
    "\n",
    "#### 4. Biological Realism\n",
    "- **Fractal Analysis**: Measure scale-free dynamics\n",
    "- **Fractal Regularization**: Enforce biological complexity\n",
    "- **Spiking Neural Networks**: LIF, Izhikevich, Hodgkin-Huxley\n",
    "- **Dale's Law**: Excitatory/inhibitory separation\n",
    "- **Synaptic Plasticity**: STDP, short-term dynamics\n",
    "\n",
    "#### 5. Brain Alignment\n",
    "- **CCA**: Canonical Correlation Analysis for alignment\n",
    "- **RSA**: Representational Similarity Analysis\n",
    "- **PLS**: Partial Least Squares for prediction\n",
    "- **Statistical Testing**: Noise ceilings, significance tests\n",
    "\n",
    "#### 6. Dynamical Systems\n",
    "- **Koopman Operators**: Linearize nonlinear dynamics\n",
    "- **Lyapunov Exponents**: Measure chaos and stability\n",
    "- **Fixed Point Analysis**: Find attractors\n",
    "- **Controllability**: Plan interventions\n",
    "\n",
    "#### 7. Information Theory\n",
    "- **Mutual Information**: MINE estimator for information flow\n",
    "- **Information Plane**: Track compression during training\n",
    "- **Energy Landscapes**: Map basins of attraction\n",
    "- **Entropy Production**: Thermodynamic analysis\n",
    "\n",
    "#### 8. Geometry & Topology\n",
    "- **Manifold Analysis**: Intrinsic dimensionality, curvature\n",
    "- **Persistent Homology**: Topological data analysis\n",
    "- **Geodesic Distances**: True distances on manifolds\n",
    "\n",
    "#### 9. Meta-Dynamics\n",
    "- **Training Trajectories**: How representations evolve\n",
    "- **Phase Detection**: Identify critical periods\n",
    "- **Feature Emergence**: Track when features appear\n",
    "- **Representational Drift**: Measure stability\n",
    "\n",
    "### The Big Picture: How These Techniques Work Together\n",
    "\n",
    "Here's a typical workflow combining multiple techniques:\n",
    "\n",
    "```\n",
    "1. Train your model\n",
    "2. Extract features (SAEs) → Understand WHAT is represented\n",
    "3. Find circuits (Patching) → Understand HOW computation happens\n",
    "4. Analyze dynamics (Koopman, fixed points) → Understand temporal evolution\n",
    "5. Compare to brain (CCA, RSA) → Validate biological relevance\n",
    "6. Generate explanations → Communicate findings\n",
    "```\n",
    "\n",
    "Each technique gives you a different lens to understand the same network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: A Roadmap for Your Learning Journey\n",
    "\n",
    "### Recommended Path\n",
    "\n",
    "**Week 1: Foundations**\n",
    "- ✓ This notebook (01)\n",
    "- Notebook 02: Sparse Autoencoders in depth\n",
    "- Notebook 03: Causal interventions\n",
    "\n",
    "**Week 2: Biological Connection**\n",
    "- Notebook 04: Fractal analysis\n",
    "- Notebook 05: Brain alignment\n",
    "- Notebook 08: Biophysical modeling\n",
    "\n",
    "**Week 3: Advanced Analysis**\n",
    "- Notebook 06: Dynamical systems\n",
    "- Notebook 07: Circuit extraction\n",
    "- Notebook 09: Information theory\n",
    "\n",
    "**Week 4: Expert Topics**\n",
    "- Notebook 10: Advanced topics\n",
    "- Your own research projects!\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "1. **Run the code**: Don't just read—experiment!\n",
    "2. **Modify parameters**: Change hyperparameters and see what happens\n",
    "3. **Use your own models**: Apply these techniques to models you care about\n",
    "4. **Connect to papers**: Read the cited research papers\n",
    "5. **Ask questions**: Open issues on GitHub or discuss with others\n",
    "\n",
    "### Common Applications\n",
    "\n",
    "**For neuroscientists:**\n",
    "```python\n",
    "# Compare your model to brain recordings\n",
    "from neuros_mechint.alignment import CCA, RSA\n",
    "\n",
    "# Align representations\n",
    "cca = CCA(n_components=20)\n",
    "cca.fit(model_activations, brain_data)\n",
    "alignment_score = cca.score(model_activations_test, brain_data_test)\n",
    "\n",
    "# Compare geometries\n",
    "rsa = RSA(metric='correlation')\n",
    "similarity = rsa.compare(model_activations, brain_data)\n",
    "```\n",
    "\n",
    "**For AI researchers:**\n",
    "```python\n",
    "# Understand a transformer\n",
    "from neuros_mechint import SparseAutoencoder\n",
    "from neuros_mechint.interventions import ActivationPatcher\n",
    "\n",
    "# Extract features\n",
    "sae = SparseAutoencoder(input_dim=768, hidden_dim=3072)\n",
    "sae.train_on_activations(activations)\n",
    "\n",
    "# Find circuits\n",
    "patcher = ActivationPatcher(model, layer_name='attention')\n",
    "results = patcher.run_full_analysis(clean_input, corrupted_input)\n",
    "```\n",
    "\n",
    "**For ML engineers:**\n",
    "```python\n",
    "# Enforce biological realism during training\n",
    "from neuros_mechint.fractals import SpectralPrior, HiguchiFractalDimension\n",
    "\n",
    "# Add fractal regularization\n",
    "fractal_loss = SpectralPrior(target_exponent=1.0, weight=0.1)\n",
    "total_loss = task_loss + fractal_loss(hidden_states)\n",
    "\n",
    "# Monitor complexity\n",
    "fd_metric = HiguchiFractalDimension(kmax=10)\n",
    "complexity = fd_metric(neural_timeseries)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practice Exercise\n",
    "\n",
    "Now it's your turn! Try modifying the code above to:\n",
    "\n",
    "### Exercise 1: Explore Different SAE Configurations\n",
    "1. Train SAEs with different sparsity coefficients (0.001, 0.01, 0.1)\n",
    "2. Try different expansion factors (2x, 4x, 8x)\n",
    "3. Compare the sparsity and reconstruction quality\n",
    "\n",
    "### Exercise 2: Test Different Interventions\n",
    "1. Create a more structured \"corruption\" (e.g., add noise only to specific positions)\n",
    "2. Test patching at individual attention heads\n",
    "3. Visualize how recovery score changes across layers\n",
    "\n",
    "### Exercise 3: Combine Techniques\n",
    "1. Train an SAE on activations\n",
    "2. Use the learned features for activation patching\n",
    "3. Identify which features are causally important\n",
    "\n",
    "Here's a starter template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise workspace\n",
    "\n",
    "# TODO: Try different SAE configurations\n",
    "sparsity_values = [0.001, 0.01, 0.1]\n",
    "\n",
    "for sparsity in sparsity_values:\n",
    "    print(f\"\\nTraining SAE with sparsity={sparsity}\")\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# TODO: Test patching with different corruptions\n",
    "# Your code here\n",
    "\n",
    "# TODO: Combine SAE features with causal analysis\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **Mechanistic interpretability** aims to reverse-engineer the \"source code\" of neural networks\n",
    "2. **Sparse Autoencoders** decompose neurons into interpretable, monosemantic features\n",
    "3. **Activation patching** reveals which components are causally necessary\n",
    "4. The library offers **10+ major technique families** for comprehensive analysis\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Interpretability is not just about *inputs* and *outputs*—it's about understanding the *algorithm*\n",
    "- Different techniques provide complementary insights (features, circuits, dynamics, etc.)\n",
    "- These methods work on models of any size (from toys to GPT-scale)\n",
    "- The same techniques apply to both artificial and biological neural networks\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Ready to dive deeper?** Here's what to explore next:\n",
    "\n",
    "1. **[Notebook 02: Sparse Autoencoders](02_sparse_autoencoders.ipynb)**\n",
    "   - Deep dive into SAE training and interpretation\n",
    "   - Hierarchical concept extraction\n",
    "   - Causal SAE interventions\n",
    "\n",
    "2. **[Notebook 03: Causal Interventions](03_causal_interventions.ipynb)**\n",
    "   - Advanced patching techniques\n",
    "   - Building causal graphs\n",
    "   - Circuit discovery workflows\n",
    "\n",
    "3. **[Notebook 04: Fractal Analysis](04_fractal_analysis.ipynb)**\n",
    "   - Understanding biological complexity\n",
    "   - Training with fractal regularization\n",
    "   - Comparing model and brain dynamics\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Papers to Read:**\n",
    "- Elhage et al. (2021): \"A Mathematical Framework for Transformer Circuits\"\n",
    "- Anthropic (2023): \"Towards Monosemanticity\"\n",
    "- Olah et al. (2020): \"Zoom In: An Introduction to Circuits\"\n",
    "- Sussillo & Barak (2013): \"Opening the Black Box\"\n",
    "\n",
    "**Websites:**\n",
    "- [Anthropic's Interpretability Research](https://transformer-circuits.pub/)\n",
    "- [Neuronpedia](https://neuronpedia.org/) - Browse interpretable features\n",
    "- [Distill.pub](https://distill.pub/) - Visual explanations\n",
    "\n",
    "### Get Involved!\n",
    "\n",
    "This library is designed to become a **community standard** for mechanistic interpretability in neuroscience and AI. We welcome:\n",
    "- Bug reports and feature requests\n",
    "- New example notebooks\n",
    "- Integrations with your research\n",
    "- Documentation improvements\n",
    "\n",
    "Together, we can make neural networks—artificial and biological—truly interpretable!\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** Open [02_sparse_autoencoders.ipynb](02_sparse_autoencoders.ipynb) to master feature discovery!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
