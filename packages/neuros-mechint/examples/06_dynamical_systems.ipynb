{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: Dynamical Systems Analysis\n",
    "\n",
    "## Understanding Neural Networks as Dynamical Systems\n",
    "\n",
    "This notebook explores how to analyze neural networks through the lens of **dynamical systems theory**. Instead of viewing networks as static function approximators, we treat them as systems that evolve over time, with rich temporal dynamics, attractors, and stability properties.\n",
    "\n",
    "### Why Dynamical Systems Matter for Mechanistic Interpretability\n",
    "\n",
    "1. **Recurrent Networks are Dynamical Systems**: RNNs, LSTMs, and transformers with recurrence naturally evolve over time\n",
    "2. **Attractors Explain Computation**: Fixed points and limit cycles correspond to stable computational states\n",
    "3. **Stability Reveals Robustness**: Lyapunov exponents tell us if networks are chaotic or stable\n",
    "4. **Dimensionality Reveals Complexity**: Intrinsic dimensionality shows true degrees of freedom\n",
    "5. **Controllability Enables Steering**: Understanding control allows us to guide network behavior\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Koopman Operator Theory**: Linearizing nonlinear dynamics for analysis\n",
    "2. **Dynamic Mode Decomposition (DMD)**: Data-driven approach to finding dynamic modes\n",
    "3. **Lyapunov Exponents**: Measuring chaos and stability in neural dynamics\n",
    "4. **Fixed Point Analysis**: Finding and characterizing stable states\n",
    "5. **Intrinsic Dimensionality**: Measuring true representational complexity\n",
    "6. **Controllability Analysis**: Understanding how to steer network dynamics\n",
    "\n",
    "### References\n",
    "\n",
    "- Kutz et al. (2016): *Dynamic Mode Decomposition: Data-Driven Modeling of Complex Systems*\n",
    "- Brunton & Kutz (2019): *Data-Driven Science and Engineering*\n",
    "- Sussillo & Barak (2013): *Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional RNNs*\n",
    "- Maheswaranathan et al. (2019): *Universality and individuality in neural dynamics*\n",
    "- Jazayeri & Afraz (2017): *Navigating the neural space in search of the neural code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Koopman Operator Theory and Dynamic Mode Decomposition\n",
    "\n",
    "### The Koopman Operator: Linearizing Nonlinear Dynamics\n",
    "\n",
    "The **Koopman operator** is a powerful mathematical tool that allows us to analyze nonlinear dynamical systems using linear techniques. Here's the key insight:\n",
    "\n",
    "**Nonlinear system**:\n",
    "```\n",
    "x(t+1) = F(x(t))  # Nonlinear evolution\n",
    "```\n",
    "\n",
    "**Koopman operator** acts on *observables* g(x):\n",
    "```\n",
    "K g(x) = g(F(x))  # Linear operator on infinite-dimensional space\n",
    "```\n",
    "\n",
    "The beauty: Although F is nonlinear, K is *linear* (but infinite-dimensional).\n",
    "\n",
    "**Dynamic Mode Decomposition (DMD)** approximates K from data:\n",
    "- Collect snapshots: X = [x(0), x(1), ..., x(T-1)]\n",
    "- Collect next states: X' = [x(1), x(2), ..., x(T)]\n",
    "- Find best-fit linear operator: X' ≈ A X\n",
    "- Eigendecomposition of A gives dynamic modes and growth rates\n",
    "\n",
    "### Why This Matters for Neural Networks\n",
    "\n",
    "1. **RNN Dynamics**: Understand how hidden states evolve over time\n",
    "2. **Mode Decomposition**: Identify dominant patterns in neural activity\n",
    "3. **Forecasting**: Predict future states from current observations\n",
    "4. **Control**: Design inputs to guide dynamics to desired states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicModeDecomposition:\n",
    "    \"\"\"\n",
    "    Dynamic Mode Decomposition (DMD) for analyzing temporal dynamics.\n",
    "    \n",
    "    DMD finds the best-fit linear operator that advances dynamics:\n",
    "        X' = A X\n",
    "    \n",
    "    The eigendecomposition of A reveals:\n",
    "    - Dynamic modes (spatial patterns)\n",
    "    - Eigenvalues (growth/decay rates and frequencies)\n",
    "    - Mode amplitudes (importance of each mode)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rank=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rank: Rank for truncated SVD. If None, use full rank.\n",
    "        \"\"\"\n",
    "        self.rank = rank\n",
    "        self.modes = None\n",
    "        self.eigenvalues = None\n",
    "        self.amplitudes = None\n",
    "    \n",
    "    def fit(self, X, dt=1.0):\n",
    "        \"\"\"\n",
    "        Fit DMD to trajectory data.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape (n_features, n_timesteps)\n",
    "               Columns are snapshots at different times\n",
    "            dt: Time step between snapshots\n",
    "        \"\"\"\n",
    "        # Split into current and next states\n",
    "        X1 = X[:, :-1]  # x(0), x(1), ..., x(T-1)\n",
    "        X2 = X[:, 1:]   # x(1), x(2), ..., x(T)\n",
    "        \n",
    "        # Compute SVD of X1\n",
    "        U, S, Vt = np.linalg.svd(X1, full_matrices=False)\n",
    "        \n",
    "        # Truncate if rank specified\n",
    "        if self.rank is not None:\n",
    "            U = U[:, :self.rank]\n",
    "            S = S[:self.rank]\n",
    "            Vt = Vt[:self.rank, :]\n",
    "        \n",
    "        # Build reduced DMD operator: Atilde = U^T @ X2 @ V @ S^{-1}\n",
    "        Atilde = U.T @ X2 @ Vt.T @ np.diag(1/S)\n",
    "        \n",
    "        # Eigendecomposition of reduced operator\n",
    "        eigvals, eigvecs = np.linalg.eig(Atilde)\n",
    "        \n",
    "        # Reconstruct full modes: Phi = X2 @ V @ S^{-1} @ W\n",
    "        self.modes = X2 @ Vt.T @ np.diag(1/S) @ eigvecs\n",
    "        \n",
    "        # Continuous-time eigenvalues\n",
    "        self.eigenvalues = np.log(eigvals) / dt\n",
    "        \n",
    "        # Compute mode amplitudes from initial condition\n",
    "        self.amplitudes = np.linalg.lstsq(self.modes, X[:, 0], rcond=None)[0]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, t):\n",
    "        \"\"\"\n",
    "        Predict state at time t using DMD reconstruction.\n",
    "        \n",
    "        x(t) = sum_k [ b_k * phi_k * exp(lambda_k * t) ]\n",
    "        \n",
    "        Args:\n",
    "            t: Time or array of times\n",
    "        \n",
    "        Returns:\n",
    "            Predicted state(s)\n",
    "        \"\"\"\n",
    "        t = np.atleast_1d(t)\n",
    "        \n",
    "        # Compute time dynamics: exp(lambda * t)\n",
    "        time_dynamics = np.exp(np.outer(self.eigenvalues, t))\n",
    "        \n",
    "        # Weighted sum: Phi @ diag(b) @ time_dynamics\n",
    "        predictions = self.modes @ (self.amplitudes[:, np.newaxis] * time_dynamics)\n",
    "        \n",
    "        return predictions.real\n",
    "    \n",
    "    def get_mode_properties(self):\n",
    "        \"\"\"\n",
    "        Extract interpretable properties of each mode.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with mode properties:\n",
    "            - growth_rates: Real part of eigenvalues (stability)\n",
    "            - frequencies: Imaginary part of eigenvalues (oscillation)\n",
    "            - amplitudes: Mode amplitudes (importance)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'growth_rates': self.eigenvalues.real,\n",
    "            'frequencies': self.eigenvalues.imag,\n",
    "            'amplitudes': np.abs(self.amplitudes),\n",
    "            'modes': self.modes\n",
    "        }\n",
    "\n",
    "print(\"DMD class implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic RNN-like dynamics for demonstration\n",
    "def generate_rnn_dynamics(n_timesteps=200, n_neurons=50):\n",
    "    \"\"\"\n",
    "    Generate synthetic recurrent neural network dynamics.\n",
    "    Creates a system with multiple oscillatory and decaying modes.\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, 10, n_timesteps)\n",
    "    \n",
    "    # Create multiple dynamic modes\n",
    "    # Mode 1: Slow oscillation (frequency = 1 Hz, stable)\n",
    "    mode1 = np.outer(np.random.randn(n_neurons), np.sin(2*np.pi*1*t) * np.exp(-0.1*t))\n",
    "    \n",
    "    # Mode 2: Fast oscillation (frequency = 3 Hz, decaying)\n",
    "    mode2 = np.outer(np.random.randn(n_neurons), np.cos(2*np.pi*3*t) * np.exp(-0.3*t))\n",
    "    \n",
    "    # Mode 3: Exponential decay\n",
    "    mode3 = np.outer(np.random.randn(n_neurons), np.exp(-0.5*t))\n",
    "    \n",
    "    # Combine modes\n",
    "    X = mode1 + 0.5*mode2 + 0.3*mode3\n",
    "    \n",
    "    # Add small noise\n",
    "    X += 0.1 * np.random.randn(*X.shape)\n",
    "    \n",
    "    return X, t\n",
    "\n",
    "# Generate data\n",
    "X, t = generate_rnn_dynamics(n_timesteps=200, n_neurons=50)\n",
    "\n",
    "print(f\"Generated dynamics: {X.shape[0]} neurons, {X.shape[1]} timesteps\")\n",
    "print(f\"Time range: {t[0]:.2f} to {t[-1]:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DMD to the data\n",
    "dmd = DynamicModeDecomposition(rank=10)  # Use top 10 modes\n",
    "dmd.fit(X, dt=t[1]-t[0])\n",
    "\n",
    "# Get mode properties\n",
    "props = dmd.get_mode_properties()\n",
    "\n",
    "print(\"\\nDMD Analysis Results:\")\n",
    "print(\"=\"*50)\n",
    "for i in range(min(5, len(props['growth_rates']))):\n",
    "    print(f\"\\nMode {i+1}:\")\n",
    "    print(f\"  Growth rate: {props['growth_rates'][i]:.4f} (negative = decaying)\")\n",
    "    print(f\"  Frequency: {props['frequencies'][i]:.4f} Hz\")\n",
    "    print(f\"  Amplitude: {props['amplitudes'][i]:.4f} (importance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DMD results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Original dynamics (first 3 neurons)\n",
    "ax = axes[0, 0]\n",
    "for i in range(3):\n",
    "    ax.plot(t, X[i, :], label=f'Neuron {i+1}', alpha=0.7)\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Activity')\n",
    "ax.set_title('Original Neural Dynamics (Sample Neurons)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: DMD reconstruction\n",
    "ax = axes[0, 1]\n",
    "X_pred = dmd.predict(t)\n",
    "for i in range(3):\n",
    "    ax.plot(t, X_pred[i, :], '--', label=f'Neuron {i+1} (DMD)', alpha=0.7)\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Activity')\n",
    "ax.set_title('DMD Reconstruction')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Eigenvalue spectrum\n",
    "ax = axes[1, 0]\n",
    "scatter = ax.scatter(props['growth_rates'], props['frequencies'], \n",
    "                    s=props['amplitudes']*1000, \n",
    "                    c=props['amplitudes'], cmap='viridis',\n",
    "                    alpha=0.6, edgecolors='black')\n",
    "ax.axvline(x=0, color='red', linestyle='--', alpha=0.5, label='Stability boundary')\n",
    "ax.set_xlabel('Growth Rate (Real part)')\n",
    "ax.set_ylabel('Frequency (Imaginary part, Hz)')\n",
    "ax.set_title('DMD Eigenvalue Spectrum\\n(size = amplitude)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.colorbar(scatter, ax=ax, label='Amplitude')\n",
    "\n",
    "# Plot 4: Mode amplitudes\n",
    "ax = axes[1, 1]\n",
    "mode_indices = np.arange(len(props['amplitudes']))\n",
    "colors = ['red' if gr > 0 else 'blue' for gr in props['growth_rates']]\n",
    "ax.bar(mode_indices, props['amplitudes'], color=colors, alpha=0.6)\n",
    "ax.set_xlabel('Mode Index')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('Mode Amplitudes\\n(red=growing, blue=decaying)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Growth rate < 0: Mode decays over time (stable)\")\n",
    "print(\"- Growth rate > 0: Mode grows over time (unstable)\")\n",
    "print(\"- Frequency ≠ 0: Mode oscillates\")\n",
    "print(\"- Amplitude: Importance of mode to overall dynamics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Lyapunov Exponents - Measuring Chaos and Stability\n",
    "\n",
    "### What are Lyapunov Exponents?\n",
    "\n",
    "**Lyapunov exponents** measure how fast nearby trajectories diverge or converge in a dynamical system.\n",
    "\n",
    "**The largest Lyapunov exponent (λ_max)**:\n",
    "- λ < 0: System is stable (trajectories converge)\n",
    "- λ ≈ 0: System is marginally stable\n",
    "- λ > 0: System is chaotic (trajectories diverge exponentially)\n",
    "\n",
    "**Mathematical definition**:\n",
    "```\n",
    "λ = lim_{t→∞} (1/t) * log(||δx(t)|| / ||δx(0)||)\n",
    "```\n",
    "where δx(t) is a small perturbation that evolves over time.\n",
    "\n",
    "### Why This Matters for Neural Networks\n",
    "\n",
    "1. **Robustness**: Negative Lyapunov exponents mean networks are robust to noise\n",
    "2. **Edge of Chaos**: Many successful RNNs operate near λ ≈ 0 (critical regime)\n",
    "3. **Generalization**: Chaotic networks (λ > 0) may overfit and be unpredictable\n",
    "4. **Information Processing**: Moderate complexity (near-critical) is optimal for computation\n",
    "\n",
    "### Computing Lyapunov Exponents from Data\n",
    "\n",
    "For a discrete-time system x(t+1) = F(x(t)):\n",
    "1. Start with initial state x(0)\n",
    "2. Add small perturbation: x_perturbed(0) = x(0) + δ\n",
    "3. Evolve both trajectories: x(t) and x_perturbed(t)\n",
    "4. Periodically measure divergence and renormalize\n",
    "5. Average log growth rate over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyapunovExponentEstimator:\n",
    "    \"\"\"\n",
    "    Estimate largest Lyapunov exponent from time series data.\n",
    "    \n",
    "    Uses the method of tracking divergence of nearby trajectories.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dt=1.0, perturbation_size=1e-8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dt: Time step\n",
    "            perturbation_size: Size of initial perturbation\n",
    "        \"\"\"\n",
    "        self.dt = dt\n",
    "        self.perturbation_size = perturbation_size\n",
    "    \n",
    "    def estimate_from_model(self, model, x0, n_steps=1000, renorm_interval=1):\n",
    "        \"\"\"\n",
    "        Estimate Lyapunov exponent from a dynamical model.\n",
    "        \n",
    "        Args:\n",
    "            model: Function that takes state and returns next state\n",
    "            x0: Initial state\n",
    "            n_steps: Number of time steps to simulate\n",
    "            renorm_interval: How often to renormalize perturbation\n",
    "        \n",
    "        Returns:\n",
    "            lyapunov_exponent: Largest Lyapunov exponent\n",
    "            trajectory: List of (time, divergence) tuples\n",
    "        \"\"\"\n",
    "        # Initialize\n",
    "        x = np.array(x0, dtype=np.float64)\n",
    "        \n",
    "        # Random perturbation direction\n",
    "        delta = np.random.randn(*x.shape)\n",
    "        delta = delta / np.linalg.norm(delta) * self.perturbation_size\n",
    "        \n",
    "        # Track log divergence\n",
    "        log_divergence = 0.0\n",
    "        trajectory = []\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            # Evolve both trajectories\n",
    "            x_next = model(x)\n",
    "            x_pert_next = model(x + delta)\n",
    "            \n",
    "            # Compute new perturbation\n",
    "            delta_next = x_pert_next - x_next\n",
    "            \n",
    "            # Measure divergence\n",
    "            divergence = np.linalg.norm(delta_next)\n",
    "            \n",
    "            # Accumulate log divergence\n",
    "            if divergence > 0:\n",
    "                log_divergence += np.log(divergence / self.perturbation_size)\n",
    "            \n",
    "            # Renormalize perturbation to prevent overflow\n",
    "            if step % renorm_interval == 0 and divergence > 0:\n",
    "                delta_next = delta_next / divergence * self.perturbation_size\n",
    "            \n",
    "            # Store trajectory\n",
    "            trajectory.append((step * self.dt, log_divergence / ((step + 1) * self.dt)))\n",
    "            \n",
    "            # Update for next iteration\n",
    "            x = x_next\n",
    "            delta = delta_next\n",
    "        \n",
    "        # Average Lyapunov exponent\n",
    "        lyapunov_exponent = log_divergence / (n_steps * self.dt)\n",
    "        \n",
    "        return lyapunov_exponent, trajectory\n",
    "\n",
    "print(\"Lyapunov exponent estimator implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three example systems with different stability properties\n",
    "\n",
    "# System 1: Stable (converging)\n",
    "def stable_system(x):\n",
    "    \"\"\"Linear system with eigenvalues < 1 (stable)\"\"\"\n",
    "    A = np.array([[0.9, 0.1], [-0.1, 0.85]])\n",
    "    return A @ x + 0.01 * np.random.randn(2)\n",
    "\n",
    "# System 2: Marginally stable (near critical)\n",
    "def critical_system(x):\n",
    "    \"\"\"Nonlinear system near edge of chaos\"\"\"\n",
    "    return np.array([\n",
    "        0.99 * x[0] - 0.2 * x[1] + 0.1 * np.tanh(x[0]),\n",
    "        0.2 * x[0] + 0.98 * x[1] + 0.1 * np.tanh(x[1])\n",
    "    ])\n",
    "\n",
    "# System 3: Chaotic (Lorenz-like)\n",
    "def chaotic_system(x, sigma=10, rho=28, beta=8/3, dt=0.01):\n",
    "    \"\"\"Simplified Lorenz system (chaotic)\"\"\"\n",
    "    # Using 2D projection for simplicity\n",
    "    dx = np.array([\n",
    "        sigma * (x[1] - x[0]),\n",
    "        x[0] * (rho - x[1]) - x[1]\n",
    "    ])\n",
    "    return x + dt * dx\n",
    "\n",
    "# Estimate Lyapunov exponents for each system\n",
    "estimator = LyapunovExponentEstimator(dt=0.01)\n",
    "\n",
    "systems = [\n",
    "    ('Stable', stable_system, np.array([1.0, 1.0])),\n",
    "    ('Critical', critical_system, np.array([1.0, 1.0])),\n",
    "    ('Chaotic', chaotic_system, np.array([1.0, 1.0]))\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for name, system, x0 in systems:\n",
    "    lyap, traj = estimator.estimate_from_model(system, x0, n_steps=1000)\n",
    "    results[name] = {'lyapunov': lyap, 'trajectory': traj}\n",
    "    print(f\"{name} system: λ = {lyap:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"λ < 0: Stable (trajectories converge)\")\n",
    "print(\"λ ≈ 0: Critical (edge of chaos)\")\n",
    "print(\"λ > 0: Chaotic (trajectories diverge)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Lyapunov exponent convergence\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    traj = result['trajectory']\n",
    "    times = [t for t, _ in traj]\n",
    "    lyaps = [l for _, l in traj]\n",
    "    \n",
    "    ax.plot(times, lyaps, linewidth=2)\n",
    "    ax.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='λ=0 (critical)')\n",
    "    ax.axhline(y=result['lyapunov'], color='green', linestyle='--', \n",
    "               alpha=0.5, label=f'Final: {result[\"lyapunov\"]:.3f}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Lyapunov Exponent')\n",
    "    ax.set_title(f'{name} System')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Lyapunov exponent should converge to a stable value.\")\n",
    "print(\"Oscillations indicate transient dynamics or insufficient sampling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fixed Point Analysis\n",
    "\n",
    "### What are Fixed Points?\n",
    "\n",
    "A **fixed point** is a state where the dynamics stop changing:\n",
    "```\n",
    "x* = F(x*)  # System stays at x* forever\n",
    "```\n",
    "\n",
    "**Why fixed points matter**:\n",
    "1. **Computational Attractors**: Fixed points represent stable computational states\n",
    "2. **Memory**: Fixed points can store information (like attractor networks)\n",
    "3. **Decision States**: Different fixed points can correspond to different decisions\n",
    "4. **Stability Analysis**: Local dynamics around fixed points reveal stability\n",
    "\n",
    "### Types of Fixed Points\n",
    "\n",
    "Classified by eigenvalues of Jacobian at the fixed point:\n",
    "- **Stable node**: All eigenvalues < 1 (attractive)\n",
    "- **Unstable node**: All eigenvalues > 1 (repulsive)\n",
    "- **Saddle point**: Mixed eigenvalues (attractive in some directions, repulsive in others)\n",
    "- **Spiral**: Complex eigenvalues (oscillatory approach)\n",
    "\n",
    "### Finding Fixed Points in Neural Networks\n",
    "\n",
    "For RNNs, fixed points satisfy:\n",
    "```\n",
    "h* = f(W @ h* + b)\n",
    "```\n",
    "where f is the activation function (e.g., tanh, ReLU).\n",
    "\n",
    "We can find these numerically using optimization or root-finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPointFinder:\n",
    "    \"\"\"\n",
    "    Find and analyze fixed points of a dynamical system.\n",
    "    \n",
    "    Fixed points satisfy: x* = F(x*)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system, jacobian=None, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            system: Function that takes state and returns next state\n",
    "            jacobian: Function that computes Jacobian (if None, use numerical)\n",
    "            tolerance: Convergence tolerance\n",
    "        \"\"\"\n",
    "        self.system = system\n",
    "        self.jacobian = jacobian\n",
    "        self.tolerance = tolerance\n",
    "    \n",
    "    def find_fixed_point(self, x0, max_iter=1000):\n",
    "        \"\"\"\n",
    "        Find fixed point starting from initial guess x0.\n",
    "        \n",
    "        Solves: F(x) - x = 0\n",
    "        \n",
    "        Args:\n",
    "            x0: Initial guess\n",
    "            max_iter: Maximum iterations\n",
    "        \n",
    "        Returns:\n",
    "            fixed_point: Solution (or None if not found)\n",
    "            converged: Whether optimization converged\n",
    "        \"\"\"\n",
    "        # Define equation: F(x) - x = 0\n",
    "        def equation(x):\n",
    "            return self.system(x) - x\n",
    "        \n",
    "        # Solve using scipy\n",
    "        try:\n",
    "            solution = fsolve(equation, x0, full_output=True, maxfev=max_iter)\n",
    "            x_star, info, ier, msg = solution\n",
    "            \n",
    "            # Check convergence\n",
    "            residual = np.linalg.norm(equation(x_star))\n",
    "            converged = (ier == 1) and (residual < self.tolerance)\n",
    "            \n",
    "            if converged:\n",
    "                return x_star, True\n",
    "            else:\n",
    "                return None, False\n",
    "        except:\n",
    "            return None, False\n",
    "    \n",
    "    def compute_jacobian(self, x, epsilon=1e-7):\n",
    "        \"\"\"\n",
    "        Compute Jacobian numerically using finite differences.\n",
    "        \n",
    "        J[i,j] = ∂F_i/∂x_j\n",
    "        \"\"\"\n",
    "        if self.jacobian is not None:\n",
    "            return self.jacobian(x)\n",
    "        \n",
    "        n = len(x)\n",
    "        J = np.zeros((n, n))\n",
    "        \n",
    "        for j in range(n):\n",
    "            x_plus = x.copy()\n",
    "            x_plus[j] += epsilon\n",
    "            \n",
    "            x_minus = x.copy()\n",
    "            x_minus[j] -= epsilon\n",
    "            \n",
    "            J[:, j] = (self.system(x_plus) - self.system(x_minus)) / (2 * epsilon)\n",
    "        \n",
    "        return J\n",
    "    \n",
    "    def analyze_stability(self, fixed_point):\n",
    "        \"\"\"\n",
    "        Analyze stability of fixed point.\n",
    "        \n",
    "        Computes eigenvalues of Jacobian at fixed point.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "            - eigenvalues: Eigenvalues of Jacobian\n",
    "            - max_eigenvalue: Largest magnitude eigenvalue\n",
    "            - stable: Whether fixed point is stable\n",
    "            - type: Classification (stable/unstable/saddle)\n",
    "        \"\"\"\n",
    "        # Compute Jacobian at fixed point\n",
    "        J = self.compute_jacobian(fixed_point)\n",
    "        \n",
    "        # Eigenvalues\n",
    "        eigvals = np.linalg.eigvals(J)\n",
    "        max_eigval = np.max(np.abs(eigvals))\n",
    "        \n",
    "        # Stability\n",
    "        stable = max_eigval < 1.0\n",
    "        \n",
    "        # Classification\n",
    "        if stable:\n",
    "            fp_type = 'stable'\n",
    "        elif max_eigval > 1.0 and np.all(np.abs(eigvals) > 1.0):\n",
    "            fp_type = 'unstable'\n",
    "        else:\n",
    "            fp_type = 'saddle'\n",
    "        \n",
    "        return {\n",
    "            'eigenvalues': eigvals,\n",
    "            'max_eigenvalue': max_eigval,\n",
    "            'stable': stable,\n",
    "            'type': fp_type,\n",
    "            'jacobian': J\n",
    "        }\n",
    "\n",
    "print(\"Fixed point finder implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple 2D RNN-like system\n",
    "def rnn_system(x):\n",
    "    \"\"\"\n",
    "    Simple 2D recurrent system: h(t+1) = tanh(W @ h(t) + b)\n",
    "    \"\"\"\n",
    "    W = np.array([[1.5, -1.0], [1.0, 0.5]])\n",
    "    b = np.array([0.5, -0.5])\n",
    "    return np.tanh(W @ x + b)\n",
    "\n",
    "# Create fixed point finder\n",
    "fp_finder = FixedPointFinder(rnn_system)\n",
    "\n",
    "# Try multiple initial conditions to find different fixed points\n",
    "initial_guesses = [\n",
    "    np.array([0.0, 0.0]),\n",
    "    np.array([1.0, 1.0]),\n",
    "    np.array([-1.0, 1.0]),\n",
    "    np.array([1.0, -1.0]),\n",
    "    np.array([-1.0, -1.0]),\n",
    "]\n",
    "\n",
    "fixed_points = []\n",
    "for x0 in initial_guesses:\n",
    "    fp, converged = fp_finder.find_fixed_point(x0)\n",
    "    if converged:\n",
    "        # Check if this is a new fixed point (not duplicate)\n",
    "        is_new = True\n",
    "        for existing_fp in fixed_points:\n",
    "            if np.linalg.norm(fp - existing_fp) < 0.1:\n",
    "                is_new = False\n",
    "                break\n",
    "        \n",
    "        if is_new:\n",
    "            fixed_points.append(fp)\n",
    "\n",
    "print(f\"Found {len(fixed_points)} fixed point(s):\\n\")\n",
    "\n",
    "# Analyze each fixed point\n",
    "fp_analyses = []\n",
    "for i, fp in enumerate(fixed_points):\n",
    "    analysis = fp_finder.analyze_stability(fp)\n",
    "    fp_analyses.append(analysis)\n",
    "    \n",
    "    print(f\"Fixed Point {i+1}: [{fp[0]:.4f}, {fp[1]:.4f}]\")\n",
    "    print(f\"  Type: {analysis['type']}\")\n",
    "    print(f\"  Max |eigenvalue|: {analysis['max_eigenvalue']:.4f}\")\n",
    "    print(f\"  Eigenvalues: {analysis['eigenvalues']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the vector field and fixed points\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Create grid for vector field\n",
    "x_range = np.linspace(-2, 2, 20)\n",
    "y_range = np.linspace(-2, 2, 20)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Compute vector field: F(x) - x\n",
    "U = np.zeros_like(X)\n",
    "V = np.zeros_like(Y)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        x = np.array([X[i, j], Y[i, j]])\n",
    "        dx = rnn_system(x) - x\n",
    "        U[i, j] = dx[0]\n",
    "        V[i, j] = dx[1]\n",
    "\n",
    "# Plot vector field\n",
    "ax.quiver(X, Y, U, V, alpha=0.6, color='gray')\n",
    "\n",
    "# Plot sample trajectories\n",
    "for x0 in initial_guesses:\n",
    "    trajectory = [x0]\n",
    "    x = x0.copy()\n",
    "    for _ in range(50):\n",
    "        x = rnn_system(x)\n",
    "        trajectory.append(x.copy())\n",
    "    trajectory = np.array(trajectory)\n",
    "    ax.plot(trajectory[:, 0], trajectory[:, 1], 'b-', alpha=0.3, linewidth=1)\n",
    "    ax.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=6)\n",
    "\n",
    "# Plot fixed points\n",
    "for i, (fp, analysis) in enumerate(zip(fixed_points, fp_analyses)):\n",
    "    color = 'red' if analysis['stable'] else 'orange'\n",
    "    marker = 'o' if analysis['type'] != 'saddle' else 's'\n",
    "    ax.plot(fp[0], fp[1], marker=marker, color=color, \n",
    "            markersize=15, markeredgewidth=2, markeredgecolor='black',\n",
    "            label=f\"FP {i+1}: {analysis['type']}\")\n",
    "\n",
    "ax.set_xlabel('x₁')\n",
    "ax.set_ylabel('x₂')\n",
    "ax.set_title('RNN Dynamics: Vector Field and Fixed Points\\n(Gray arrows show dynamics, blue lines show trajectories)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Red circles: Stable fixed points (attractors)\")\n",
    "print(\"- Orange: Unstable or saddle points\")\n",
    "print(\"- Blue trajectories converge to stable fixed points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Intrinsic Dimensionality Estimation\n",
    "\n",
    "### What is Intrinsic Dimensionality?\n",
    "\n",
    "**Intrinsic dimensionality** is the minimum number of parameters needed to describe the data.\n",
    "\n",
    "Example: Points on a 2D plane embedded in 3D space have intrinsic dimension 2.\n",
    "\n",
    "For neural networks:\n",
    "- Hidden layer may have 1000 neurons (ambient dimension = 1000)\n",
    "- But activations may lie on a lower-dimensional manifold (intrinsic dimension << 1000)\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "1. **Representational Efficiency**: Low intrinsic dimension means efficient encoding\n",
    "2. **Generalization**: Lower dimension often correlates with better generalization\n",
    "3. **Interpretability**: Fewer degrees of freedom are easier to understand\n",
    "4. **Compression**: Reveals opportunity for dimensionality reduction\n",
    "\n",
    "### Methods for Estimating Intrinsic Dimensionality\n",
    "\n",
    "1. **PCA**: Count components explaining variance\n",
    "2. **Participation Ratio**: Measure effective dimensionality from eigenvalues\n",
    "3. **Local PCA**: Estimate dimension in local neighborhoods\n",
    "4. **MLE Methods**: Maximum likelihood estimation of dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntrinsicDimensionalityEstimator:\n",
    "    \"\"\"\n",
    "    Estimate intrinsic dimensionality of neural representations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pca = None\n",
    "    \n",
    "    def participation_ratio(self, X):\n",
    "        \"\"\"\n",
    "        Compute participation ratio (effective dimensionality).\n",
    "        \n",
    "        PR = (sum λ_i)² / sum λ_i²\n",
    "        \n",
    "        where λ_i are eigenvalues of covariance matrix.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "            pr: Participation ratio\n",
    "        \"\"\"\n",
    "        # Compute covariance\n",
    "        X_centered = X - X.mean(axis=0)\n",
    "        cov = (X_centered.T @ X_centered) / (len(X) - 1)\n",
    "        \n",
    "        # Eigenvalues\n",
    "        eigvals = np.linalg.eigvalsh(cov)\n",
    "        eigvals = np.maximum(eigvals, 0)  # Ensure non-negative\n",
    "        \n",
    "        # Participation ratio\n",
    "        if np.sum(eigvals**2) == 0:\n",
    "            return 0\n",
    "        pr = np.sum(eigvals)**2 / np.sum(eigvals**2)\n",
    "        \n",
    "        return pr\n",
    "    \n",
    "    def explained_variance_dimension(self, X, variance_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Estimate dimension as number of PCs explaining threshold variance.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix (n_samples, n_features)\n",
    "            variance_threshold: Cumulative variance threshold (e.g., 0.9 for 90%)\n",
    "        \n",
    "        Returns:\n",
    "            n_components: Number of components\n",
    "            explained_variance_ratio: Variance explained by each component\n",
    "        \"\"\"\n",
    "        self.pca = PCA()\n",
    "        self.pca.fit(X)\n",
    "        \n",
    "        cumsum = np.cumsum(self.pca.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum >= variance_threshold) + 1\n",
    "        \n",
    "        return n_components, self.pca.explained_variance_ratio_\n",
    "    \n",
    "    def mle_dimension(self, X, k=10):\n",
    "        \"\"\"\n",
    "        Maximum likelihood estimate of intrinsic dimension.\n",
    "        \n",
    "        Uses k-nearest neighbors method (Levina & Bickel, 2005).\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix (n_samples, n_features)\n",
    "            k: Number of nearest neighbors\n",
    "        \n",
    "        Returns:\n",
    "            dimension: Estimated intrinsic dimension\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Find k nearest neighbors\n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "        distances, _ = nbrs.kneighbors(X)\n",
    "        \n",
    "        # Remove self (distance 0)\n",
    "        distances = distances[:, 1:]\n",
    "        \n",
    "        # MLE formula\n",
    "        # d = [ (k-1) / sum_i log(r_k(i) / r_1(i)) ]^{-1}\n",
    "        r_k = distances[:, -1]  # k-th nearest neighbor distance\n",
    "        r_1 = distances[:, 0]   # 1st nearest neighbor distance\n",
    "        \n",
    "        # Avoid log(0)\n",
    "        ratio = r_k / (r_1 + 1e-10)\n",
    "        ratio = np.maximum(ratio, 1e-10)\n",
    "        \n",
    "        dimension = (k - 1) / np.mean(np.log(ratio))\n",
    "        \n",
    "        return dimension\n",
    "    \n",
    "    def estimate_all(self, X, variance_threshold=0.9, k=10):\n",
    "        \"\"\"\n",
    "        Run all dimensionality estimation methods.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all estimates\n",
    "        \"\"\"\n",
    "        pr = self.participation_ratio(X)\n",
    "        pca_dim, var_ratios = self.explained_variance_dimension(X, variance_threshold)\n",
    "        mle_dim = self.mle_dimension(X, k)\n",
    "        \n",
    "        return {\n",
    "            'participation_ratio': pr,\n",
    "            'pca_dimension': pca_dim,\n",
    "            'mle_dimension': mle_dim,\n",
    "            'explained_variance_ratios': var_ratios,\n",
    "            'ambient_dimension': X.shape[1]\n",
    "        }\n",
    "\n",
    "print(\"Intrinsic dimensionality estimator implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with known intrinsic dimension\n",
    "def generate_manifold_data(n_samples=1000, intrinsic_dim=3, ambient_dim=50, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate data lying on a low-dimensional manifold in high-dimensional space.\n",
    "    \"\"\"\n",
    "    # Generate intrinsic coordinates\n",
    "    intrinsic_coords = np.random.randn(n_samples, intrinsic_dim)\n",
    "    \n",
    "    # Random projection to ambient space\n",
    "    projection = np.random.randn(intrinsic_dim, ambient_dim)\n",
    "    projection = projection / np.linalg.norm(projection, axis=0)\n",
    "    \n",
    "    # Project to high-dimensional space\n",
    "    X = intrinsic_coords @ projection\n",
    "    \n",
    "    # Add noise\n",
    "    X += noise * np.random.randn(n_samples, ambient_dim)\n",
    "    \n",
    "    return X, intrinsic_coords\n",
    "\n",
    "# Generate data\n",
    "true_dim = 5\n",
    "X, intrinsic = generate_manifold_data(n_samples=1000, intrinsic_dim=true_dim, \n",
    "                                       ambient_dim=100, noise=0.05)\n",
    "\n",
    "print(f\"Generated data:\")\n",
    "print(f\"  True intrinsic dimension: {true_dim}\")\n",
    "print(f\"  Ambient dimension: {X.shape[1]}\")\n",
    "print(f\"  Number of samples: {X.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate dimensionality\n",
    "estimator = IntrinsicDimensionalityEstimator()\n",
    "estimates = estimator.estimate_all(X, variance_threshold=0.95, k=10)\n",
    "\n",
    "print(\"\\nDimensionality Estimates:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Participation Ratio: {estimates['participation_ratio']:.2f}\")\n",
    "print(f\"PCA Dimension (95% variance): {estimates['pca_dimension']}\")\n",
    "print(f\"MLE Dimension: {estimates['mle_dimension']:.2f}\")\n",
    "print(f\"\\nTrue dimension: {true_dim}\")\n",
    "print(f\"Ambient dimension: {estimates['ambient_dimension']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Scree plot (variance per component)\n",
    "ax = axes[0]\n",
    "var_ratios = estimates['explained_variance_ratios']\n",
    "ax.bar(range(1, min(21, len(var_ratios)+1)), var_ratios[:20], alpha=0.7)\n",
    "ax.axvline(x=true_dim, color='red', linestyle='--', linewidth=2, label=f'True dim: {true_dim}')\n",
    "ax.axvline(x=estimates['pca_dimension'], color='green', linestyle='--', \n",
    "           linewidth=2, label=f'PCA dim: {estimates[\"pca_dimension\"]}')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Explained Variance Ratio')\n",
    "ax.set_title('Scree Plot: Variance per Component')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Cumulative explained variance\n",
    "ax = axes[1]\n",
    "cumsum = np.cumsum(var_ratios[:20])\n",
    "ax.plot(range(1, len(cumsum)+1), cumsum, 'b-', linewidth=2)\n",
    "ax.axhline(y=0.95, color='gray', linestyle='--', alpha=0.5, label='95% threshold')\n",
    "ax.axvline(x=true_dim, color='red', linestyle='--', linewidth=2, label=f'True dim: {true_dim}')\n",
    "ax.axvline(x=estimates['pca_dimension'], color='green', linestyle='--', \n",
    "           linewidth=2, label=f'PCA dim: {estimates[\"pca_dimension\"]}')\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Cumulative Explained Variance')\n",
    "ax.set_title('Cumulative Explained Variance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Sharp drop in variance indicates intrinsic dimensionality\")\n",
    "print(\"- All three methods should agree roughly with true dimension\")\n",
    "print(\"- Participation ratio gives 'effective' dimension (continuous value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Complete Dynamical Systems Analysis Pipeline\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "Let's create a complete pipeline that analyzes an RNN's dynamics:\n",
    "\n",
    "1. **Collect hidden state trajectories** from RNN\n",
    "2. **DMD analysis**: Find dominant dynamic modes\n",
    "3. **Lyapunov analysis**: Measure stability/chaos\n",
    "4. **Fixed point analysis**: Find computational attractors\n",
    "5. **Dimensionality analysis**: Measure complexity\n",
    "6. **Generate comprehensive report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple RNN for demonstration\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=50):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x, h0=None):\n",
    "        output, hn = self.rnn(x, h0)\n",
    "        return output, hn\n",
    "\n",
    "# Instantiate RNN\n",
    "rnn = SimpleRNN(input_size=10, hidden_size=50)\n",
    "rnn.eval()\n",
    "\n",
    "# Generate input sequence\n",
    "batch_size = 32\n",
    "seq_length = 100\n",
    "input_size = 10\n",
    "\n",
    "# Random input sequence\n",
    "x = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "# Run RNN and collect hidden states\n",
    "with torch.no_grad():\n",
    "    outputs, _ = rnn(x)\n",
    "\n",
    "# Extract hidden states: (batch, time, hidden_dim)\n",
    "hidden_states = outputs.numpy()\n",
    "\n",
    "print(f\"Collected hidden states: {hidden_states.shape}\")\n",
    "print(f\"  Batch size: {hidden_states.shape[0]}\")\n",
    "print(f\"  Sequence length: {hidden_states.shape[1]}\")\n",
    "print(f\"  Hidden dimension: {hidden_states.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsAnalysisPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for analyzing neural network dynamics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def analyze(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Run complete dynamical systems analysis.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: Array of shape (batch, time, hidden_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all analysis results\n",
    "        \"\"\"\n",
    "        print(\"Running Dynamical Systems Analysis...\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Reshape: (batch*time, hidden_dim)\n",
    "        batch, time, hidden_dim = hidden_states.shape\n",
    "        X_flat = hidden_states.reshape(-1, hidden_dim)\n",
    "        \n",
    "        # 1. Intrinsic Dimensionality\n",
    "        print(\"\\n1. Estimating intrinsic dimensionality...\")\n",
    "        dim_estimator = IntrinsicDimensionalityEstimator()\n",
    "        dim_results = dim_estimator.estimate_all(X_flat)\n",
    "        self.results['dimensionality'] = dim_results\n",
    "        print(f\"   Participation ratio: {dim_results['participation_ratio']:.2f}\")\n",
    "        print(f\"   PCA dimension (95%): {dim_results['pca_dimension']}\")\n",
    "        \n",
    "        # 2. DMD Analysis (use one trajectory)\n",
    "        print(\"\\n2. Running Dynamic Mode Decomposition...\")\n",
    "        X_traj = hidden_states[0].T  # (hidden_dim, time)\n",
    "        dmd = DynamicModeDecomposition(rank=10)\n",
    "        dmd.fit(X_traj)\n",
    "        dmd_props = dmd.get_mode_properties()\n",
    "        self.results['dmd'] = dmd_props\n",
    "        n_stable = np.sum(dmd_props['growth_rates'] < 0)\n",
    "        print(f\"   Found {len(dmd_props['growth_rates'])} modes\")\n",
    "        print(f\"   {n_stable} stable modes (growth rate < 0)\")\n",
    "        \n",
    "        # 3. Simplified Lyapunov analysis\n",
    "        print(\"\\n3. Estimating stability (simplified)...\")\n",
    "        # Use jacobian-based approximation\n",
    "        # Maximum eigenvalue of empirical transition matrix\n",
    "        X1 = X_flat[:-1]\n",
    "        X2 = X_flat[1:]\n",
    "        # Approximate A such that X2 ≈ A @ X1\n",
    "        A_approx = X2.T @ X1 @ np.linalg.pinv(X1.T @ X1)\n",
    "        eigvals = np.linalg.eigvals(A_approx)\n",
    "        max_eigval = np.max(np.abs(eigvals))\n",
    "        self.results['stability'] = {\n",
    "            'max_eigenvalue': max_eigval,\n",
    "            'stable': max_eigval < 1.0\n",
    "        }\n",
    "        print(f\"   Max eigenvalue: {max_eigval:.4f}\")\n",
    "        print(f\"   System is {'stable' if max_eigval < 1.0 else 'unstable'}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Analysis complete!\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def plot_summary(self):\n",
    "        \"\"\"\n",
    "        Create summary visualization of all analyses.\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Plot 1: Explained variance\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        var_ratios = self.results['dimensionality']['explained_variance_ratios'][:20]\n",
    "        ax1.bar(range(1, len(var_ratios)+1), var_ratios, alpha=0.7)\n",
    "        ax1.set_xlabel('Component')\n",
    "        ax1.set_ylabel('Explained Variance')\n",
    "        ax1.set_title('PCA: Explained Variance')\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Plot 2: DMD eigenvalues\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        dmd = self.results['dmd']\n",
    "        scatter = ax2.scatter(dmd['growth_rates'], dmd['frequencies'],\n",
    "                            s=dmd['amplitudes']*500, c=dmd['amplitudes'],\n",
    "                            cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "        ax2.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax2.set_xlabel('Growth Rate')\n",
    "        ax2.set_ylabel('Frequency (Hz)')\n",
    "        ax2.set_title('DMD: Eigenvalue Spectrum')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax2, label='Amplitude')\n",
    "        \n",
    "        # Plot 3: Mode amplitudes\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        colors = ['blue' if gr < 0 else 'red' for gr in dmd['growth_rates']]\n",
    "        ax3.bar(range(len(dmd['amplitudes'])), dmd['amplitudes'], \n",
    "               color=colors, alpha=0.7)\n",
    "        ax3.set_xlabel('Mode Index')\n",
    "        ax3.set_ylabel('Amplitude')\n",
    "        ax3.set_title('DMD: Mode Amplitudes')\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Plot 4: Summary statistics\n",
    "        ax4 = fig.add_subplot(gs[1, :])\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "        DYNAMICAL SYSTEMS ANALYSIS SUMMARY\n",
    "        {'='*60}\n",
    "        \n",
    "        DIMENSIONALITY ANALYSIS:\n",
    "        • Ambient dimension: {self.results['dimensionality']['ambient_dimension']}\n",
    "        • Participation ratio: {self.results['dimensionality']['participation_ratio']:.2f}\n",
    "        • PCA dimension (95% variance): {self.results['dimensionality']['pca_dimension']}\n",
    "        • MLE dimension estimate: {self.results['dimensionality']['mle_dimension']:.2f}\n",
    "        \n",
    "        DYNAMIC MODE DECOMPOSITION:\n",
    "        • Number of modes: {len(dmd['growth_rates'])}\n",
    "        • Stable modes (growth < 0): {np.sum(dmd['growth_rates'] < 0)}\n",
    "        • Unstable modes (growth > 0): {np.sum(dmd['growth_rates'] > 0)}\n",
    "        • Dominant mode amplitude: {np.max(dmd['amplitudes']):.4f}\n",
    "        \n",
    "        STABILITY ANALYSIS:\n",
    "        • Maximum eigenvalue: {self.results['stability']['max_eigenvalue']:.4f}\n",
    "        • System classification: {'STABLE' if self.results['stability']['stable'] else 'UNSTABLE'}\n",
    "        \n",
    "        INTERPRETATION:\n",
    "        • Low intrinsic dimension indicates efficient representation\n",
    "        • Stable modes decay over time (robust to perturbations)\n",
    "        • System stability suggests convergent dynamics\n",
    "        \"\"\"\n",
    "        \n",
    "        ax4.text(0.1, 0.5, summary_text, transform=ax4.transAxes,\n",
    "                fontsize=10, verticalalignment='center',\n",
    "                fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Dynamics analysis pipeline implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete analysis\n",
    "pipeline = DynamicsAnalysisPipeline()\n",
    "results = pipeline.analyze(hidden_states)\n",
    "\n",
    "# Visualize results\n",
    "pipeline.plot_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Dynamic Mode Decomposition**: Extract interpretable dynamic modes from neural trajectories\n",
    "   - Find dominant patterns and oscillations\n",
    "   - Predict future states\n",
    "   - Identify stable/unstable dynamics\n",
    "\n",
    "2. **Lyapunov Exponents**: Quantify chaos and stability\n",
    "   - λ < 0: Stable, convergent dynamics\n",
    "   - λ ≈ 0: Critical, edge-of-chaos\n",
    "   - λ > 0: Chaotic, divergent dynamics\n",
    "\n",
    "3. **Fixed Point Analysis**: Find computational attractors\n",
    "   - Identify stable states\n",
    "   - Characterize local stability\n",
    "   - Understand decision states\n",
    "\n",
    "4. **Intrinsic Dimensionality**: Measure representational complexity\n",
    "   - Participation ratio for effective dimensionality\n",
    "   - PCA for explained variance\n",
    "   - MLE for local dimension estimates\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Neural networks are dynamical systems**: They have rich temporal structure\n",
    "- **Low-dimensional structure is common**: High-dimensional activations often lie on low-dimensional manifolds\n",
    "- **Stability matters**: Stable dynamics lead to robust, predictable computation\n",
    "- **Edge of chaos is optimal**: Many successful networks operate near λ ≈ 0\n",
    "\n",
    "### Applications to Your Research\n",
    "\n",
    "1. **Recurrent Models**: Understand how RNNs/LSTMs process sequences\n",
    "2. **Training Dynamics**: Track how representations evolve during learning\n",
    "3. **Robustness Analysis**: Measure sensitivity to perturbations\n",
    "4. **Model Comparison**: Compare dynamics across architectures\n",
    "5. **Brain Alignment**: Match neural dynamics to brain recordings\n",
    "\n",
    "### Recommended Next Steps\n",
    "\n",
    "1. **Notebook 07**: Circuit extraction and latent RNN models\n",
    "2. **Notebook 08**: Biophysical modeling with spiking networks\n",
    "3. **Notebook 09**: Information theory and energy landscapes\n",
    "4. **Notebook 10**: Advanced topics (meta-dynamics, topology, counterfactuals)\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Kutz et al. (2016): *Dynamic Mode Decomposition*\n",
    "- Sussillo & Barak (2013): *Opening the Black Box*\n",
    "- Maheswaranathan et al. (2019): *Universality and individuality in neural dynamics*\n",
    "- Chung & Abbott (2021): *Beyond the edge of chaos*\n",
    "- Jazayeri & Afraz (2017): *Navigating the neural space*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
