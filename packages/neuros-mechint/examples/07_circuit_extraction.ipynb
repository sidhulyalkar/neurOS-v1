{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07: Circuit Extraction and Latent Models\n",
    "\n",
    "## Extracting Minimal Computational Circuits from Neural Networks\n",
    "\n",
    "This notebook explores how to **extract interpretable, minimal computational circuits** from complex neural networks. Instead of analyzing all neurons, we identify the essential computations and build simplified models that capture the core algorithmic behavior.\n",
    "\n",
    "### Why Circuit Extraction Matters\n",
    "\n",
    "1. **Interpretability**: Small circuits are easier to understand than full networks\n",
    "2. **Generalization**: Core circuits reveal what the network actually learned\n",
    "3. **Debugging**: Identify specific computational failures\n",
    "4. **Transfer**: Extract and reuse learned algorithms\n",
    "5. **Mechanistic Understanding**: Move from \"what\" to \"how\" networks compute\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Latent Circuit Models**: Extract minimal RNN-like circuits from large networks\n",
    "2. **DUNL (Disentangled and Unified Networks through Latent)**: Decompose mixed selectivity into factors\n",
    "3. **Feature Visualization**: Find optimal stimuli for neurons and circuits\n",
    "4. **Activation Maximization**: Generate inputs that maximally activate specific features\n",
    "5. **Circuit Motifs**: Identify recurring computational patterns\n",
    "6. **Recurrent Dynamics Analysis**: Understand temporal processing in circuits\n",
    "\n",
    "### References\n",
    "\n",
    "- Langdon & Engel (2025): *Latent circuit inference from data*\n",
    "- Sussillo & Barak (2013): *Opening the black box: Low-dimensional dynamics in high-dimensional RNNs*\n",
    "- Olah et al. (2018): *The building blocks of interpretability* (Distill)\n",
    "- Gu et al. (2021): *Disentangling and unifying neural representations*\n",
    "- Rigotti et al. (2013): *The importance of mixed selectivity in complex cognitive tasks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Latent Circuit Models\n",
    "\n",
    "### The Latent Circuit Framework\n",
    "\n",
    "**Key Insight**: Large neural networks often implement simple algorithms using only a small subset of their capacity.\n",
    "\n",
    "**Latent Circuit Model** extracts a minimal RNN that:\n",
    "1. Has much lower dimensionality than original network\n",
    "2. Captures essential computational structure\n",
    "3. Generalizes to new inputs\n",
    "4. Is interpretable\n",
    "\n",
    "**The extraction process**:\n",
    "```\n",
    "High-dimensional network → Low-dimensional latent circuit\n",
    "\n",
    "z(t+1) = f(W_rec @ z(t) + W_in @ u(t) + b)\n",
    "```\n",
    "\n",
    "where:\n",
    "- z: Low-dimensional latent state (e.g., 5-10 dimensions)\n",
    "- u: Input\n",
    "- W_rec: Recurrent weights (captures dynamics)\n",
    "- W_in: Input weights\n",
    "- f: Nonlinearity (tanh, ReLU, etc.)\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "- **Intrinsic dimensionality**: Neural dynamics often live on low-dimensional manifolds\n",
    "- **Task constraints**: Many tasks only require simple computations\n",
    "- **Regularization**: Networks prefer simple solutions\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. **RNN compression**: Extract minimal models from LSTMs/GRUs\n",
    "2. **Cognitive modeling**: Build mechanistic models of behavior\n",
    "3. **Transfer learning**: Extract and reuse learned circuits\n",
    "4. **Debugging**: Identify where computations fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentCircuitModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-dimensional latent circuit extracted from a larger network.\n",
    "    \n",
    "    The circuit is a minimal RNN:\n",
    "        z(t+1) = tanh(W_rec @ z(t) + W_in @ u(t) + b)\n",
    "        y(t) = W_out @ z(t)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Recurrent weights (the \"circuit\")\n",
    "        self.W_rec = nn.Parameter(torch.randn(latent_dim, latent_dim) * 0.5 / np.sqrt(latent_dim))\n",
    "        \n",
    "        # Input projection\n",
    "        self.W_in = nn.Parameter(torch.randn(latent_dim, input_dim) * 0.5 / np.sqrt(input_dim))\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_out = nn.Parameter(torch.randn(output_dim, latent_dim) * 0.5 / np.sqrt(latent_dim))\n",
    "        \n",
    "        # Bias\n",
    "        self.bias = nn.Parameter(torch.zeros(latent_dim))\n",
    "    \n",
    "    def forward(self, inputs, z0=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: (batch, time, input_dim)\n",
    "            z0: Initial latent state (batch, latent_dim)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch, time, output_dim)\n",
    "            latent_states: (batch, time, latent_dim)\n",
    "        \"\"\"\n",
    "        batch, time, _ = inputs.shape\n",
    "        device = inputs.device\n",
    "        \n",
    "        # Initialize latent state\n",
    "        if z0 is None:\n",
    "            z = torch.zeros(batch, self.latent_dim, device=device)\n",
    "        else:\n",
    "            z = z0\n",
    "        \n",
    "        # Store trajectories\n",
    "        latent_traj = []\n",
    "        output_traj = []\n",
    "        \n",
    "        # Run dynamics\n",
    "        for t in range(time):\n",
    "            # z(t+1) = tanh(W_rec @ z(t) + W_in @ u(t) + b)\n",
    "            u_t = inputs[:, t, :]\n",
    "            z = torch.tanh(z @ self.W_rec.T + u_t @ self.W_in.T + self.bias)\n",
    "            \n",
    "            # Output: y(t) = W_out @ z(t)\n",
    "            y = z @ self.W_out.T\n",
    "            \n",
    "            latent_traj.append(z)\n",
    "            output_traj.append(y)\n",
    "        \n",
    "        # Stack into tensors\n",
    "        latent_states = torch.stack(latent_traj, dim=1)\n",
    "        outputs = torch.stack(output_traj, dim=1)\n",
    "        \n",
    "        return outputs, latent_states\n",
    "    \n",
    "    def get_fixed_points(self, input_val=None, n_inits=10):\n",
    "        \"\"\"\n",
    "        Find fixed points of the circuit.\n",
    "        \n",
    "        Fixed points satisfy: z* = tanh(W_rec @ z* + W_in @ u + b)\n",
    "        \"\"\"\n",
    "        from scipy.optimize import fsolve\n",
    "        \n",
    "        W_rec_np = self.W_rec.detach().cpu().numpy()\n",
    "        W_in_np = self.W_in.detach().cpu().numpy()\n",
    "        bias_np = self.bias.detach().cpu().numpy()\n",
    "        \n",
    "        # Input contribution\n",
    "        if input_val is None:\n",
    "            input_contrib = np.zeros(self.latent_dim)\n",
    "        else:\n",
    "            input_contrib = W_in_np @ input_val\n",
    "        \n",
    "        # Define fixed point equation\n",
    "        def fp_equation(z):\n",
    "            return np.tanh(W_rec_np @ z + input_contrib + bias_np) - z\n",
    "        \n",
    "        # Try multiple initializations\n",
    "        fixed_points = []\n",
    "        for _ in range(n_inits):\n",
    "            z0 = np.random.randn(self.latent_dim) * 0.5\n",
    "            try:\n",
    "                z_star = fsolve(fp_equation, z0)\n",
    "                residual = np.linalg.norm(fp_equation(z_star))\n",
    "                if residual < 1e-6:\n",
    "                    fixed_points.append(z_star)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return fixed_points\n",
    "\n",
    "print(\"Latent circuit model implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitFitter:\n",
    "    \"\"\"\n",
    "    Fit a latent circuit model to high-dimensional neural data.\n",
    "    \n",
    "    Two-step process:\n",
    "    1. Dimensionality reduction: Find low-dimensional latent space\n",
    "    2. Dynamics fitting: Learn recurrent weights in latent space\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=10, learning_rate=1e-3):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.circuit = None\n",
    "        self.encoder = None  # Projects high-dim → latent\n",
    "        self.decoder = None  # Projects latent → high-dim\n",
    "    \n",
    "    def fit(self, inputs, neural_data, n_epochs=100, verbose=True):\n",
    "        \"\"\"\n",
    "        Fit latent circuit to neural recordings.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Task inputs (batch, time, input_dim)\n",
    "            neural_data: Neural recordings (batch, time, neural_dim)\n",
    "            n_epochs: Training epochs\n",
    "        \"\"\"\n",
    "        batch, time, input_dim = inputs.shape\n",
    "        _, _, neural_dim = neural_data.shape\n",
    "        \n",
    "        # Convert to torch\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            inputs = torch.FloatTensor(inputs)\n",
    "        if not isinstance(neural_data, torch.Tensor):\n",
    "            neural_data = torch.FloatTensor(neural_data)\n",
    "        \n",
    "        # Step 1: Initialize with PCA\n",
    "        neural_flat = neural_data.reshape(-1, neural_dim).numpy()\n",
    "        pca = PCA(n_components=self.latent_dim)\n",
    "        pca.fit(neural_flat)\n",
    "        \n",
    "        # Step 2: Create circuit model\n",
    "        self.circuit = LatentCircuitModel(\n",
    "            input_dim=input_dim,\n",
    "            latent_dim=self.latent_dim,\n",
    "            output_dim=neural_dim\n",
    "        )\n",
    "        \n",
    "        # Initialize encoder/decoder with PCA\n",
    "        self.encoder = nn.Linear(neural_dim, self.latent_dim)\n",
    "        self.decoder = nn.Linear(self.latent_dim, neural_dim)\n",
    "        self.encoder.weight.data = torch.FloatTensor(pca.components_)\n",
    "        self.decoder.weight.data = torch.FloatTensor(pca.components_.T)\n",
    "        \n",
    "        # Step 3: Optimize circuit to match neural data\n",
    "        optimizer = Adam(\n",
    "            list(self.circuit.parameters()) + \n",
    "            list(self.encoder.parameters()) + \n",
    "            list(self.decoder.parameters()),\n",
    "            lr=self.learning_rate\n",
    "        )\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(n_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Encode neural data to latent\n",
    "            z_target = self.encoder(neural_data.reshape(-1, neural_dim))\n",
    "            z_target = z_target.reshape(batch, time, self.latent_dim)\n",
    "            \n",
    "            # Run circuit\n",
    "            outputs, z_pred = self.circuit(inputs)\n",
    "            \n",
    "            # Reconstruction loss: match neural data\n",
    "            neural_pred = self.decoder(z_pred.reshape(-1, self.latent_dim))\n",
    "            neural_pred = neural_pred.reshape(batch, time, neural_dim)\n",
    "            recon_loss = F.mse_loss(neural_pred, neural_data)\n",
    "            \n",
    "            # Dynamics loss: match latent dynamics\n",
    "            dynamics_loss = F.mse_loss(z_pred, z_target)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = recon_loss + 0.5 * dynamics_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if verbose and (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def get_circuit_weights(self):\n",
    "        \"\"\"Extract circuit connectivity matrix.\"\"\"\n",
    "        return self.circuit.W_rec.detach().cpu().numpy()\n",
    "\n",
    "print(\"Circuit fitter implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data: RNN performing a simple task\n",
    "# Task: Integrate input over time\n",
    "\n",
    "def generate_integration_data(n_trials=100, seq_length=50, input_dim=5, hidden_dim=30):\n",
    "    \"\"\"\n",
    "    Generate data from an RNN performing temporal integration.\n",
    "    \"\"\"\n",
    "    # Create RNN\n",
    "    rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "    rnn.eval()\n",
    "    \n",
    "    # Generate inputs\n",
    "    inputs = torch.randn(n_trials, seq_length, input_dim) * 0.5\n",
    "    \n",
    "    # Run RNN\n",
    "    with torch.no_grad():\n",
    "        outputs, _ = rnn(inputs)\n",
    "    \n",
    "    return inputs, outputs\n",
    "\n",
    "# Generate data\n",
    "inputs, neural_data = generate_integration_data(\n",
    "    n_trials=100,\n",
    "    seq_length=50,\n",
    "    input_dim=5,\n",
    "    hidden_dim=30\n",
    ")\n",
    "\n",
    "print(f\"Generated data:\")\n",
    "print(f\"  Inputs: {inputs.shape} (trials, time, input_dim)\")\n",
    "print(f\"  Neural recordings: {neural_data.shape} (trials, time, neurons)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit latent circuit\n",
    "fitter = CircuitFitter(latent_dim=5, learning_rate=1e-3)\n",
    "losses = fitter.fit(inputs, neural_data, n_epochs=100, verbose=True)\n",
    "\n",
    "# Extract circuit\n",
    "W_rec = fitter.get_circuit_weights()\n",
    "\n",
    "print(f\"\\nExtracted circuit: {W_rec.shape[0]} latent dimensions\")\n",
    "print(f\"Original network: {neural_data.shape[2]} neurons\")\n",
    "print(f\"Compression ratio: {neural_data.shape[2] / W_rec.shape[0]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize circuit extraction results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Training loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(losses, linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Circuit Fitting: Training Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Circuit connectivity\n",
    "ax = axes[0, 1]\n",
    "im = ax.imshow(W_rec, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "ax.set_xlabel('Latent Unit (from)')\n",
    "ax.set_ylabel('Latent Unit (to)')\n",
    "ax.set_title('Extracted Circuit: Recurrent Weights')\n",
    "plt.colorbar(im, ax=ax, label='Weight')\n",
    "\n",
    "# Plot 3: Eigenvalue spectrum (stability)\n",
    "ax = axes[1, 0]\n",
    "eigvals = np.linalg.eigvals(W_rec)\n",
    "ax.scatter(eigvals.real, eigvals.imag, s=100, alpha=0.6, edgecolors='black')\n",
    "circle = plt.Circle((0, 0), 1, fill=False, color='red', linestyle='--', \n",
    "                    linewidth=2, label='Stability boundary')\n",
    "ax.add_patch(circle)\n",
    "ax.set_xlabel('Real Part')\n",
    "ax.set_ylabel('Imaginary Part')\n",
    "ax.set_title('Circuit Eigenvalues (Stability Analysis)')\n",
    "ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Example trajectory comparison\n",
    "ax = axes[1, 1]\n",
    "# Run circuit on first trial\n",
    "with torch.no_grad():\n",
    "    _, latent_traj = fitter.circuit(inputs[:1])\n",
    "    latent_traj = latent_traj.squeeze().numpy()\n",
    "\n",
    "for i in range(min(3, latent_traj.shape[1])):\n",
    "    ax.plot(latent_traj[:, i], label=f'Latent {i+1}', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Latent Activity')\n",
    "ax.set_title('Example Latent Circuit Trajectory')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Low training loss indicates good circuit fit\")\n",
    "print(\"- Connectivity matrix shows circuit wiring\")\n",
    "print(\"- Eigenvalues inside unit circle = stable dynamics\")\n",
    "print(\"- Latent trajectories show temporal evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DUNL - Disentangling Mixed Selectivity\n",
    "\n",
    "### The Mixed Selectivity Problem\n",
    "\n",
    "**Mixed selectivity**: Individual neurons respond to multiple task variables simultaneously.\n",
    "\n",
    "Example: A neuron might respond to:\n",
    "- Stimulus identity AND\n",
    "- Decision choice AND\n",
    "- Time in trial\n",
    "\n",
    "This makes interpretation difficult!\n",
    "\n",
    "### DUNL Solution\n",
    "\n",
    "**Disentangled and Unified Networks through Latent (DUNL)** decomposes mixed selectivity:\n",
    "\n",
    "```\n",
    "Neural response = Factor 1 ⊗ Factor 2 ⊗ ... ⊗ Factor K\n",
    "```\n",
    "\n",
    "where:\n",
    "- Each factor corresponds to one task variable\n",
    "- ⊗ represents tensor product (interaction)\n",
    "- Factors are disentangled (independent)\n",
    "\n",
    "**Benefits**:\n",
    "1. Interpretable factors (each = one variable)\n",
    "2. Understand interactions between variables\n",
    "3. Measure importance of each factor\n",
    "4. Predict generalization to new conditions\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "For K task variables with dimensions [d1, d2, ..., dK]:\n",
    "\n",
    "```\n",
    "x = (F1 ⊗ F2 ⊗ ... ⊗ FK) @ c + noise\n",
    "```\n",
    "\n",
    "where:\n",
    "- Fi: Factor matrices (di × rank)\n",
    "- c: Core tensor (combining coefficients)\n",
    "- x: Neural population response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DUNLModel:\n",
    "    \"\"\"\n",
    "    Disentangled and Unified Network through Latent (DUNL).\n",
    "    \n",
    "    Decomposes neural responses into task-relevant factors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_factors=2, rank=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_factors: Number of task variables\n",
    "            rank: Rank of each factor decomposition\n",
    "        \"\"\"\n",
    "        self.n_factors = n_factors\n",
    "        self.rank = rank\n",
    "        self.factors = None\n",
    "        self.core = None\n",
    "    \n",
    "    def fit(self, neural_data, task_labels, n_components=5):\n",
    "        \"\"\"\n",
    "        Fit DUNL model to neural data with task labels.\n",
    "        \n",
    "        Args:\n",
    "            neural_data: (n_samples, n_neurons)\n",
    "            task_labels: List of (n_samples,) arrays, one per factor\n",
    "            n_components: Rank for NMF\n",
    "        \"\"\"\n",
    "        n_samples, n_neurons = neural_data.shape\n",
    "        \n",
    "        # Use NMF for non-negative factorization\n",
    "        nmf = NMF(n_components=n_components, init='random', random_state=42)\n",
    "        \n",
    "        # Factor 1: Stimulus-selective components\n",
    "        W = nmf.fit_transform(neural_data)\n",
    "        H = nmf.components_\n",
    "        \n",
    "        # Store factors\n",
    "        self.factors = [W, H.T]\n",
    "        \n",
    "        # Compute selectivity scores\n",
    "        selectivity_scores = self.compute_selectivity(W, task_labels)\n",
    "        \n",
    "        return selectivity_scores\n",
    "    \n",
    "    def compute_selectivity(self, factors, task_labels):\n",
    "        \"\"\"\n",
    "        Compute how selective each factor is to task variables.\n",
    "        \n",
    "        Uses ANOVA-like measure.\n",
    "        \"\"\"\n",
    "        if len(task_labels) == 0:\n",
    "            return None\n",
    "        \n",
    "        labels = task_labels[0]\n",
    "        unique_labels = np.unique(labels)\n",
    "        \n",
    "        selectivity = np.zeros(factors.shape[1])\n",
    "        \n",
    "        for i in range(factors.shape[1]):\n",
    "            # Between-condition variance / within-condition variance\n",
    "            between_var = 0\n",
    "            within_var = 0\n",
    "            \n",
    "            for label in unique_labels:\n",
    "                mask = labels == label\n",
    "                if np.sum(mask) > 0:\n",
    "                    group_mean = factors[mask, i].mean()\n",
    "                    between_var += np.sum(mask) * (group_mean - factors[:, i].mean())**2\n",
    "                    within_var += np.sum((factors[mask, i] - group_mean)**2)\n",
    "            \n",
    "            if within_var > 0:\n",
    "                selectivity[i] = between_var / within_var\n",
    "        \n",
    "        return selectivity\n",
    "    \n",
    "    def analyze_mixing(self, neural_data):\n",
    "        \"\"\"\n",
    "        Measure degree of mixed selectivity.\n",
    "        \n",
    "        Returns:\n",
    "            mixing_score: Higher = more mixed selectivity\n",
    "        \"\"\"\n",
    "        # Compute pairwise correlations\n",
    "        corr = np.corrcoef(neural_data.T)\n",
    "        \n",
    "        # Average absolute correlation (off-diagonal)\n",
    "        mask = ~np.eye(corr.shape[0], dtype=bool)\n",
    "        mixing_score = np.abs(corr[mask]).mean()\n",
    "        \n",
    "        return mixing_score\n",
    "\n",
    "print(\"DUNL model implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with mixed selectivity\n",
    "def generate_mixed_selectivity_data(n_samples=500, n_neurons=50, n_conditions=4):\n",
    "    \"\"\"\n",
    "    Generate neural data with mixed selectivity.\n",
    "    \n",
    "    Each neuron responds to combination of:\n",
    "    - Stimulus type (4 levels)\n",
    "    - Context (2 levels)\n",
    "    \"\"\"\n",
    "    # Task variables\n",
    "    stimulus = np.random.randint(0, n_conditions, n_samples)\n",
    "    context = np.random.randint(0, 2, n_samples)\n",
    "    \n",
    "    # Generate basis responses\n",
    "    # Pure stimulus tuning\n",
    "    stim_tuning = np.random.randn(n_neurons, n_conditions)\n",
    "    \n",
    "    # Pure context tuning\n",
    "    context_tuning = np.random.randn(n_neurons, 2)\n",
    "    \n",
    "    # Generate mixed responses\n",
    "    neural_data = np.zeros((n_samples, n_neurons))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Mixed response = stimulus effect + context effect + interaction\n",
    "        stim_effect = stim_tuning[:, stimulus[i]]\n",
    "        context_effect = context_tuning[:, context[i]]\n",
    "        interaction = stim_effect * context_effect * 0.5\n",
    "        \n",
    "        neural_data[i] = stim_effect + context_effect + interaction\n",
    "    \n",
    "    # Add noise\n",
    "    neural_data += np.random.randn(*neural_data.shape) * 0.5\n",
    "    \n",
    "    # Make non-negative (like firing rates)\n",
    "    neural_data = np.maximum(neural_data, 0)\n",
    "    \n",
    "    return neural_data, [stimulus, context]\n",
    "\n",
    "# Generate data\n",
    "neural_data, task_labels = generate_mixed_selectivity_data(\n",
    "    n_samples=500,\n",
    "    n_neurons=50,\n",
    "    n_conditions=4\n",
    ")\n",
    "\n",
    "print(f\"Generated data with mixed selectivity:\")\n",
    "print(f\"  Neural data: {neural_data.shape}\")\n",
    "print(f\"  Stimulus conditions: {len(np.unique(task_labels[0]))}\")\n",
    "print(f\"  Context conditions: {len(np.unique(task_labels[1]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DUNL model\n",
    "dunl = DUNLModel(n_factors=2, rank=5)\n",
    "selectivity = dunl.fit(neural_data, task_labels, n_components=5)\n",
    "\n",
    "# Measure mixing\n",
    "mixing_score = dunl.analyze_mixing(neural_data)\n",
    "\n",
    "print(f\"\\nDUNL Analysis:\")\n",
    "print(f\"  Mixing score: {mixing_score:.3f} (higher = more mixed)\")\n",
    "print(f\"\\nFactor selectivity scores:\")\n",
    "for i, s in enumerate(selectivity):\n",
    "    print(f\"  Factor {i+1}: {s:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DUNL decomposition\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Factor loadings\n",
    "ax = axes[0, 0]\n",
    "W = dunl.factors[0]\n",
    "im = ax.imshow(W.T, aspect='auto', cmap='viridis')\n",
    "ax.set_xlabel('Sample')\n",
    "ax.set_ylabel('Factor')\n",
    "ax.set_title('DUNL: Factor Loadings Across Samples')\n",
    "plt.colorbar(im, ax=ax, label='Loading')\n",
    "\n",
    "# Plot 2: Selectivity scores\n",
    "ax = axes[0, 1]\n",
    "ax.bar(range(len(selectivity)), selectivity, alpha=0.7, color='steelblue')\n",
    "ax.set_xlabel('Factor')\n",
    "ax.set_ylabel('Selectivity Score')\n",
    "ax.set_title('Factor Selectivity to Task Variables')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Factor correlation matrix\n",
    "ax = axes[1, 0]\n",
    "factor_corr = np.corrcoef(W.T)\n",
    "im = ax.imshow(factor_corr, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "ax.set_xlabel('Factor')\n",
    "ax.set_ylabel('Factor')\n",
    "ax.set_title('Factor Correlation Matrix\\n(Disentanglement: low off-diagonal)')\n",
    "plt.colorbar(im, ax=ax, label='Correlation')\n",
    "\n",
    "# Plot 4: Example factor activations by condition\n",
    "ax = axes[1, 1]\n",
    "stimulus_labels = task_labels[0]\n",
    "for cond in range(4):\n",
    "    mask = stimulus_labels == cond\n",
    "    if np.sum(mask) > 0:\n",
    "        mean_activation = W[mask, 0].mean()\n",
    "        std_activation = W[mask, 0].std()\n",
    "        ax.bar(cond, mean_activation, yerr=std_activation, alpha=0.7,\n",
    "              label=f'Stimulus {cond+1}')\n",
    "\n",
    "ax.set_xlabel('Stimulus Condition')\n",
    "ax.set_ylabel('Factor 1 Activation')\n",
    "ax.set_title('Factor 1: Stimulus Selectivity')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Factor loadings show how each factor varies across samples\")\n",
    "print(\"- High selectivity = factor strongly represents task variable\")\n",
    "print(\"- Low factor correlation = successful disentanglement\")\n",
    "print(\"- Condition-specific activations reveal tuning properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Visualization and Activation Maximization\n",
    "\n",
    "### What is Feature Visualization?\n",
    "\n",
    "**Goal**: Find the input that maximally activates a specific neuron or feature.\n",
    "\n",
    "Instead of analyzing responses to existing stimuli, we **generate** optimal stimuli:\n",
    "\n",
    "```\n",
    "x* = argmax_x f(x; neuron_i)\n",
    "```\n",
    "\n",
    "where:\n",
    "- x: Input (e.g., image, sequence)\n",
    "- f(x; neuron_i): Activation of neuron i given input x\n",
    "- x*: Optimal stimulus\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "1. **Understand selectivity**: What does this neuron \"look for\"?\n",
    "2. **Interpretability**: Optimal stimuli are often interpretable\n",
    "3. **Debugging**: Identify unexpected selectivity patterns\n",
    "4. **Adversarial robustness**: Find edge cases\n",
    "\n",
    "### Optimization Methods\n",
    "\n",
    "1. **Gradient Ascent**: Iteratively adjust input to maximize activation\n",
    "2. **Regularization**: Add constraints (naturalness, smoothness)\n",
    "3. **Diversity**: Generate multiple different optimal stimuli\n",
    "\n",
    "### Applications\n",
    "\n",
    "- **Vision**: Visualize what CNN filters detect\n",
    "- **Language**: Find phrases that activate concepts\n",
    "- **Neuroscience**: Design stimuli for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureVisualizer:\n",
    "    \"\"\"\n",
    "    Generate optimal stimuli that maximally activate specific features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, layer_name=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Neural network model\n",
    "            layer_name: Which layer to visualize (if None, use output)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.layer_name = layer_name\n",
    "    \n",
    "    def visualize_neuron(self, neuron_idx, input_shape, \n",
    "                        n_iterations=200, learning_rate=0.1,\n",
    "                        l2_penalty=0.01):\n",
    "        \"\"\"\n",
    "        Generate optimal stimulus for specific neuron.\n",
    "        \n",
    "        Args:\n",
    "            neuron_idx: Index of neuron to visualize\n",
    "            input_shape: Shape of input (e.g., (1, time, features))\n",
    "            n_iterations: Number of optimization steps\n",
    "            learning_rate: Step size\n",
    "            l2_penalty: Regularization strength\n",
    "        \n",
    "        Returns:\n",
    "            optimal_input: Input that maximally activates neuron\n",
    "            activations: Activation trajectory during optimization\n",
    "        \"\"\"\n",
    "        # Initialize input randomly\n",
    "        optimal_input = torch.randn(input_shape, requires_grad=True)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = Adam([optimal_input], lr=learning_rate)\n",
    "        \n",
    "        activations = []\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            if hasattr(self.model, 'forward'):\n",
    "                output = self.model(optimal_input)\n",
    "                if isinstance(output, tuple):\n",
    "                    output = output[0]\n",
    "            else:\n",
    "                output = self.model(optimal_input)\n",
    "            \n",
    "            # Get target neuron activation\n",
    "            # Mean across time/batch dimensions\n",
    "            activation = output[..., neuron_idx].mean()\n",
    "            \n",
    "            # L2 regularization (keep inputs reasonable)\n",
    "            l2_reg = l2_penalty * (optimal_input ** 2).mean()\n",
    "            \n",
    "            # Objective: maximize activation, minimize L2\n",
    "            loss = -activation + l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            activations.append(activation.item())\n",
    "        \n",
    "        return optimal_input.detach(), activations\n",
    "    \n",
    "    def visualize_multiple_neurons(self, neuron_indices, input_shape, **kwargs):\n",
    "        \"\"\"\n",
    "        Visualize multiple neurons simultaneously.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping neuron_idx → (optimal_input, activations)\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for neuron_idx in neuron_indices:\n",
    "            opt_input, activations = self.visualize_neuron(\n",
    "                neuron_idx, input_shape, **kwargs\n",
    "            )\n",
    "            results[neuron_idx] = (opt_input, activations)\n",
    "        return results\n",
    "\n",
    "print(\"Feature visualizer implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model to visualize\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, input_dim=10, hidden_dim=20, output_dim=5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = torch.tanh(self.fc1(x))\n",
    "        y = self.fc2(h)\n",
    "        return h, y  # Return both hidden and output\n",
    "\n",
    "# Instantiate model\n",
    "model = SimpleFFN(input_dim=10, hidden_dim=20, output_dim=5)\n",
    "model.eval()\n",
    "\n",
    "print(\"Created model for feature visualization\")\n",
    "print(f\"  Input dim: 10\")\n",
    "print(f\"  Hidden dim: 20\")\n",
    "print(f\"  Output dim: 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple hidden neurons\n",
    "visualizer = FeatureVisualizer(model)\n",
    "\n",
    "# Visualize neurons 0, 5, 10\n",
    "neuron_indices = [0, 5, 10]\n",
    "results = visualizer.visualize_multiple_neurons(\n",
    "    neuron_indices,\n",
    "    input_shape=(1, 10),\n",
    "    n_iterations=200,\n",
    "    learning_rate=0.1,\n",
    "    l2_penalty=0.01\n",
    ")\n",
    "\n",
    "print(\"Feature visualization complete!\")\n",
    "for neuron_idx in neuron_indices:\n",
    "    opt_input, activations = results[neuron_idx]\n",
    "    final_activation = activations[-1]\n",
    "    print(f\"  Neuron {neuron_idx}: Final activation = {final_activation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for plot_idx, neuron_idx in enumerate(neuron_indices):\n",
    "    opt_input, activations = results[neuron_idx]\n",
    "    \n",
    "    # Plot activation trajectory\n",
    "    ax = axes[0, plot_idx]\n",
    "    ax.plot(activations, linewidth=2, color='steelblue')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Activation')\n",
    "    ax.set_title(f'Neuron {neuron_idx}: Optimization Trajectory')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot optimal input pattern\n",
    "    ax = axes[1, plot_idx]\n",
    "    input_pattern = opt_input.squeeze().numpy()\n",
    "    ax.bar(range(len(input_pattern)), input_pattern, alpha=0.7, color='coral')\n",
    "    ax.set_xlabel('Input Dimension')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title(f'Optimal Input for Neuron {neuron_idx}')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Activation increases during optimization\")\n",
    "print(\"- Optimal input shows what features the neuron prefers\")\n",
    "print(\"- Different neurons have different preferred patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Latent Circuit Models**: Extract minimal, interpretable circuits\n",
    "   - Dimensionality reduction to find latent space\n",
    "   - Learn recurrent dynamics in latent space\n",
    "   - Achieve massive compression while preserving computation\n",
    "\n",
    "2. **DUNL Decomposition**: Disentangle mixed selectivity\n",
    "   - Separate responses into task-relevant factors\n",
    "   - Measure selectivity to task variables\n",
    "   - Understand interactions between factors\n",
    "\n",
    "3. **Feature Visualization**: Generate optimal stimuli\n",
    "   - Gradient-based optimization\n",
    "   - Understand neuron selectivity\n",
    "   - Design targeted experiments\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Circuits are sparse**: Most computation uses few dimensions\n",
    "- **Mixed selectivity is common**: But can be disentangled\n",
    "- **Visualization reveals function**: Optimal stimuli show what neurons compute\n",
    "- **Interpretability requires simplification**: Extract minimal models\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. **Model Compression**: Deploy circuits instead of full networks\n",
    "2. **Neuroscience**: Design optimal experimental stimuli\n",
    "3. **Debugging**: Identify where computations fail\n",
    "4. **Knowledge Extraction**: Transfer learned algorithms to new domains\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Notebook 08**: Biophysical modeling with spiking networks\n",
    "2. **Notebook 09**: Information theory and energy landscapes\n",
    "3. **Notebook 10**: Advanced topics (meta-dynamics, topology, counterfactuals)\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Langdon & Engel (2025): *Latent circuit inference*\n",
    "- Sussillo & Barak (2013): *Opening the black box*\n",
    "- Olah et al. (2018): *Feature visualization* (Distill)\n",
    "- Rigotti et al. (2013): *Mixed selectivity in cognitive tasks*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
