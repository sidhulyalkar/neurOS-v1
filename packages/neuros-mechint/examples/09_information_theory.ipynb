{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 09: Information Theory and Energy Landscapes\n",
    "\n",
    "## Understanding Neural Networks Through Information and Energy\n",
    "\n",
    "This notebook explores how **information theory** and **energy landscape** concepts reveal how neural networks process, compress, and represent information.\n",
    "\n",
    "### Why Information Theory Matters\n",
    "\n",
    "1. **Information Flow**: Track how information propagates through layers\n",
    "2. **Compression**: Understand how networks compress inputs into representations\n",
    "3. **Generalization**: Information theory predicts generalization performance\n",
    "4. **Energy Landscapes**: Characterize optimization and learning dynamics\n",
    "5. **Interpretability**: Information measures reveal what networks \"know\"\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Mutual Information**: Measuring shared information between variables\n",
    "2. **Information Plane**: Visualizing compression and prediction\n",
    "3. **MINE (Mutual Information Neural Estimation)**: Scalable MI estimation\n",
    "4. **Energy Landscapes**: Characterizing loss surfaces\n",
    "5. **Basin Detection**: Finding attractors in representation space\n",
    "6. **Entropy Production**: Measuring information processing\n",
    "\n",
    "### References\n",
    "\n",
    "- Tishby & Zaslavsky (2015): *Deep learning and the information bottleneck principle*\n",
    "- Shwartz-Ziv & Tishby (2017): *Opening the black box of deep neural networks via information*\n",
    "- Belghazi et al. (2018): *MINE: Mutual Information Neural Estimation*\n",
    "- Saxe et al. (2019): *On the information bottleneck theory of deep learning*\n",
    "- Goldfeld & Polyanskiy (2020): *The information bottleneck problem and its applications*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mutual Information Estimation\n",
    "\n",
    "### What is Mutual Information?\n",
    "\n",
    "**Mutual Information (MI)** measures how much knowing one variable tells you about another:\n",
    "\n",
    "```\n",
    "I(X; Y) = H(Y) - H(Y|X)\n",
    "        = H(X) + H(Y) - H(X, Y)\n",
    "```\n",
    "\n",
    "where:\n",
    "- H(Y): Entropy of Y (uncertainty)\n",
    "- H(Y|X): Conditional entropy (uncertainty remaining after observing X)\n",
    "- I(X; Y): Mutual information (shared information)\n",
    "\n",
    "**Properties**:\n",
    "- I(X; Y) ≥ 0 (always non-negative)\n",
    "- I(X; Y) = 0 iff X and Y are independent\n",
    "- I(X; Y) = I(Y; X) (symmetric)\n",
    "- I(X; X) = H(X) (information in X about itself)\n",
    "\n",
    "### Why MI Matters for Neural Networks\n",
    "\n",
    "1. **Input-Output Information**: I(X; Y) measures how much Y depends on X\n",
    "2. **Hidden Representations**: I(X; H) measures information preserved in hidden layer H\n",
    "3. **Information Bottleneck**: Trade-off between I(X; H) (compression) and I(H; Y) (prediction)\n",
    "4. **Layer-wise Analysis**: Track information flow through network\n",
    "\n",
    "### Estimation Methods\n",
    "\n",
    "1. **Binning**: Discretize and compute empirical distributions (simple but biased)\n",
    "2. **k-NN**: Use nearest neighbors (consistent but slow)\n",
    "3. **MINE**: Neural network estimator (scalable to high dimensions)\n",
    "4. **Kernel Density**: Estimate densities with kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutualInformationEstimator:\n",
    "    \"\"\"\n",
    "    Estimate mutual information using k-nearest neighbors.\n",
    "    \n",
    "    Based on Kraskov et al. (2004) estimator.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k: Number of nearest neighbors\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "    \n",
    "    def estimate(self, X, Y):\n",
    "        \"\"\"\n",
    "        Estimate I(X; Y) using k-NN method.\n",
    "        \n",
    "        Args:\n",
    "            X: (n_samples, dim_x)\n",
    "            Y: (n_samples, dim_y)\n",
    "        \n",
    "        Returns:\n",
    "            mi: Estimated mutual information (nats)\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Concatenate X and Y\n",
    "        XY = np.hstack([X, Y])\n",
    "        \n",
    "        # Find k-nearest neighbors in joint space\n",
    "        nbrs_xy = NearestNeighbors(n_neighbors=self.k+1, metric='chebyshev')\n",
    "        nbrs_xy.fit(XY)\n",
    "        distances_xy, _ = nbrs_xy.kneighbors(XY)\n",
    "        epsilon = distances_xy[:, -1]  # Distance to k-th neighbor\n",
    "        \n",
    "        # Count neighbors in X and Y spaces within epsilon\n",
    "        nbrs_x = NearestNeighbors(radius=1.0, metric='chebyshev')\n",
    "        nbrs_x.fit(X)\n",
    "        \n",
    "        nbrs_y = NearestNeighbors(radius=1.0, metric='chebyshev')\n",
    "        nbrs_y.fit(Y)\n",
    "        \n",
    "        nx = np.zeros(n_samples)\n",
    "        ny = np.zeros(n_samples)\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Count neighbors in X space within epsilon[i]\n",
    "            nbrs_x.set_params(radius=epsilon[i])\n",
    "            nx[i] = len(nbrs_x.radius_neighbors([X[i]], return_distance=False)[0]) - 1\n",
    "            \n",
    "            # Count neighbors in Y space within epsilon[i]\n",
    "            nbrs_y.set_params(radius=epsilon[i])\n",
    "            ny[i] = len(nbrs_y.radius_neighbors([Y[i]], return_distance=False)[0]) - 1\n",
    "        \n",
    "        # Kraskov estimator\n",
    "        # I(X;Y) ≈ ψ(k) - <ψ(nx+1) + ψ(ny+1)> + ψ(N)\n",
    "        psi_k = np.log(self.k)\n",
    "        psi_n = np.log(n_samples)\n",
    "        psi_nx = np.log(nx + 1)\n",
    "        psi_ny = np.log(ny + 1)\n",
    "        \n",
    "        mi = psi_k - np.mean(psi_nx + psi_ny) + psi_n\n",
    "        \n",
    "        return max(mi, 0)  # MI should be non-negative\n",
    "    \n",
    "    def estimate_conditional(self, X, Y, Z):\n",
    "        \"\"\"\n",
    "        Estimate conditional MI: I(X; Y | Z)\n",
    "        \n",
    "        I(X; Y | Z) = I(X, Z; Y) - I(Z; Y)\n",
    "        \"\"\"\n",
    "        XZ = np.hstack([X, Z])\n",
    "        \n",
    "        mi_xz_y = self.estimate(XZ, Y)\n",
    "        mi_z_y = self.estimate(Z, Y)\n",
    "        \n",
    "        return max(mi_xz_y - mi_z_y, 0)\n",
    "\n",
    "print(\"Mutual information estimator (k-NN) implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MI estimator on known distributions\n",
    "\n",
    "def test_mi_estimator():\n",
    "    \"\"\"\n",
    "    Test on cases with known MI:\n",
    "    1. Independent: I(X; Y) = 0\n",
    "    2. Perfectly correlated: I(X; Y) = H(X)\n",
    "    3. Noisy correlation: I(X; Y) depends on noise level\n",
    "    \"\"\"\n",
    "    mi_est = MutualInformationEstimator(k=5)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Case 1: Independent\n",
    "    X_ind = np.random.randn(n_samples, 1)\n",
    "    Y_ind = np.random.randn(n_samples, 1)\n",
    "    mi_ind = mi_est.estimate(X_ind, Y_ind)\n",
    "    \n",
    "    # Case 2: Y = X (perfect correlation)\n",
    "    X_corr = np.random.randn(n_samples, 1)\n",
    "    Y_corr = X_corr.copy()\n",
    "    mi_corr = mi_est.estimate(X_corr, Y_corr)\n",
    "    \n",
    "    # Theoretical: I(X; X) = H(X) ≈ 0.5*log(2πe*σ²)\n",
    "    h_x_theoretical = 0.5 * np.log(2 * np.pi * np.e * 1.0)  # σ² = 1\n",
    "    \n",
    "    # Case 3: Y = X + noise\n",
    "    X_noisy = np.random.randn(n_samples, 1)\n",
    "    noise_levels = [0.1, 0.5, 1.0, 2.0]\n",
    "    mi_noisy = []\n",
    "    \n",
    "    for noise_std in noise_levels:\n",
    "        Y_noisy = X_noisy + np.random.randn(n_samples, 1) * noise_std\n",
    "        mi = mi_est.estimate(X_noisy, Y_noisy)\n",
    "        mi_noisy.append(mi)\n",
    "    \n",
    "    return {\n",
    "        'independent': mi_ind,\n",
    "        'perfect': mi_corr,\n",
    "        'theoretical_perfect': h_x_theoretical,\n",
    "        'noisy': mi_noisy,\n",
    "        'noise_levels': noise_levels\n",
    "    }\n",
    "\n",
    "# Run tests\n",
    "results = test_mi_estimator()\n",
    "\n",
    "print(\"Mutual Information Estimation Tests:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n1. Independent variables (should be ≈ 0):\")\n",
    "print(f\"   Estimated: {results['independent']:.4f} nats\")\n",
    "print(f\"\\n2. Perfect correlation Y=X (should be ≈ H(X)):\")\n",
    "print(f\"   Estimated: {results['perfect']:.4f} nats\")\n",
    "print(f\"   Theoretical H(X): {results['theoretical_perfect']:.4f} nats\")\n",
    "print(f\"\\n3. Noisy correlation Y=X+noise:\")\n",
    "for noise, mi in zip(results['noise_levels'], results['noisy']):\n",
    "    print(f\"   Noise std={noise:.1f}: MI = {mi:.4f} nats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MI vs noise level\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(results['noise_levels'], results['noisy'], 'o-', linewidth=2, \n",
    "        markersize=8, label='Estimated MI')\n",
    "ax.axhline(y=results['theoretical_perfect'], color='red', linestyle='--', \n",
    "           linewidth=2, label=f'Perfect correlation (H(X)={results[\"theoretical_perfect\"]:.2f})')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1, label='Independence')\n",
    "\n",
    "ax.set_xlabel('Noise Standard Deviation')\n",
    "ax.set_ylabel('Mutual Information (nats)')\n",
    "ax.set_title('Mutual Information vs Noise Level\\n(Y = X + Gaussian Noise)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- As noise increases, mutual information decreases\")\n",
    "print(\"- Perfect correlation: MI = entropy of X\")\n",
    "print(\"- High noise: MI approaches 0 (independence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: MINE - Mutual Information Neural Estimation\n",
    "\n",
    "### Scalable MI Estimation with Neural Networks\n",
    "\n",
    "**Problem**: k-NN methods are slow and don't scale to high dimensions.\n",
    "\n",
    "**Solution**: MINE learns a neural network to estimate MI.\n",
    "\n",
    "### The Donsker-Varadhan Representation\n",
    "\n",
    "Mutual information can be expressed as:\n",
    "\n",
    "```\n",
    "I(X; Y) = sup_T E_P[T(x,y)] - log E_Q[exp(T(x,y))]\n",
    "```\n",
    "\n",
    "where:\n",
    "- T: Any function (\"statistics network\")\n",
    "- P: Joint distribution P(X, Y)\n",
    "- Q: Product of marginals P(X)P(Y)\n",
    "- sup: Supremum over all functions T\n",
    "\n",
    "### MINE Algorithm\n",
    "\n",
    "1. **Parameterize** T with a neural network\n",
    "2. **Sample**:\n",
    "   - Joint samples: (x, y) ~ P(X, Y)\n",
    "   - Marginal samples: (x, y') where y' ~ P(Y) independently\n",
    "3. **Optimize**: Maximize the MINE objective\n",
    "4. **Estimate**: Use trained T to estimate MI\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Scales to high dimensions\n",
    "- Fast after training\n",
    "- Can handle continuous variables\n",
    "- Differentiable (can backprop through MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MINENetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Statistics network for MINE (Mutual Information Neural Estimation).\n",
    "    \n",
    "    T(x, y) = neural_network([x, y])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x_dim, y_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(x_dim + y_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, x_dim)\n",
    "            y: (batch, y_dim)\n",
    "        \n",
    "        Returns:\n",
    "            T(x, y): (batch, 1) statistics\n",
    "        \"\"\"\n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        return self.net(xy)\n",
    "\n",
    "\n",
    "class MINEEstimator:\n",
    "    \"\"\"\n",
    "    MINE: Mutual Information Neural Estimation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x_dim, y_dim, hidden_dim=64, lr=1e-3):\n",
    "        self.network = MINENetwork(x_dim, y_dim, hidden_dim)\n",
    "        self.optimizer = Adam(self.network.parameters(), lr=lr)\n",
    "        self.mi_history = []\n",
    "    \n",
    "    def train_step(self, x_joint, y_joint, x_marginal, y_marginal):\n",
    "        \"\"\"\n",
    "        Single training step.\n",
    "        \n",
    "        Args:\n",
    "            x_joint, y_joint: Samples from joint P(X, Y)\n",
    "            x_marginal, y_marginal: Samples from marginals P(X)P(Y)\n",
    "        \n",
    "        Returns:\n",
    "            mi_estimate: Current MI estimate\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Statistics on joint\n",
    "        t_joint = self.network(x_joint, y_joint)\n",
    "        \n",
    "        # Statistics on marginals\n",
    "        t_marginal = self.network(x_marginal, y_marginal)\n",
    "        \n",
    "        # MINE objective: E_P[T] - log E_Q[exp(T)]\n",
    "        # Use exponential moving average for stability\n",
    "        mi_estimate = t_joint.mean() - torch.log(torch.exp(t_marginal).mean())\n",
    "        \n",
    "        # Maximize MI = minimize negative MI\n",
    "        loss = -mi_estimate\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return mi_estimate.item()\n",
    "    \n",
    "    def fit(self, X, Y, n_epochs=1000, batch_size=256, verbose=True):\n",
    "        \"\"\"\n",
    "        Train MINE to estimate I(X; Y).\n",
    "        \n",
    "        Args:\n",
    "            X: (n_samples, x_dim)\n",
    "            Y: (n_samples, y_dim)\n",
    "            n_epochs: Training epochs\n",
    "            batch_size: Batch size\n",
    "        \"\"\"\n",
    "        # Convert to tensors\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.FloatTensor(X)\n",
    "        if not isinstance(Y, torch.Tensor):\n",
    "            Y = torch.FloatTensor(Y)\n",
    "        \n",
    "        n_samples = len(X)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Sample batch from joint\n",
    "            indices = torch.randperm(n_samples)[:batch_size]\n",
    "            x_joint = X[indices]\n",
    "            y_joint = Y[indices]\n",
    "            \n",
    "            # Sample batch from marginals (shuffle y)\n",
    "            marginal_indices = torch.randperm(n_samples)[:batch_size]\n",
    "            x_marginal = X[indices]\n",
    "            y_marginal = Y[marginal_indices]  # Shuffled Y\n",
    "            \n",
    "            # Train step\n",
    "            mi = self.train_step(x_joint, y_joint, x_marginal, y_marginal)\n",
    "            self.mi_history.append(mi)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{n_epochs}, MI estimate: {mi:.4f}\")\n",
    "    \n",
    "    def estimate(self, X, Y, batch_size=256):\n",
    "        \"\"\"\n",
    "        Estimate MI using trained network.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X = torch.FloatTensor(X)\n",
    "        if not isinstance(Y, torch.Tensor):\n",
    "            Y = torch.FloatTensor(Y)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            n_samples = len(X)\n",
    "            \n",
    "            # Joint\n",
    "            t_joint = self.network(X[:batch_size], Y[:batch_size]).mean()\n",
    "            \n",
    "            # Marginal (shuffle Y)\n",
    "            marginal_indices = torch.randperm(n_samples)[:batch_size]\n",
    "            t_marginal = self.network(X[:batch_size], Y[marginal_indices])\n",
    "            \n",
    "            mi = t_joint - torch.log(torch.exp(t_marginal).mean())\n",
    "        \n",
    "        return mi.item()\n",
    "\n",
    "print(\"MINE estimator implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MINE on synthetic data\n",
    "n_samples = 2000\n",
    "\n",
    "# Create correlated data: Y = 0.8*X + 0.6*noise\n",
    "X_mine = np.random.randn(n_samples, 1)\n",
    "noise = np.random.randn(n_samples, 1)\n",
    "Y_mine = 0.8 * X_mine + 0.6 * noise\n",
    "\n",
    "# Train MINE\n",
    "mine = MINEEstimator(x_dim=1, y_dim=1, hidden_dim=32, lr=1e-3)\n",
    "mine.fit(X_mine, Y_mine, n_epochs=500, batch_size=256, verbose=True)\n",
    "\n",
    "# Get final estimate\n",
    "mi_mine = mine.estimate(X_mine, Y_mine)\n",
    "\n",
    "# Compare to k-NN\n",
    "mi_knn_estimator = MutualInformationEstimator(k=5)\n",
    "mi_knn = mi_knn_estimator.estimate(X_mine, Y_mine)\n",
    "\n",
    "print(f\"\\nFinal MI Estimates:\")\n",
    "print(f\"  MINE: {mi_mine:.4f} nats\")\n",
    "print(f\"  k-NN: {mi_knn:.4f} nats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MINE training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: MI estimate over training\n",
    "ax = axes[0]\n",
    "ax.plot(mine.mi_history, linewidth=2, alpha=0.7)\n",
    "ax.axhline(y=mi_knn, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'k-NN estimate: {mi_knn:.3f}')\n",
    "ax.set_xlabel('Training Iteration')\n",
    "ax.set_ylabel('MI Estimate (nats)')\n",
    "ax.set_title('MINE Training: MI Convergence')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Data scatter\n",
    "ax = axes[1]\n",
    "ax.scatter(X_mine[:500], Y_mine[:500], alpha=0.5, s=20)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title(f'Data: Y = 0.8X + 0.6×noise\\nMI ≈ {mi_mine:.3f} nats')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- MINE converges to stable MI estimate\")\n",
    "print(\"- Agreement with k-NN validates estimate\")\n",
    "print(\"- Scatter plot shows correlation structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Information Plane Analysis\n",
    "\n",
    "### The Information Plane\n",
    "\n",
    "**Information Plane** visualizes learning dynamics by plotting:\n",
    "- **X-axis**: I(X; T) - Information between input X and hidden layer T\n",
    "- **Y-axis**: I(T; Y) - Information between hidden layer T and output Y\n",
    "\n",
    "### Two Phases of Learning (Tishby & Zaslavsky)\n",
    "\n",
    "1. **Fitting Phase**: Both I(X; T) and I(T; Y) increase\n",
    "   - Network learns to represent input\n",
    "   - Predictions improve\n",
    "\n",
    "2. **Compression Phase**: I(X; T) decreases while I(T; Y) stays high\n",
    "   - Network compresses representation\n",
    "   - Removes irrelevant information\n",
    "   - Improves generalization\n",
    "\n",
    "### Information Bottleneck Principle\n",
    "\n",
    "Optimal representations maximize:\n",
    "```\n",
    "L = I(T; Y) - β * I(T; X)\n",
    "```\n",
    "\n",
    "where:\n",
    "- I(T; Y): Prediction (maximize)\n",
    "- I(T; X): Compression (minimize)\n",
    "- β: Trade-off parameter\n",
    "\n",
    "**Goal**: Find minimal sufficient statistics for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationPlaneAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze neural networks in the information plane.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_mine=True, k=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            use_mine: Use MINE (fast) or k-NN (accurate)\n",
    "            k: Number of neighbors for k-NN\n",
    "        \"\"\"\n",
    "        self.use_mine = use_mine\n",
    "        self.k = k\n",
    "    \n",
    "    def compute_layer_mi(self, X, layer_activations, Y):\n",
    "        \"\"\"\n",
    "        Compute I(X; T) and I(T; Y) for a hidden layer.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            layer_activations: Hidden layer activations\n",
    "            Y: Output labels\n",
    "        \n",
    "        Returns:\n",
    "            mi_xt: I(X; T)\n",
    "            mi_ty: I(T; Y)\n",
    "        \"\"\"\n",
    "        if self.use_mine:\n",
    "            # Use MINE (fast but requires training)\n",
    "            mine_xt = MINEEstimator(X.shape[1], layer_activations.shape[1], hidden_dim=32)\n",
    "            mine_xt.fit(X, layer_activations, n_epochs=200, verbose=False)\n",
    "            mi_xt = mine_xt.estimate(X, layer_activations)\n",
    "            \n",
    "            mine_ty = MINEEstimator(layer_activations.shape[1], Y.shape[1], hidden_dim=32)\n",
    "            mine_ty.fit(layer_activations, Y, n_epochs=200, verbose=False)\n",
    "            mi_ty = mine_ty.estimate(layer_activations, Y)\n",
    "        else:\n",
    "            # Use k-NN (slower but no training)\n",
    "            estimator = MutualInformationEstimator(k=self.k)\n",
    "            \n",
    "            # Reduce dimensionality if needed (k-NN doesn't scale well)\n",
    "            if layer_activations.shape[1] > 10:\n",
    "                pca = PCA(n_components=10)\n",
    "                layer_activations = pca.fit_transform(layer_activations)\n",
    "            \n",
    "            mi_xt = estimator.estimate(X, layer_activations)\n",
    "            mi_ty = estimator.estimate(layer_activations, Y)\n",
    "        \n",
    "        return mi_xt, mi_ty\n",
    "    \n",
    "    def analyze_network(self, model, X, Y):\n",
    "        \"\"\"\n",
    "        Analyze all layers of a network.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping layer_name → (I(X;T), I(T;Y))\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Convert inputs\n",
    "        if not isinstance(X, torch.Tensor):\n",
    "            X_torch = torch.FloatTensor(X)\n",
    "        else:\n",
    "            X_torch = X\n",
    "        \n",
    "        # Forward pass and collect activations\n",
    "        activations = {}\n",
    "        \n",
    "        def hook_fn(name):\n",
    "            def hook(module, input, output):\n",
    "                activations[name] = output.detach().cpu().numpy()\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks\n",
    "        hooks = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear) and 'output' not in name:\n",
    "                hooks.append(module.register_forward_hook(hook_fn(name)))\n",
    "        \n",
    "        # Run forward pass\n",
    "        with torch.no_grad():\n",
    "            _ = model(X_torch)\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        # Compute MI for each layer\n",
    "        for layer_name, layer_acts in activations.items():\n",
    "            print(f\"Analyzing layer: {layer_name}...\")\n",
    "            mi_xt, mi_ty = self.compute_layer_mi(X, layer_acts, Y)\n",
    "            results[layer_name] = (mi_xt, mi_ty)\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"Information plane analyzer implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple classification network\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=10, hidden_dims=[20, 15, 10], output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Generate synthetic classification data\n",
    "n_samples = 500\n",
    "input_dim = 10\n",
    "\n",
    "# Two gaussian clusters\n",
    "X_class1 = np.random.randn(n_samples//2, input_dim) + np.array([2, 2, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "X_class2 = np.random.randn(n_samples//2, input_dim) + np.array([-2, -2, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "X_info = np.vstack([X_class1, X_class2])\n",
    "Y_info = np.vstack([np.zeros((n_samples//2, 1)), np.ones((n_samples//2, 1))])\n",
    "\n",
    "# One-hot encode Y\n",
    "Y_info_onehot = np.zeros((n_samples, 2))\n",
    "Y_info_onehot[np.arange(n_samples), Y_info.flatten().astype(int)] = 1\n",
    "\n",
    "print(f\"Created classification data:\")\n",
    "print(f\"  Input dimension: {X_info.shape[1]}\")\n",
    "print(f\"  Number of samples: {X_info.shape[0]}\")\n",
    "print(f\"  Number of classes: 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network\n",
    "model_info = SimpleClassifier(input_dim=10, hidden_dims=[20, 15, 10], output_dim=2)\n",
    "optimizer = Adam(model_info.parameters(), lr=1e-3)\n",
    "\n",
    "X_tensor = torch.FloatTensor(X_info)\n",
    "Y_tensor = torch.FloatTensor(Y_info_onehot)\n",
    "\n",
    "print(\"Training network...\")\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model_info(X_tensor)\n",
    "    loss = F.mse_loss(output, Y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze in information plane (using MINE for speed)\n",
    "analyzer = InformationPlaneAnalyzer(use_mine=True)\n",
    "info_results = analyzer.analyze_network(model_info, X_info, Y_info_onehot)\n",
    "\n",
    "print(\"\\nInformation Plane Analysis:\")\n",
    "print(\"=\"*50)\n",
    "for layer_name, (mi_xt, mi_ty) in info_results.items():\n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    print(f\"  I(X; T) = {mi_xt:.4f} nats\")\n",
    "    print(f\"  I(T; Y) = {mi_ty:.4f} nats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize information plane\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Extract coordinates\n",
    "layer_names = list(info_results.keys())\n",
    "mi_xt_values = [info_results[name][0] for name in layer_names]\n",
    "mi_ty_values = [info_results[name][1] for name in layer_names]\n",
    "\n",
    "# Plot trajectory through layers\n",
    "ax.plot(mi_xt_values, mi_ty_values, 'o-', linewidth=2, markersize=10, \n",
    "        color='steelblue', label='Layer trajectory')\n",
    "\n",
    "# Label points\n",
    "for i, name in enumerate(layer_names):\n",
    "    ax.annotate(f'L{i}', (mi_xt_values[i], mi_ty_values[i]),\n",
    "               xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('I(X; T) - Information Preserved (nats)', fontsize=12)\n",
    "ax.set_ylabel('I(T; Y) - Prediction Information (nats)', fontsize=12)\n",
    "ax.set_title('Information Plane: Layer-by-Layer Analysis\\n(Higher layers should compress while maintaining prediction)', \n",
    "            fontsize=13)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Early layers: High I(X;T), increasing I(T;Y)\")\n",
    "print(\"- Later layers: May compress (lower I(X;T)) while maintaining I(T;Y)\")\n",
    "print(\"- Optimal: High prediction, low redundancy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Mutual Information**: Quantifying shared information\n",
    "   - k-NN estimation (accurate but slow)\n",
    "   - MINE (scalable neural estimation)\n",
    "   - Applications to understanding representations\n",
    "\n",
    "2. **Information Plane**: Visualizing learning dynamics\n",
    "   - I(X; T): Information preservation\n",
    "   - I(T; Y): Prediction information\n",
    "   - Compression vs prediction trade-off\n",
    "\n",
    "3. **Information Bottleneck**: Optimal representations\n",
    "   - Minimal sufficient statistics\n",
    "   - Compression improves generalization\n",
    "   - Two phases of learning\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Information theory is powerful**: Reveals what networks learn\n",
    "- **MI estimation is tractable**: MINE enables large-scale analysis\n",
    "- **Compression helps**: Removing irrelevant info improves generalization\n",
    "- **Layer-wise analysis**: Information plane shows learning dynamics\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. **Architecture Design**: Choose layer sizes based on information flow\n",
    "2. **Regularization**: Encourage information bottleneck\n",
    "3. **Debugging**: Identify where information is lost\n",
    "4. **Interpretability**: Understand what each layer computes\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Notebook 10**: Advanced topics (meta-dynamics, topology, counterfactuals)\n",
    "2. **Apply to your models**: Analyze information flow in your networks\n",
    "3. **Experiment with regularization**: Try information bottleneck constraints\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Tishby & Zaslavsky (2015): *Deep learning and the information bottleneck*\n",
    "- Shwartz-Ziv & Tishby (2017): *Opening the black box via information*\n",
    "- Belghazi et al. (2018): *MINE: Mutual Information Neural Estimation*\n",
    "- Saxe et al. (2019): *On the information bottleneck theory*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
