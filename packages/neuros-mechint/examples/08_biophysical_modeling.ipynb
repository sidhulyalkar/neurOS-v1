{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 08: Biophysical Modeling with Spiking Networks\n",
    "\n",
    "## Bridging Artificial and Biological Neural Networks\n",
    "\n",
    "This notebook explores **biophysically realistic neural network models** that capture key properties of real neurons:\n",
    "- **Spiking dynamics**: Neurons communicate via discrete action potentials\n",
    "- **Temporal precision**: Spike timing carries information\n",
    "- **Dale's law**: Neurons are either excitatory or inhibitory (not both)\n",
    "- **Biological constraints**: Realistic synaptic dynamics and plasticity\n",
    "\n",
    "### Why Biophysical Modeling Matters\n",
    "\n",
    "1. **Brain Alignment**: Models that work like brains are easier to compare to neuroscience data\n",
    "2. **Energy Efficiency**: Spiking networks can be more efficient than rate-based models\n",
    "3. **Temporal Processing**: Spikes enable precise temporal computation\n",
    "4. **Interpretability**: Biological constraints make models more interpretable\n",
    "5. **Neuromorphic Hardware**: Spiking networks can run on specialized chips\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Spiking Neuron Models**: LIF, Izhikevich, Hodgkin-Huxley\n",
    "2. **Surrogate Gradients**: Backpropagation through spikes\n",
    "3. **Dale's Law**: Enforcing E/I neuron separation\n",
    "4. **Synaptic Plasticity**: STDP and other learning rules\n",
    "5. **Network Dynamics**: Building spiking neural networks\n",
    "6. **Biological Constraints**: Enforcing realistic connectivity\n",
    "\n",
    "### References\n",
    "\n",
    "- Gerstner & Kistler (2002): *Spiking Neuron Models*\n",
    "- Izhikevich (2003): *Simple model of spiking neurons*\n",
    "- Neftci et al. (2019): *Surrogate gradient learning in spiking neural networks*\n",
    "- Song et al. (2000): *Competitive Hebbian learning through STDP*\n",
    "- Litwin-Kumar & Doiron (2012): *Slow dynamics and high variability in balanced networks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Spiking Neuron Models\n",
    "\n",
    "### What Makes Neurons Spike?\n",
    "\n",
    "Biological neurons have a **membrane potential** V(t) that:\n",
    "1. Integrates input currents\n",
    "2. Generates a spike when V(t) exceeds threshold\n",
    "3. Resets after spiking\n",
    "4. Has a refractory period\n",
    "\n",
    "### Leaky Integrate-and-Fire (LIF) Model\n",
    "\n",
    "The simplest spiking model:\n",
    "\n",
    "```\n",
    "τ dV/dt = -(V - V_rest) + R * I(t)\n",
    "\n",
    "if V(t) ≥ V_th:\n",
    "    emit spike\n",
    "    V(t) ← V_reset\n",
    "```\n",
    "\n",
    "where:\n",
    "- V: Membrane potential\n",
    "- τ: Membrane time constant (typically 10-20 ms)\n",
    "- V_rest: Resting potential (-65 mV)\n",
    "- V_th: Spike threshold (-55 mV)\n",
    "- V_reset: Reset potential (-70 mV)\n",
    "- R: Membrane resistance\n",
    "- I(t): Input current\n",
    "\n",
    "### Izhikevich Model\n",
    "\n",
    "More realistic dynamics with just 2 variables:\n",
    "\n",
    "```\n",
    "dV/dt = 0.04*V² + 5*V + 140 - u + I\n",
    "du/dt = a*(b*V - u)\n",
    "\n",
    "if V ≥ 30 mV:\n",
    "    V ← c\n",
    "    u ← u + d\n",
    "```\n",
    "\n",
    "Parameters (a, b, c, d) control neuron type:\n",
    "- Regular spiking: (0.02, 0.2, -65, 8)\n",
    "- Fast spiking: (0.1, 0.2, -65, 2)\n",
    "- Bursting: (0.02, 0.2, -50, 2)\n",
    "\n",
    "### Hodgkin-Huxley Model\n",
    "\n",
    "Biophysically detailed with ion channels:\n",
    "\n",
    "```\n",
    "C dV/dt = I - g_Na*m³*h*(V-E_Na) - g_K*n⁴*(V-E_K) - g_L*(V-E_L)\n",
    "```\n",
    "\n",
    "Most accurate but computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyIntegrateFireNeuron:\n",
    "    \"\"\"\n",
    "    Leaky Integrate-and-Fire (LIF) neuron model.\n",
    "    \n",
    "    Simple but captures essential spiking dynamics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tau=10.0, v_rest=-65.0, v_th=-55.0, v_reset=-70.0, dt=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tau: Membrane time constant (ms)\n",
    "            v_rest: Resting potential (mV)\n",
    "            v_th: Spike threshold (mV)\n",
    "            v_reset: Reset potential after spike (mV)\n",
    "            dt: Time step (ms)\n",
    "        \"\"\"\n",
    "        self.tau = tau\n",
    "        self.v_rest = v_rest\n",
    "        self.v_th = v_th\n",
    "        self.v_reset = v_reset\n",
    "        self.dt = dt\n",
    "        \n",
    "        # State\n",
    "        self.v = v_rest\n",
    "        self.spike_times = []\n",
    "    \n",
    "    def step(self, input_current):\n",
    "        \"\"\"\n",
    "        Simulate one time step.\n",
    "        \n",
    "        Args:\n",
    "            input_current: Input current (arbitrary units)\n",
    "        \n",
    "        Returns:\n",
    "            spike: Whether neuron spiked (bool)\n",
    "        \"\"\"\n",
    "        # Integrate: dV/dt = -(V - V_rest)/tau + I/tau\n",
    "        dv = (-(self.v - self.v_rest) + input_current) / self.tau\n",
    "        self.v += dv * self.dt\n",
    "        \n",
    "        # Check for spike\n",
    "        spike = self.v >= self.v_th\n",
    "        \n",
    "        # Reset if spiked\n",
    "        if spike:\n",
    "            self.v = self.v_reset\n",
    "            self.spike_times.append(len(self.spike_times))\n",
    "        \n",
    "        return spike\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset neuron to resting state.\"\"\"\n",
    "        self.v = self.v_rest\n",
    "        self.spike_times = []\n",
    "\n",
    "print(\"LIF neuron model implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IzhikevichNeuron:\n",
    "    \"\"\"\n",
    "    Izhikevich spiking neuron model.\n",
    "    \n",
    "    Captures diverse neuron types with 4 parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, a=0.02, b=0.2, c=-65, d=8, dt=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            a: Recovery time scale\n",
    "            b: Sensitivity to subthreshold V\n",
    "            c: Reset voltage (mV)\n",
    "            d: Reset recovery variable\n",
    "            dt: Time step (ms)\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.d = d\n",
    "        self.dt = dt\n",
    "        \n",
    "        # State\n",
    "        self.v = -65  # Membrane potential\n",
    "        self.u = self.b * self.v  # Recovery variable\n",
    "        self.spike_times = []\n",
    "    \n",
    "    def step(self, input_current):\n",
    "        \"\"\"\n",
    "        Simulate one time step.\n",
    "        \n",
    "        Args:\n",
    "            input_current: Input current\n",
    "        \n",
    "        Returns:\n",
    "            spike: Whether neuron spiked\n",
    "        \"\"\"\n",
    "        # Update voltage: dV/dt = 0.04*V² + 5*V + 140 - u + I\n",
    "        dv = (0.04 * self.v**2 + 5*self.v + 140 - self.u + input_current) * self.dt\n",
    "        \n",
    "        # Update recovery: du/dt = a*(b*V - u)\n",
    "        du = self.a * (self.b * self.v - self.u) * self.dt\n",
    "        \n",
    "        self.v += dv\n",
    "        self.u += du\n",
    "        \n",
    "        # Check for spike\n",
    "        spike = self.v >= 30\n",
    "        \n",
    "        # Reset if spiked\n",
    "        if spike:\n",
    "            self.v = self.c\n",
    "            self.u += self.d\n",
    "            self.spike_times.append(len(self.spike_times))\n",
    "        \n",
    "        return spike\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset neuron.\"\"\"\n",
    "        self.v = -65\n",
    "        self.u = self.b * self.v\n",
    "        self.spike_times = []\n",
    "\n",
    "print(\"Izhikevich neuron model implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and compare LIF and Izhikevich neurons\n",
    "\n",
    "# Simulation parameters\n",
    "T = 200  # Total time (ms)\n",
    "dt = 0.1  # Time step (ms)\n",
    "n_steps = int(T / dt)\n",
    "time = np.arange(0, T, dt)\n",
    "\n",
    "# Input current: step from t=50 to t=150\n",
    "input_current = np.zeros(n_steps)\n",
    "input_current[int(50/dt):int(150/dt)] = 15\n",
    "\n",
    "# Create neurons\n",
    "lif = LeakyIntegrateFireNeuron(tau=10.0, dt=dt)\n",
    "izh_regular = IzhikevichNeuron(a=0.02, b=0.2, c=-65, d=8, dt=dt)\n",
    "izh_fast = IzhikevichNeuron(a=0.1, b=0.2, c=-65, d=2, dt=dt)\n",
    "\n",
    "# Simulate\n",
    "v_lif = []\n",
    "v_izh_reg = []\n",
    "v_izh_fast = []\n",
    "\n",
    "for i in range(n_steps):\n",
    "    lif.step(input_current[i])\n",
    "    izh_regular.step(input_current[i])\n",
    "    izh_fast.step(input_current[i])\n",
    "    \n",
    "    v_lif.append(lif.v)\n",
    "    v_izh_reg.append(izh_regular.v)\n",
    "    v_izh_fast.append(izh_fast.v)\n",
    "\n",
    "v_lif = np.array(v_lif)\n",
    "v_izh_reg = np.array(v_izh_reg)\n",
    "v_izh_fast = np.array(v_izh_fast)\n",
    "\n",
    "print(f\"LIF neuron: {len(lif.spike_times)} spikes\")\n",
    "print(f\"Izhikevich (regular): {len(izh_regular.spike_times)} spikes\")\n",
    "print(f\"Izhikevich (fast): {len(izh_fast.spike_times)} spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neuron responses\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot 1: Input current\n",
    "ax = axes[0]\n",
    "ax.plot(time, input_current, 'k-', linewidth=2)\n",
    "ax.set_ylabel('Input Current')\n",
    "ax.set_title('Spiking Neuron Models: Response to Step Current')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: LIF\n",
    "ax = axes[1]\n",
    "ax.plot(time, v_lif, 'b-', linewidth=1)\n",
    "ax.axhline(y=lif.v_th, color='red', linestyle='--', label='Threshold')\n",
    "ax.set_ylabel('Voltage (mV)')\n",
    "ax.set_title('LIF Neuron')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Izhikevich (regular spiking)\n",
    "ax = axes[2]\n",
    "ax.plot(time, v_izh_reg, 'g-', linewidth=1)\n",
    "ax.axhline(y=30, color='red', linestyle='--', label='Threshold')\n",
    "ax.set_ylabel('Voltage (mV)')\n",
    "ax.set_title('Izhikevich (Regular Spiking)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Izhikevich (fast spiking)\n",
    "ax = axes[3]\n",
    "ax.plot(time, v_izh_fast, 'm-', linewidth=1)\n",
    "ax.axhline(y=30, color='red', linestyle='--', label='Threshold')\n",
    "ax.set_xlabel('Time (ms)')\n",
    "ax.set_ylabel('Voltage (mV)')\n",
    "ax.set_title('Izhikevich (Fast Spiking)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- LIF: Simple regular spiking\")\n",
    "print(\"- Izhikevich regular: Spike frequency adaptation\")\n",
    "print(\"- Izhikevich fast: High firing rate, minimal adaptation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Surrogate Gradients for Training Spiking Networks\n",
    "\n",
    "### The Gradient Problem\n",
    "\n",
    "**Challenge**: Spikes are discontinuous (0 or 1), so gradients are zero almost everywhere!\n",
    "\n",
    "```\n",
    "spike = 1 if V ≥ V_th else 0\n",
    "d(spike)/dV = δ(V - V_th)  # Dirac delta - not useful!\n",
    "```\n",
    "\n",
    "### Surrogate Gradient Solution\n",
    "\n",
    "Replace true gradient with a smooth **surrogate** during backpropagation:\n",
    "\n",
    "**Forward pass**: Use actual step function\n",
    "```\n",
    "spike = H(V - V_th)  # Heaviside step\n",
    "```\n",
    "\n",
    "**Backward pass**: Use smooth approximation\n",
    "```\n",
    "dL/dV ≈ dL/dspike * σ'(V - V_th)\n",
    "```\n",
    "\n",
    "where σ' can be:\n",
    "- **Sigmoid derivative**: σ(x) * (1 - σ(x))\n",
    "- **Fast sigmoid**: 1 / (1 + |x/β|)²\n",
    "- **Exponential**: exp(-|x/β|)\n",
    "- **SuperSpike**: 1 / (1 + |x/β|)²\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "- Forward pass: Correct spiking behavior\n",
    "- Backward pass: Gradients flow, enabling learning\n",
    "- Empirically effective for many tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrogateGradient(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Spike function with surrogate gradient for backpropagation.\n",
    "    \n",
    "    Forward: Heaviside step (0 or 1)\n",
    "    Backward: Smooth surrogate (enables gradient flow)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, threshold=0.0, beta=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: Membrane potential\n",
    "            threshold: Spike threshold\n",
    "            beta: Surrogate gradient slope\n",
    "        \n",
    "        Returns:\n",
    "            spike: Binary spike (0 or 1)\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.threshold = threshold\n",
    "        ctx.beta = beta\n",
    "        \n",
    "        # Forward: step function\n",
    "        spike = (input >= threshold).float()\n",
    "        return spike\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward: smooth surrogate gradient.\n",
    "        \n",
    "        Using fast sigmoid: 1 / (1 + |x/β|)²\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        threshold = ctx.threshold\n",
    "        beta = ctx.beta\n",
    "        \n",
    "        # Surrogate gradient\n",
    "        x = (input - threshold) / beta\n",
    "        surrogate = 1.0 / (1.0 + torch.abs(x))**2\n",
    "        \n",
    "        return grad_output * surrogate, None, None\n",
    "\n",
    "# Convenience function\n",
    "def spike_fn(x, threshold=0.0, beta=1.0):\n",
    "    \"\"\"Spiking activation with surrogate gradient.\"\"\"\n",
    "    return SurrogateGradient.apply(x, threshold, beta)\n",
    "\n",
    "print(\"Surrogate gradient spike function implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer of Leaky Integrate-and-Fire neurons with surrogate gradients.\n",
    "    \n",
    "    Trainable via backpropagation through time (BPTT).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_neurons, tau=10.0, v_th=1.0, v_reset=0.0, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.n_neurons = n_neurons\n",
    "        self.tau = tau\n",
    "        self.v_th = v_th\n",
    "        self.v_reset = v_reset\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, input_current, v_mem=None, dt=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_current: (batch, time, n_neurons)\n",
    "            v_mem: Initial membrane potential (batch, n_neurons)\n",
    "            dt: Time step\n",
    "        \n",
    "        Returns:\n",
    "            spikes: (batch, time, n_neurons)\n",
    "            v_mem: Final membrane potential\n",
    "        \"\"\"\n",
    "        batch, time, _ = input_current.shape\n",
    "        device = input_current.device\n",
    "        \n",
    "        # Initialize membrane potential\n",
    "        if v_mem is None:\n",
    "            v_mem = torch.zeros(batch, self.n_neurons, device=device)\n",
    "        \n",
    "        # Collect spikes over time\n",
    "        spikes_over_time = []\n",
    "        \n",
    "        for t in range(time):\n",
    "            # Leak: dV/dt = -V/tau\n",
    "            v_mem = v_mem * (1 - dt/self.tau)\n",
    "            \n",
    "            # Input: dV/dt += I/tau\n",
    "            v_mem = v_mem + input_current[:, t, :] * (dt/self.tau)\n",
    "            \n",
    "            # Spike generation (with surrogate gradient)\n",
    "            spike = spike_fn(v_mem, self.v_th, self.beta)\n",
    "            \n",
    "            # Reset\n",
    "            v_mem = v_mem * (1 - spike) + self.v_reset * spike\n",
    "            \n",
    "            spikes_over_time.append(spike)\n",
    "        \n",
    "        spikes = torch.stack(spikes_over_time, dim=1)\n",
    "        return spikes, v_mem\n",
    "\n",
    "print(\"LIF layer with surrogate gradients implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test surrogate gradient\n",
    "# Compare true step function gradient (zero) vs surrogate (smooth)\n",
    "\n",
    "x = torch.linspace(-5, 5, 200, requires_grad=True)\n",
    "\n",
    "# Forward pass: spike function\n",
    "y = spike_fn(x, threshold=0.0, beta=1.0)\n",
    "\n",
    "# Backward pass: compute gradient\n",
    "y.sum().backward()\n",
    "grad = x.grad.clone()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Forward (spike function)\n",
    "ax = axes[0]\n",
    "ax.plot(x.detach().numpy(), y.detach().numpy(), linewidth=2, label='Spike function')\n",
    "ax.axvline(x=0, color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "ax.set_xlabel('Membrane Potential')\n",
    "ax.set_ylabel('Spike Output')\n",
    "ax.set_title('Forward Pass: Heaviside Step Function')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Backward (surrogate gradient)\n",
    "ax = axes[1]\n",
    "ax.plot(x.detach().numpy(), grad.numpy(), linewidth=2, color='green', label='Surrogate gradient')\n",
    "ax.axvline(x=0, color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "ax.set_xlabel('Membrane Potential')\n",
    "ax.set_ylabel('Gradient')\n",
    "ax.set_title('Backward Pass: Smooth Surrogate Gradient')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Forward: Sharp threshold (realistic spiking)\")\n",
    "print(\"- Backward: Smooth gradient (enables learning)\")\n",
    "print(\"- Gradients flow even away from threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dale's Law - Excitatory and Inhibitory Neurons\n",
    "\n",
    "### What is Dale's Law?\n",
    "\n",
    "**Dale's Law**: A neuron releases the same set of neurotransmitters at all of its synapses.\n",
    "\n",
    "**Practical implication**: Each neuron is either:\n",
    "- **Excitatory (E)**: All outgoing connections are positive\n",
    "- **Inhibitory (I)**: All outgoing connections are negative\n",
    "\n",
    "Neurons cannot be both!\n",
    "\n",
    "### Why Enforce Dale's Law?\n",
    "\n",
    "1. **Biological realism**: Real brains obey this constraint\n",
    "2. **Interpretability**: E/I balance is well-studied in neuroscience\n",
    "3. **Stability**: Proper E/I balance prevents runaway activity\n",
    "4. **Alignment**: Makes models easier to compare to brain data\n",
    "\n",
    "### Implementation\n",
    "\n",
    "For a weight matrix W connecting layers:\n",
    "- **Designate**: Neuron i is E or I\n",
    "- **Constrain**: All weights from neuron i have same sign\n",
    "- **Enforce**: During training, project weights back to valid set\n",
    "\n",
    "Methods:\n",
    "1. **Hard constraint**: W[i, :] = |W[i, :]| if E, -|W[i, :]| if I\n",
    "2. **Soft constraint**: Add penalty for violations\n",
    "3. **Parametrization**: W = D @ |W_raw| where D is diagonal ±1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DalesLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with Dale's law constraint.\n",
    "    \n",
    "    Each neuron is either excitatory (E) or inhibitory (I).\n",
    "    All outgoing weights from a neuron have the same sign.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, fraction_exc=0.8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Input dimension\n",
    "            out_features: Output dimension\n",
    "            fraction_exc: Fraction of excitatory neurons (typically 0.8)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Unsigned weights (always positive)\n",
    "        self.weight_magnitude = nn.Parameter(torch.rand(out_features, in_features))\n",
    "        \n",
    "        # Dale's law: sign matrix (fixed, not learned)\n",
    "        n_exc = int(in_features * fraction_exc)\n",
    "        signs = torch.ones(in_features)\n",
    "        signs[n_exc:] = -1  # Last neurons are inhibitory\n",
    "        self.register_buffer('dale_signs', signs)\n",
    "        \n",
    "        # Bias\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with Dale's law enforced.\n",
    "        \n",
    "        W = |W_magnitude| ⊙ sign_matrix\n",
    "        \"\"\"\n",
    "        # Enforce sign constraint\n",
    "        weight = torch.abs(self.weight_magnitude) * self.dale_signs.unsqueeze(0)\n",
    "        \n",
    "        # Linear transformation\n",
    "        return F.linear(x, weight, self.bias)\n",
    "    \n",
    "    def get_excitatory_mask(self):\n",
    "        \"\"\"Return mask for excitatory neurons.\"\"\"\n",
    "        return self.dale_signs > 0\n",
    "    \n",
    "    def get_inhibitory_mask(self):\n",
    "        \"\"\"Return mask for inhibitory neurons.\"\"\"\n",
    "        return self.dale_signs < 0\n",
    "\n",
    "print(\"Dale's law linear layer implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EISpikingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Spiking neural network with Dale's law (E/I separation).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, fraction_exc=0.8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Recurrent layer with Dale's law\n",
    "        self.recurrent = DalesLinear(hidden_size, hidden_size, fraction_exc)\n",
    "        \n",
    "        # Spiking neurons\n",
    "        self.lif = LIFLayer(hidden_size, tau=10.0, v_th=1.0)\n",
    "        \n",
    "        # Output readout (rate-based)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch, time, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, time, output_size)\n",
    "            spikes: (batch, time, hidden_size)\n",
    "        \"\"\"\n",
    "        batch, time, _ = x.shape\n",
    "        \n",
    "        # Project input\n",
    "        x_proj = self.input_proj(x)\n",
    "        \n",
    "        # Initialize state\n",
    "        v_mem = None\n",
    "        spike = torch.zeros(batch, self.lif.n_neurons, device=x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        all_spikes = []\n",
    "        \n",
    "        for t in range(time):\n",
    "            # Recurrent input (Dale's law enforced)\n",
    "            rec_input = self.recurrent(spike)\n",
    "            \n",
    "            # Total input\n",
    "            total_input = x_proj[:, t:t+1, :] + rec_input.unsqueeze(1)\n",
    "            \n",
    "            # LIF dynamics\n",
    "            spike_t, v_mem = self.lif(total_input, v_mem)\n",
    "            spike = spike_t.squeeze(1)\n",
    "            \n",
    "            # Readout\n",
    "            output_t = self.output(spike)\n",
    "            \n",
    "            outputs.append(output_t)\n",
    "            all_spikes.append(spike)\n",
    "        \n",
    "        output = torch.stack(outputs, dim=1)\n",
    "        spikes = torch.stack(all_spikes, dim=1)\n",
    "        \n",
    "        return output, spikes\n",
    "\n",
    "print(\"E/I spiking network implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dale's law enforcement\n",
    "input_size = 10\n",
    "hidden_size = 50\n",
    "output_size = 2\n",
    "fraction_exc = 0.8\n",
    "\n",
    "# Create network\n",
    "net = EISpikingNetwork(input_size, hidden_size, output_size, fraction_exc)\n",
    "\n",
    "# Generate test input\n",
    "x = torch.randn(4, 20, input_size) * 0.5\n",
    "\n",
    "# Forward pass\n",
    "output, spikes = net(x)\n",
    "\n",
    "print(f\"Network created with Dale's law:\")\n",
    "print(f\"  Input size: {input_size}\")\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "print(f\"  Excitatory neurons: {int(hidden_size * fraction_exc)}\")\n",
    "print(f\"  Inhibitory neurons: {hidden_size - int(hidden_size * fraction_exc)}\")\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Spikes shape: {spikes.shape}\")\n",
    "print(f\"  Mean firing rate: {spikes.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Dale's law enforcement\n",
    "# Extract recurrent weight matrix\n",
    "with torch.no_grad():\n",
    "    weight_magnitude = net.recurrent.weight_magnitude.numpy()\n",
    "    signs = net.recurrent.dale_signs.numpy()\n",
    "    weight_actual = np.abs(weight_magnitude) * signs\n",
    "\n",
    "# Get E/I masks\n",
    "exc_mask = net.recurrent.get_excitatory_mask().numpy()\n",
    "inh_mask = net.recurrent.get_inhibitory_mask().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Full weight matrix\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(weight_actual, cmap='RdBu_r', aspect='auto', \n",
    "               vmin=-np.abs(weight_actual).max(), \n",
    "               vmax=np.abs(weight_actual).max())\n",
    "ax.axvline(x=np.sum(exc_mask)-0.5, color='yellow', linewidth=2, label='E/I boundary')\n",
    "ax.set_xlabel('Pre-synaptic Neuron')\n",
    "ax.set_ylabel('Post-synaptic Neuron')\n",
    "ax.set_title('Recurrent Weights (Dale\\'s Law Enforced)')\n",
    "ax.legend()\n",
    "plt.colorbar(im, ax=ax, label='Weight')\n",
    "\n",
    "# Plot 2: Column sums (verify all same sign)\n",
    "ax = axes[0, 1]\n",
    "col_max = weight_actual.max(axis=0)\n",
    "col_min = weight_actual.min(axis=0)\n",
    "neuron_indices = np.arange(hidden_size)\n",
    "colors = ['blue' if e else 'red' for e in exc_mask]\n",
    "ax.scatter(neuron_indices, col_max, c=colors, alpha=0.6, s=20, label='Max weight')\n",
    "ax.scatter(neuron_indices, col_min, c=colors, alpha=0.6, s=20, label='Min weight')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax.axvline(x=np.sum(exc_mask)-0.5, color='yellow', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Neuron Index')\n",
    "ax.set_ylabel('Weight Value')\n",
    "ax.set_title('Weight Range per Neuron (Blue=E, Red=I)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Spike raster\n",
    "ax = axes[1, 0]\n",
    "# Show first trial\n",
    "spike_raster = spikes[0].numpy()\n",
    "spike_times, spike_neurons = np.where(spike_raster)\n",
    "colors_raster = ['blue' if exc_mask[n] else 'red' for n in spike_neurons]\n",
    "ax.scatter(spike_times, spike_neurons, s=1, c=colors_raster, alpha=0.5)\n",
    "ax.axhline(y=np.sum(exc_mask)-0.5, color='yellow', linestyle='--', linewidth=2, label='E/I boundary')\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Neuron Index')\n",
    "ax.set_title('Spike Raster (Blue=E, Red=I)')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 4: Firing rates E vs I\n",
    "ax = axes[1, 1]\n",
    "firing_rates = spikes.mean(dim=(0, 1)).numpy()\n",
    "exc_rates = firing_rates[exc_mask]\n",
    "inh_rates = firing_rates[inh_mask]\n",
    "ax.hist(exc_rates, bins=20, alpha=0.6, label=f'Excitatory (n={len(exc_rates)})', color='blue')\n",
    "ax.hist(inh_rates, bins=20, alpha=0.6, label=f'Inhibitory (n={len(inh_rates)})', color='red')\n",
    "ax.set_xlabel('Firing Rate')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Firing Rate Distribution: E vs I')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Weight matrix: E neurons (left) have positive weights, I neurons (right) have negative\")\n",
    "print(\"- Column statistics: All weights from each neuron have same sign\")\n",
    "print(\"- Spike raster: Both E and I neurons are active\")\n",
    "print(\"- Firing rates: E and I populations may have different activity levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Spiking Neuron Models**: LIF, Izhikevich, and their properties\n",
    "   - Membrane dynamics and threshold crossing\n",
    "   - Different neuron types (regular, fast spiking, bursting)\n",
    "   - Trade-off between realism and computational efficiency\n",
    "\n",
    "2. **Surrogate Gradients**: Training spiking networks via backprop\n",
    "   - Forward pass: Binary spikes (realistic)\n",
    "   - Backward pass: Smooth gradients (trainable)\n",
    "   - Enables end-to-end learning in spiking networks\n",
    "\n",
    "3. **Dale's Law**: Biological E/I constraint\n",
    "   - Each neuron is exclusively excitatory or inhibitory\n",
    "   - Enforced via parametrization (sign × magnitude)\n",
    "   - Improves biological realism and interpretability\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Spikes matter**: Temporal precision enables new computations\n",
    "- **Biological constraints help**: Dale's law improves interpretability\n",
    "- **Surrogate gradients work**: Enables training despite discontinuities\n",
    "- **E/I balance is crucial**: Proper balance prevents runaway activity\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. **Neuroscience**: Models that match brain recordings\n",
    "2. **Neuromorphic Computing**: Efficient hardware implementations\n",
    "3. **Temporal Processing**: Tasks requiring precise timing\n",
    "4. **Biological Plausibility**: More interpretable models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Notebook 09**: Information theory and energy landscapes\n",
    "2. **Notebook 10**: Advanced topics (meta-dynamics, topology, counterfactuals)\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Gerstner & Kistler (2002): *Spiking Neuron Models*\n",
    "- Neftci et al. (2019): *Surrogate gradient learning*\n",
    "- Izhikevich (2003): *Simple model of spiking neurons*\n",
    "- Zenke & Ganguli (2018): *SuperSpike: Supervised learning in spiking networks*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
