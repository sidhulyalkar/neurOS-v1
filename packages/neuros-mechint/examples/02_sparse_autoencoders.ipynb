{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Autoencoders: Decomposing Neural Representations\n",
    "\n",
    "**The fundamental tool for finding interpretable features in neural networks**\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you'll master:\n",
    "1. The problem of **polysemanticity** and why it makes interpretation hard\n",
    "2. How **Sparse Autoencoders (SAEs)** solve this problem\n",
    "3. Training SAEs on neural network activations\n",
    "4. **Hierarchical SAEs** for multi-level concept extraction\n",
    "5. Using SAE features for **causal interventions**\n",
    "6. Best practices and practical tips\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Notebook 01 (Introduction)\n",
    "- Understanding of autoencoders\n",
    "- Familiarity with PyTorch\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from neuros_mechint import (\n",
    "    SparseAutoencoder,\n",
    "    HierarchicalSAE,\n",
    "    ConceptDictionary,\n",
    "    CausalSAEProbe,\n",
    "    ActivationCache,\n",
    "    SAEVisualizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Problem - Polysemanticity\n",
    "\n",
    "### Why Can't We Just Interpret Individual Neurons?\n",
    "\n",
    "The naive approach to interpretability would be:\n",
    "1. Look at each neuron\n",
    "2. Find what activates it\n",
    "3. Give it a label\n",
    "\n",
    "But this **doesn't work** because of **polysemanticity**: individual neurons respond to multiple, unrelated concepts.\n",
    "\n",
    "### Demonstrating Polysemanticity\n",
    "\n",
    "Let's create a simple example that shows why neurons become polysemantic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyssemanticDemo(nn.Module):\n",
    "    \"\"\"A simple network that will develop polysemantic neurons\"\"\"\n",
    "    def __init__(self, input_dim=100, hidden_dim=20, output_dim=10):\n",
    "        super().__init__()\n",
    "        # Bottleneck: fewer hidden neurons than input features\n",
    "        # This forces neurons to be polysemantic!\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = F.relu(self.encoder(x))\n",
    "        return self.decoder(hidden), hidden\n",
    "\n",
    "# Create synthetic data with clear concepts\n",
    "n_samples = 1000\n",
    "n_concepts = 100  # Many more concepts than neurons!\n",
    "\n",
    "# Each sample has a few active concepts\n",
    "data = torch.zeros(n_samples, n_concepts)\n",
    "labels = torch.randint(0, 10, (n_samples,))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Activate 5 random concepts per sample\n",
    "    active_concepts = np.random.choice(n_concepts, 5, replace=False)\n",
    "    data[i, active_concepts] = torch.randn(5).abs()\n",
    "\n",
    "# Train the network\n",
    "model = PolyssemanticDemo(input_dim=n_concepts, hidden_dim=20, output_dim=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Training polysemantic network...\")\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(data)\n",
    "    loss = F.cross_entropy(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Analyze what each neuron responds to\n",
    "with torch.no_grad():\n",
    "    _, hidden = model(data)\n",
    "\n",
    "# For each neuron, find which input concepts activate it\n",
    "print(\"\\nAnalyzing neuron selectivity...\")\n",
    "for neuron_idx in range(5):  # Look at first 5 neurons\n",
    "    # Find samples where this neuron is highly active\n",
    "    active_samples = hidden[:, neuron_idx] > hidden[:, neuron_idx].quantile(0.9)\n",
    "    \n",
    "    # Which input concepts are present in those samples?\n",
    "    concept_frequency = (data[active_samples] > 0).float().mean(dim=0)\n",
    "    top_concepts = concept_frequency.topk(5).indices.tolist()\n",
    "    \n",
    "    print(f\"\\nNeuron {neuron_idx} responds to concepts: {top_concepts}\")\n",
    "    print(f\"  (These concepts appear together in {concept_frequency[top_concepts[0]]:.1%} of high-activation samples)\")\n",
    "\n",
    "print(\"\\n⚠️  Notice: Each neuron responds to MULTIPLE unrelated concepts!\")\n",
    "print(\"This is polysemanticity - it makes neurons hard to interpret.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Does Polysemanticity Happen?\n",
    "\n",
    "**Fundamental reason**: **Superposition**\n",
    "\n",
    "Neural networks can represent more features than they have neurons by using superposition:\n",
    "- Like storing multiple vectors in the same subspace\n",
    "- Features are represented as *directions* rather than dedicated neurons\n",
    "- This is efficient but makes individual neurons uninterpretable\n",
    "\n",
    "**Mathematical intuition**:\n",
    "- Network has $n$ neurons\n",
    "- Data has $m$ meaningful features where $m >> n$\n",
    "- Network can represent ~$n^2$ features via superposition\n",
    "- But each neuron now responds to $m/n$ features on average\n",
    "\n",
    "**Analogy**: Like packing a suitcase—you can fit more items by using creative arrangements, but then each item isn't in its own compartment.\n",
    "\n",
    "### The Solution: Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sparse Autoencoders (SAEs)\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "SAEs solve polysemanticity by:\n",
    "1. **Overcomplete dictionary**: Use more features than neurons (e.g., 4× expansion)\n",
    "2. **Sparsity**: Only a few features can be active at once\n",
    "3. **Reconstruction**: Features must reconstruct original activations\n",
    "\n",
    "Result: Each feature becomes **monosemantic** (represents one concept)!\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given neural activations $h \\in \\mathbb{R}^d$:\n",
    "\n",
    "**Encoder**: \n",
    "$$f(h) = \\text{ReLU}(W_e h + b_e) \\in \\mathbb{R}^m$$\n",
    "where $m > d$ (overcomplete)\n",
    "\n",
    "**Decoder**:\n",
    "$$\\hat{h} = W_d f(h) + b_d$$\n",
    "\n",
    "**Loss function**:\n",
    "$$\\mathcal{L} = \\underbrace{\\|h - \\hat{h}\\|^2}_{\\text{Reconstruction}} + \\underbrace{\\lambda \\|f(h)\\|_1}_{\\text{Sparsity}}$$\n",
    "\n",
    "- **Reconstruction term**: Ensures features capture all information\n",
    "- **Sparsity term** ($L_1$): Encourages few active features\n",
    "- **Hyperparameter $\\lambda$**: Trade-off between reconstruction and sparsity\n",
    "\n",
    "**Key properties**:\n",
    "- ReLU activation ensures non-negativity\n",
    "- L1 penalty promotes exact zeros (sparse code)\n",
    "- Tied weights option: $W_d = W_e^T$ (reduces parameters)\n",
    "\n",
    "### Training an SAE from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train an SAE on the polysemantic network's activations\n",
    "\n",
    "# Collect activations\n",
    "with torch.no_grad():\n",
    "    _, activations = model(data)\n",
    "\n",
    "print(f\"Collected activations: {activations.shape}\")\n",
    "print(f\"  {activations.shape[0]} samples\")\n",
    "print(f\"  {activations.shape[1]} neurons per sample\")\n",
    "\n",
    "# Create SAE with 4x overcomplete expansion\n",
    "input_dim = activations.shape[1]\n",
    "hidden_dim = input_dim * 4  # 4x overcomplete\n",
    "\n",
    "sae = SparseAutoencoder(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    sparsity_coef=0.01,  # L1 penalty strength\n",
    "    tied_weights=False   # Use separate encoder/decoder\n",
    ")\n",
    "\n",
    "print(f\"\\nSAE Architecture:\")\n",
    "print(f\"  Input dimension: {input_dim}\")\n",
    "print(f\"  Feature dimension: {hidden_dim} ({hidden_dim/input_dim:.1f}x overcomplete)\")\n",
    "print(f\"  Sparsity coefficient: {sae.sparsity_coef}\")\n",
    "\n",
    "# Train the SAE\n",
    "print(\"\\nTraining SAE...\")\n",
    "losses = sae.train_on_activations(\n",
    "    activations,\n",
    "    num_epochs=200,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Total Loss')\n",
    "axes[0].set_title('SAE Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot on log scale to see details\n",
    "axes[1].plot(losses)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Total Loss (log scale)')\n",
    "axes[1].set_title('SAE Training Loss (Log Scale)')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Learned Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature activations\n",
    "feature_acts = sae.get_feature_activations(activations)\n",
    "\n",
    "print(f\"Feature activations shape: {feature_acts.shape}\")\n",
    "print(f\"\\nSparsity Statistics:\")\n",
    "print(f\"  Average features active per sample: {(feature_acts > 0).float().sum(dim=1).mean():.2f}\")\n",
    "print(f\"  Percentage of features active: {(feature_acts > 0).float().mean():.2%}\")\n",
    "print(f\"  Max features active: {(feature_acts > 0).float().sum(dim=1).max():.0f}\")\n",
    "\n",
    "# Reconstruction quality\n",
    "reconstructed = sae(activations)\n",
    "mse = F.mse_loss(reconstructed, activations)\n",
    "explained_var = 1 - mse / activations.var()\n",
    "\n",
    "print(f\"\\nReconstruction Quality:\")\n",
    "print(f\"  MSE: {mse.item():.6f}\")\n",
    "print(f\"  Explained variance: {explained_var.item():.2%}\")\n",
    "\n",
    "# Visualize sparsity pattern\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top-left: Feature activation heatmap\n",
    "sample_indices = np.random.choice(feature_acts.shape[0], 100, replace=False)\n",
    "im = axes[0, 0].imshow(\n",
    "    feature_acts[sample_indices].T.detach().numpy(),\n",
    "    aspect='auto',\n",
    "    cmap='viridis',\n",
    "    interpolation='none'\n",
    ")\n",
    "axes[0, 0].set_xlabel('Sample')\n",
    "axes[0, 0].set_ylabel('Feature Index')\n",
    "axes[0, 0].set_title('Feature Activations (100 random samples)')\n",
    "plt.colorbar(im, ax=axes[0, 0])\n",
    "\n",
    "# Top-right: Activation distribution\n",
    "axes[0, 1].hist(feature_acts.flatten().detach().numpy(), bins=50, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Activation Value')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Feature Activation Distribution')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-left: Features active per sample\n",
    "features_per_sample = (feature_acts > 0).float().sum(dim=1).detach().numpy()\n",
    "axes[1, 0].hist(features_per_sample, bins=30, edgecolor='black')\n",
    "axes[1, 0].axvline(\n",
    "    features_per_sample.mean(),\n",
    "    color='red',\n",
    "    linestyle='--',\n",
    "    label=f'Mean: {features_per_sample.mean():.1f}'\n",
    ")\n",
    "axes[1, 0].set_xlabel('Number of Active Features')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Features Active per Sample')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom-right: Feature frequency\n",
    "feature_freq = (feature_acts > 0).float().mean(dim=0).detach().numpy()\n",
    "axes[1, 1].hist(feature_freq, bins=30, edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Activation Frequency')\n",
    "axes[1, 1].set_ylabel('Number of Features')\n",
    "axes[1, 1].set_title('How Often Each Feature Activates')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Success! The SAE learned sparse, interpretable features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "Notice from the visualizations:\n",
    "\n",
    "1. **Sparsity**: Only a small fraction of features are active at once\n",
    "2. **Reconstruction**: Despite sparsity, we still reconstruct activations well\n",
    "3. **Diversity**: Different features activate at different frequencies\n",
    "\n",
    "This is exactly what we want! Now each feature can be monosemantic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Applying SAEs to Real Models\n",
    "\n",
    "Let's train SAEs on a real transformer and extract interpretable features.\n",
    "\n",
    "### Setup: A Simple Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer (same as Notebook 01)\n",
    "d_model = 64\n",
    "nhead = 4\n",
    "batch_size = 16\n",
    "seq_len = 12\n",
    "\n",
    "transformer = nn.TransformerEncoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.0  # No dropout for cleaner analysis\n",
    ").to(device)\n",
    "\n",
    "transformer.eval()  # Evaluation mode\n",
    "\n",
    "print(f\"Transformer ready: {sum(p.numel() for p in transformer.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Activations Efficiently\n",
    "\n",
    "The `ActivationCache` makes it easy to collect activations from multiple layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use activation cache for efficient collection\n",
    "cache = ActivationCache(\n",
    "    model=transformer,\n",
    "    layer_names=['self_attn', 'linear1', 'linear2'],  # Layers to cache\n",
    "    max_samples=5000  # Collect up to 5000 samples\n",
    ")\n",
    "\n",
    "# Generate data and collect activations\n",
    "print(\"Collecting activations from transformer...\")\n",
    "n_batches = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(n_batches)):\n",
    "        x = torch.randn(seq_len, batch_size, d_model).to(device)\n",
    "        _ = transformer(x)\n",
    "        cache.step()  # Store activations from this batch\n",
    "\n",
    "# Get collected activations\n",
    "collected_acts = cache.get_activations()\n",
    "\n",
    "print(\"\\nCollected activations:\")\n",
    "for layer_name, acts in collected_acts.items():\n",
    "    print(f\"  {layer_name}: {acts.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SAEs on Multiple Layers\n",
    "\n",
    "Now let's train SAEs on each layer's activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuros_mechint import MultiLayerSAETrainer\n",
    "\n",
    "# Configure SAE training for each layer\n",
    "sae_config = {\n",
    "    'expansion_factor': 4,  # 4x overcomplete\n",
    "    'sparsity_coef': 0.01,\n",
    "    'tied_weights': False,\n",
    "    'num_epochs': 150,\n",
    "    'batch_size': 128,\n",
    "    'lr': 1e-3\n",
    "}\n",
    "\n",
    "# Train SAEs for all layers\n",
    "trainer = MultiLayerSAETrainer(config=sae_config)\n",
    "trained_saes = trainer.train(\n",
    "    activations_dict=collected_acts,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Trained SAEs for all layers!\")\n",
    "print(f\"Total SAEs: {len(trained_saes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing SAEs Across Layers\n",
    "\n",
    "Let's visualize how features differ across layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuros_mechint import MultiLayerSAEVisualizer\n",
    "\n",
    "# Create visualizer\n",
    "visualizer = MultiLayerSAEVisualizer(saes=trained_saes)\n",
    "\n",
    "# Compare sparsity across layers\n",
    "fig = visualizer.plot_sparsity_comparison(collected_acts)\n",
    "plt.show()\n",
    "\n",
    "# Compare reconstruction quality\n",
    "fig = visualizer.plot_reconstruction_comparison(collected_acts)\n",
    "plt.show()\n",
    "\n",
    "# Feature frequency distributions\n",
    "fig = visualizer.plot_feature_frequency_distributions(collected_acts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Hierarchical SAEs - Multi-Level Concept Extraction\n",
    "\n",
    "### The Idea: Concept Hierarchies\n",
    "\n",
    "Real-world concepts have hierarchical structure:\n",
    "```\n",
    "Abstract:   \"Animal\"  → \"Mammal\"  → \"Dog\" → \"Golden Retriever\"\n",
    "Fine:       edges     → textures → parts → whole object\n",
    "```\n",
    "\n",
    "**Hierarchical SAEs** learn this structure by training multiple SAE levels:\n",
    "- **Level 0** (bottom): Fine-grained features\n",
    "- **Level 1** (middle): Mid-level concepts  \n",
    "- **Level 2** (top): Abstract concepts\n",
    "\n",
    "Each level encodes the previous level's features!\n",
    "\n",
    "### Training a Hierarchical SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use activations from the MLP layer\n",
    "mlp_acts = collected_acts['linear1']\n",
    "\n",
    "# Create hierarchical SAE with 3 levels\n",
    "hier_sae = HierarchicalSAE(\n",
    "    input_dim=mlp_acts.shape[1],\n",
    "    hidden_dims=[mlp_acts.shape[1] * 4, 128, 64],  # Each level's size\n",
    "    sparsity_coefs=[0.01, 0.02, 0.03],  # Increasing sparsity at higher levels\n",
    "    num_levels=3\n",
    ")\n",
    "\n",
    "print(\"Hierarchical SAE Architecture:\")\n",
    "print(f\"  Input: {mlp_acts.shape[1]} neurons\")\n",
    "print(f\"  Level 0: {mlp_acts.shape[1] * 4} fine-grained features\")\n",
    "print(f\"  Level 1: 128 mid-level concepts\")\n",
    "print(f\"  Level 2: 64 abstract concepts\")\n",
    "\n",
    "# Train hierarchically (bottom-up)\n",
    "print(\"\\nTraining hierarchical SAE...\")\n",
    "hier_losses = hier_sae.train_hierarchical(\n",
    "    activations=mlp_acts,\n",
    "    num_epochs_per_level=[100, 100, 100],\n",
    "    batch_size=128,\n",
    "    lr=1e-3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualize hierarchy\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for level in range(3):\n",
    "    axes[level].plot(hier_losses[level])\n",
    "    axes[level].set_title(f'Level {level} Training Loss')\n",
    "    axes[level].set_xlabel('Epoch')\n",
    "    axes[level].set_ylabel('Loss')\n",
    "    axes[level].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Concept Tree\n",
    "\n",
    "Now let's see which low-level features connect to higher-level concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hierarchical encodings\n",
    "test_sample = mlp_acts[:10]  # Use 10 test samples\n",
    "\n",
    "# Encode at each level\n",
    "level_0_features = hier_sae.encode_level(test_sample, level=0)\n",
    "level_1_concepts = hier_sae.encode_level(level_0_features, level=1)\n",
    "level_2_concepts = hier_sae.encode_level(level_1_concepts, level=2)\n",
    "\n",
    "print(\"Hierarchical encoding:\")\n",
    "print(f\"  Level 0: {(level_0_features > 0).sum().item()} active features\")\n",
    "print(f\"  Level 1: {(level_1_concepts > 0).sum().item()} active concepts\")\n",
    "print(f\"  Level 2: {(level_2_concepts > 0).sum().item()} active concepts\")\n",
    "\n",
    "# Build concept tree: which level-0 features contribute to level-1?\n",
    "concept_tree = hier_sae.get_concept_tree(test_sample)\n",
    "\n",
    "# Visualize for one example\n",
    "example_idx = 0\n",
    "print(f\"\\nConcept tree for sample {example_idx}:\")\n",
    "\n",
    "# Find active level-1 concepts\n",
    "active_l1 = torch.where(level_1_concepts[example_idx] > 0)[0][:5]  # Top 5\n",
    "\n",
    "for l1_idx in active_l1:\n",
    "    print(f\"\\n  Level-1 Concept {l1_idx.item()}:\")\n",
    "    \n",
    "    # Find level-0 features that contribute to this concept\n",
    "    l0_contributors = concept_tree[l1_idx.item()]['level_0_features'][:3]  # Top 3\n",
    "    \n",
    "    for l0_idx in l0_contributors:\n",
    "        print(f\"    ← Level-0 Feature {l0_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Causal SAE Interventions\n",
    "\n",
    "### From Features to Causal Understanding\n",
    "\n",
    "Finding features is great, but we need to know:\n",
    "- **Are these features actually used?**\n",
    "- **Which features are causally important?**\n",
    "\n",
    "Enter: **Causal SAE Probes**\n",
    "\n",
    "### The CausalSAEProbe Class\n",
    "\n",
    "This allows us to:\n",
    "1. **Ablate features**: Set features to zero\n",
    "2. **Reinsert features**: Force features to specific values\n",
    "3. **Measure causal importance**: Which features affect the output?\n",
    "\n",
    "Let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a causal probe\n",
    "causal_probe = CausalSAEProbe(\n",
    "    model=transformer,\n",
    "    layer_names=['linear1'],\n",
    "    saes={'linear1': trained_saes['linear1']}\n",
    ")\n",
    "\n",
    "# Test data\n",
    "test_input = torch.randn(seq_len, 1, d_model).to(device)\n",
    "\n",
    "print(\"Computing causal importance of SAE features...\")\n",
    "importance_scores = causal_probe.causal_importance_scores(\n",
    "    input_data=test_input,\n",
    "    target_layer='linear1',\n",
    "    target_neuron=10,  # Measure impact on neuron 10\n",
    "    num_samples=100,\n",
    "    method='ablation'  # Ablate each feature individually\n",
    ")\n",
    "\n",
    "# Visualize most important features\n",
    "top_k = 20\n",
    "top_features = importance_scores.argsort(descending=True)[:top_k]\n",
    "top_scores = importance_scores[top_features]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(top_k), top_scores.cpu().numpy())\n",
    "plt.xlabel('Feature Index (ranked)')\n",
    "plt.ylabel('Causal Importance Score')\n",
    "plt.title(f'Top {top_k} Most Causally Important Features')\n",
    "plt.xticks(range(top_k), top_features.cpu().numpy(), rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 5 most important features: {top_features[:5].tolist()}\")\n",
    "print(f\"Their importance scores: {top_scores[:5].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Steering: Controlling Model Behavior\n",
    "\n",
    "We can also **steer** the model by activating specific features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a feature to amplify\n",
    "feature_to_amplify = top_features[0].item()\n",
    "\n",
    "print(f\"Steering model by amplifying feature {feature_to_amplify}...\")\n",
    "\n",
    "# Normal forward pass\n",
    "with torch.no_grad():\n",
    "    normal_output = transformer(test_input)\n",
    "\n",
    "# Amplify the feature\n",
    "steered_output = causal_probe.reinsert_feature(\n",
    "    input_data=test_input,\n",
    "    layer='linear1',\n",
    "    feature_idx=feature_to_amplify,\n",
    "    feature_value=5.0  # Amplify to value 5.0\n",
    ")\n",
    "\n",
    "# Compare outputs\n",
    "output_change = (steered_output - normal_output).abs().mean()\n",
    "print(f\"Average change in output: {output_change.item():.4f}\")\n",
    "\n",
    "# Visualize the effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot output changes\n",
    "change = (steered_output - normal_output).squeeze().cpu().numpy()\n",
    "im = axes[0].imshow(change, aspect='auto', cmap='RdBu_r', vmin=-change.max(), vmax=change.max())\n",
    "axes[0].set_xlabel('Feature Dimension')\n",
    "axes[0].set_ylabel('Sequence Position')\n",
    "axes[0].set_title(f'Output Change from Amplifying Feature {feature_to_amplify}')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Plot magnitude of changes\n",
    "axes[1].plot(change.mean(axis=0))\n",
    "axes[1].set_xlabel('Feature Dimension')\n",
    "axes[1].set_ylabel('Average Change')\n",
    "axes[1].set_title('Average Output Change per Dimension')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Successfully steered the model using SAE features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Best Practices & Practical Tips\n",
    "\n",
    "### Hyperparameter Selection\n",
    "\n",
    "**Expansion Factor** ($m/d$ ratio):\n",
    "- Start with 4x (proven to work well)\n",
    "- Increase to 8x or 16x for very polysemantic layers\n",
    "- Larger = more features but slower training\n",
    "\n",
    "**Sparsity Coefficient** ($\\lambda$):\n",
    "- Too low → dense activations (not sparse enough)\n",
    "- Too high → poor reconstruction\n",
    "- Good range: [0.001, 0.1]\n",
    "- Use cross-validation to find optimal value\n",
    "\n",
    "**Tied vs Untied Weights**:\n",
    "- Tied ($W_d = W_e^T$): Fewer parameters, faster\n",
    "- Untied: More flexible, better reconstruction\n",
    "- Start with tied, switch to untied if reconstruction is poor\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "1. **Collect enough data**: At least 10,000+ activation samples\n",
    "2. **Normalize activations**: Can help training stability\n",
    "3. **Monitor both metrics**: Reconstruction AND sparsity\n",
    "4. **Use learning rate decay**: Cosine annealing works well\n",
    "5. **Check for dead features**: Features that never activate\n",
    "\n",
    "### Evaluation Checklist\n",
    "\n",
    "Good SAE should have:\n",
    "- ✓ **Sparsity**: < 10% features active per sample\n",
    "- ✓ **Reconstruction**: > 90% variance explained\n",
    "- ✓ **Feature diversity**: Not too many dead features\n",
    "- ✓ **Interpretability**: Features correspond to concepts\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "**Problem**: All features are nearly identical\n",
    "- **Cause**: Sparsity too high, collapsed solution\n",
    "- **Fix**: Reduce sparsity coefficient\n",
    "\n",
    "**Problem**: Poor reconstruction\n",
    "- **Cause**: Not enough features or too much sparsity\n",
    "- **Fix**: Increase expansion factor or reduce sparsity\n",
    "\n",
    "**Problem**: Features not interpretable\n",
    "- **Cause**: Layer doesn't have clean features\n",
    "- **Fix**: Try different layers, increase expansion factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What You've Mastered\n",
    "\n",
    "1. ✓ **Problem understanding**: Polysemanticity makes neurons uninterpretable\n",
    "2. ✓ **Core technique**: Sparse Autoencoders find monosemantic features\n",
    "3. ✓ **Training SAEs**: On both toy and real models\n",
    "4. ✓ **Multi-layer analysis**: Comparing features across layers\n",
    "5. ✓ **Hierarchical SAEs**: Extracting concept hierarchies\n",
    "6. ✓ **Causal interventions**: Testing feature importance\n",
    "7. ✓ **Feature steering**: Controlling model behavior\n",
    "\n",
    "### Key Equations to Remember\n",
    "\n",
    "**SAE Loss**:\n",
    "$$\\mathcal{L} = \\|h - \\hat{h}\\|^2 + \\lambda \\|f(h)\\|_1$$\n",
    "\n",
    "**Sparsity**:\n",
    "$$\\text{Sparsity} = \\frac{\\#(\\text{active features})}{\\#(\\text{total features})}$$\n",
    "\n",
    "**Explained Variance**:\n",
    "$$R^2 = 1 - \\frac{\\text{MSE}(\\hat{h}, h)}{\\text{Var}(h)}$$\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "You can now:\n",
    "- Extract interpretable features from any neural network\n",
    "- Build hierarchical concept dictionaries\n",
    "- Identify causally important features\n",
    "- Steer model behavior by manipulating features\n",
    "- Compare feature representations across models\n",
    "\n",
    "### Next Notebook\n",
    "\n",
    "**[03_causal_interventions.ipynb](03_causal_interventions.ipynb)**\n",
    "\n",
    "Now that we have interpretable features, let's use them to discover **computational circuits**:\n",
    "- Advanced activation patching techniques\n",
    "- Path tracing through networks\n",
    "- Building complete causal graphs\n",
    "- Identifying minimal sufficient subnetworks\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "**Essential Papers**:\n",
    "1. Anthropic (2023): \"Towards Monosemanticity: Decomposing Language Models With Dictionary Learning\"\n",
    "2. Bricken et al. (2023): \"Monosemanticity in Toy Models\"\n",
    "3. Cunningham et al. (2023): \"Sparse Autoencoders Find Highly Interpretable Features\"\n",
    "\n",
    "**Code & Resources**:\n",
    "- [Neuronpedia](https://neuronpedia.org/): Browse SAE features from large models\n",
    "- [TransformerLens](https://github.com/neelnanda-io/TransformerLens): SAE tools for transformers\n",
    "\n",
    "---\n",
    "\n",
    "Great work! You now understand one of the most important tools in mechanistic interpretability. Let's continue the journey! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
