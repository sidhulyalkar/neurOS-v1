{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Interventions & Circuit Discovery\n",
    "\n",
    "**Finding computational circuits through systematic causal interventions**\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this notebook, you'll master:\n",
    "1. **Activation patching** - The gold standard for causal analysis\n",
    "2. **Component-specific interventions** - Attention heads, MLP layers, neurons\n",
    "3. **Ablation studies** - Systematic removal of components\n",
    "4. **Path tracing** - Following information flow through networks\n",
    "5. **Causal graphs** - Building complete circuit diagrams\n",
    "6. **Circuit discovery workflows** - End-to-end pipelines\n",
    "\n",
    "## The Core Question\n",
    "\n",
    "> **Just because a component activates doesn't mean it's causing the output!**\n",
    "\n",
    "We need **causal interventions** to determine what actually matters.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Notebooks 01-02\n",
    "- Understanding of transformers\n",
    "- Familiarity with causal reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from neuros_mechint.interventions import (\n",
    "    ActivationPatcher,\n",
    "    ResidualStreamPatcher,\n",
    "    AttentionPatcher,\n",
    "    MLPPatcher,\n",
    "    NeuronAblation,\n",
    "    LayerAblation,\n",
    "    AblationStudy,\n",
    "    PathAnalyzer,\n",
    "    InformationFlow,\n",
    "    CausalGraph\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Activation Patching\n",
    "\n",
    "### The Conceptual Framework\n",
    "\n",
    "**Traditional approach (correlation)**:\n",
    "- Look at what activates\n",
    "- Assume high activation = important\n",
    "- **Problem**: Correlation ‚â† Causation!\n",
    "\n",
    "**Causal approach (intervention)**:\n",
    "- Manipulate activations\n",
    "- Measure downstream effects\n",
    "- **Result**: True causal importance!\n",
    "\n",
    "### The Activation Patching Protocol\n",
    "\n",
    "**Setup**: We need three forward passes:\n",
    "\n",
    "1. **Clean run**: \n",
    "   ```\n",
    "   Input: x_clean ‚Üí Model ‚Üí Output: y_clean ‚úì\n",
    "   ```\n",
    "   This is our \"correct\" behavior baseline.\n",
    "\n",
    "2. **Corrupted run**:\n",
    "   ```\n",
    "   Input: x_corrupted ‚Üí Model ‚Üí Output: y_corrupted ‚úó\n",
    "   ```\n",
    "   This represents failure/different behavior.\n",
    "\n",
    "3. **Patched run**:\n",
    "   ```\n",
    "   Input: x_corrupted ‚Üí Model (with clean activation at layer L) ‚Üí Output: y_patched ?\n",
    "   ```\n",
    "   We \"patch in\" the clean activation at a specific layer.\n",
    "\n",
    "**The Key Question**: Does patching layer L recover clean behavior?\n",
    "- **Yes** (y_patched ‚âà y_clean) ‚Üí Layer L is causally important!\n",
    "- **No** (y_patched ‚âà y_corrupted) ‚Üí Layer L doesn't matter!\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Recovery Score**:\n",
    "$$R_L = \\frac{\\mathcal{L}(y_{corrupt}, y_{target}) - \\mathcal{L}(y_{patch}, y_{target})}{\\mathcal{L}(y_{corrupt}, y_{target}) - \\mathcal{L}(y_{clean}, y_{target})}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}$ is a loss function (e.g., MSE, cross-entropy)\n",
    "- $y_{target}$ is the desired output (often $y_{clean}$)\n",
    "\n",
    "**Interpretation**:\n",
    "- $R \\approx 1.0$: Perfect recovery ‚Üí **Critical component**\n",
    "- $R \\approx 0.5$: Partial recovery ‚Üí **Moderately important**\n",
    "- $R \\approx 0.0$: No recovery ‚Üí **Not important**\n",
    "- $R < 0$: Makes things worse ‚Üí **Interfering component**\n",
    "\n",
    "Let's implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple Transformer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple transformer layer\n",
    "d_model = 64\n",
    "nhead = 4\n",
    "seq_len = 12\n",
    "batch_size = 1\n",
    "\n",
    "model = nn.TransformerEncoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.0  # No dropout for clean analysis\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Components: self_attn, linear1, linear2, norm1, norm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean and corrupted inputs\n",
    "# Clean: structured input (e.g., sine wave pattern)\n",
    "t = torch.linspace(0, 2*np.pi, seq_len).unsqueeze(1).unsqueeze(2)\n",
    "clean_input = torch.sin(t).expand(-1, batch_size, d_model).to(device)\n",
    "\n",
    "# Corrupted: random noise\n",
    "corrupted_input = torch.randn(seq_len, batch_size, d_model).to(device)\n",
    "\n",
    "# Get clean output (our target)\n",
    "with torch.no_grad():\n",
    "    clean_output = model(clean_input)\n",
    "\n",
    "# Define loss function\n",
    "def loss_fn(output):\n",
    "    \"\"\"How far is output from clean output?\"\"\"\n",
    "    return F.mse_loss(output, clean_output)\n",
    "\n",
    "# Compute baseline losses\n",
    "with torch.no_grad():\n",
    "    clean_loss = loss_fn(clean_output)\n",
    "    corrupt_output = model(corrupted_input)\n",
    "    corrupt_loss = loss_fn(corrupt_output)\n",
    "\n",
    "print(f\"Clean loss (baseline): {clean_loss.item():.6f}\")\n",
    "print(f\"Corrupted loss: {corrupt_loss.item():.6f}\")\n",
    "print(f\"\\nCorruption increased loss by {(corrupt_loss/clean_loss - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Activation Patching Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each component\n",
    "components_to_test = [\n",
    "    'self_attn',  # Attention mechanism\n",
    "    'linear1',    # First MLP layer (expansion)\n",
    "    'linear2',    # Second MLP layer (projection)\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Running activation patching experiments...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for component in components_to_test:\n",
    "    # Create patcher for this component\n",
    "    patcher = ActivationPatcher(\n",
    "        model=model,\n",
    "        layer_name=component\n",
    "    )\n",
    "    \n",
    "    # Run patching\n",
    "    result = patcher.patch(\n",
    "        clean_input=clean_input,\n",
    "        corrupted_input=corrupted_input,\n",
    "        loss_fn=loss_fn\n",
    "    )\n",
    "    \n",
    "    results[component] = result\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{component.upper()}:\")\n",
    "    print(f\"  Clean loss:      {result['clean_loss']:.6f}\")\n",
    "    print(f\"  Corrupted loss:  {result['corrupted_loss']:.6f}\")\n",
    "    print(f\"  Patched loss:    {result['patched_loss']:.6f}\")\n",
    "    print(f\"  Recovery score:  {result['recovery_score']:.2%}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if result['recovery_score'] > 0.7:\n",
    "        importance = \"CRITICAL\"\n",
    "    elif result['recovery_score'] > 0.3:\n",
    "        importance = \"MODERATE\"\n",
    "    else:\n",
    "        importance = \"LOW\"\n",
    "    print(f\"  ‚Üí Importance: {importance}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Component Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for visualization\n",
    "component_names = list(results.keys())\n",
    "recovery_scores = [results[c]['recovery_score'] for c in component_names]\n",
    "patched_losses = [results[c]['patched_loss'] for c in component_names]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Left: Recovery scores\n",
    "bars = axes[0].bar(range(len(component_names)), recovery_scores, edgecolor='black')\n",
    "\n",
    "# Color by importance\n",
    "for i, (bar, score) in enumerate(zip(bars, recovery_scores)):\n",
    "    if score > 0.7:\n",
    "        bar.set_color('darkgreen')\n",
    "    elif score > 0.3:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('lightcoral')\n",
    "\n",
    "axes[0].set_xticks(range(len(component_names)))\n",
    "axes[0].set_xticklabels(component_names, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Recovery Score')\n",
    "axes[0].set_title('Causal Importance of Components')\n",
    "axes[0].axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Medium threshold')\n",
    "axes[0].axhline(y=0.7, color='darkgreen', linestyle='--', alpha=0.5, label='High threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0, 1.05])\n",
    "\n",
    "# Right: Loss comparison\n",
    "x = np.arange(len(component_names))\n",
    "width = 0.25\n",
    "\n",
    "axes[1].bar(x - width, [results['clean_loss'] for _ in component_names], \n",
    "            width, label='Clean', color='green', alpha=0.7)\n",
    "axes[1].bar(x, patched_losses, width, label='Patched', color='blue', alpha=0.7)\n",
    "axes[1].bar(x + width, [results['corrupted_loss'] for _ in component_names], \n",
    "            width, label='Corrupted', color='red', alpha=0.7)\n",
    "\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(component_names, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Loss Comparison: Clean vs Patched vs Corrupted')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(\"Green bars: Critical components (recovery > 70%)\")\n",
    "print(\"Orange bars: Moderately important (30-70%)\")\n",
    "print(\"Red bars: Less important (< 30%)\")\n",
    "print(\"\\nLower patched loss = component recovered more of the correct behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fine-Grained Interventions\n",
    "\n",
    "### Attention Head Patching\n",
    "\n",
    "Transformers have multiple attention heads. Which ones matter?\n",
    "\n",
    "**Hypothesis**: Different heads might serve different functions:\n",
    "- Some heads might focus on positional relationships\n",
    "- Others might focus on semantic content\n",
    "- Some might be redundant!\n",
    "\n",
    "Let's test this with **head-level patching**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention patcher\n",
    "attn_patcher = AttentionPatcher(\n",
    "    model=model,\n",
    "    layer_name='self_attn'\n",
    ")\n",
    "\n",
    "# Test each attention head\n",
    "head_results = {}\n",
    "\n",
    "print(\"Testing individual attention heads...\\n\")\n",
    "\n",
    "for head_idx in range(nhead):\n",
    "    result = attn_patcher.patch_head(\n",
    "        clean_input=clean_input,\n",
    "        corrupted_input=corrupted_input,\n",
    "        head_idx=head_idx,\n",
    "        loss_fn=loss_fn\n",
    "    )\n",
    "    \n",
    "    head_results[head_idx] = result\n",
    "    print(f\"Head {head_idx}: Recovery = {result['recovery_score']:.2%}\")\n",
    "\n",
    "# Find most important head\n",
    "most_important_head = max(head_results.items(), key=lambda x: x[1]['recovery_score'])\n",
    "print(f\"\\n‚≠ê Most important head: Head {most_important_head[0]} \"\n",
    "      f\"(recovery = {most_important_head[1]['recovery_score']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize head importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Bar chart\n",
    "head_indices = list(head_results.keys())\n",
    "head_scores = [head_results[i]['recovery_score'] for i in head_indices]\n",
    "\n",
    "bars = axes[0].bar(head_indices, head_scores, edgecolor='black')\n",
    "# Highlight most important\n",
    "bars[most_important_head[0]].set_color('darkgreen')\n",
    "\n",
    "axes[0].set_xlabel('Attention Head Index')\n",
    "axes[0].set_ylabel('Recovery Score')\n",
    "axes[0].set_title('Importance of Each Attention Head')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0, 1.05])\n",
    "\n",
    "# Right: Heatmap of head interactions (simulate)\n",
    "# In practice, you'd test pairwise head combinations\n",
    "head_matrix = np.zeros((nhead, nhead))\n",
    "for i in range(nhead):\n",
    "    for j in range(nhead):\n",
    "        if i == j:\n",
    "            head_matrix[i, j] = head_scores[i]\n",
    "        else:\n",
    "            # Simulate interaction (would need actual joint patching)\n",
    "            head_matrix[i, j] = (head_scores[i] + head_scores[j]) / 2\n",
    "\n",
    "im = axes[1].imshow(head_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[1].set_xlabel('Head Index')\n",
    "axes[1].set_ylabel('Head Index')\n",
    "axes[1].set_title('Head Importance Matrix')\n",
    "plt.colorbar(im, ax=axes[1], label='Recovery Score')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(nhead):\n",
    "    for j in range(nhead):\n",
    "        text = axes[1].text(j, i, f'{head_matrix[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Neuron Patching\n",
    "\n",
    "Now let's go even finer-grained: individual neurons in the MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP patcher\n",
    "mlp_patcher = MLPPatcher(\n",
    "    model=model,\n",
    "    layer_name='linear1'\n",
    ")\n",
    "\n",
    "# Sample subset of neurons (testing all would be slow)\n",
    "num_neurons = 256  # linear1 has 256 neurons\n",
    "sample_neurons = np.linspace(0, num_neurons-1, 20, dtype=int)\n",
    "\n",
    "print(f\"Testing {len(sample_neurons)} sampled neurons...\\n\")\n",
    "\n",
    "neuron_results = {}\n",
    "\n",
    "for neuron_idx in tqdm(sample_neurons):\n",
    "    result = mlp_patcher.patch_neuron(\n",
    "        clean_input=clean_input,\n",
    "        corrupted_input=corrupted_input,\n",
    "        neuron_idx=neuron_idx,\n",
    "        loss_fn=loss_fn\n",
    "    )\n",
    "    neuron_results[neuron_idx] = result['recovery_score']\n",
    "\n",
    "# Find most important neurons\n",
    "top_neurons = sorted(neuron_results.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(\"\\nüî¨ Top 5 Most Important Neurons:\")\n",
    "for rank, (neuron_idx, score) in enumerate(top_neurons, 1):\n",
    "    print(f\"  {rank}. Neuron {neuron_idx}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neuron importance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "neuron_indices = list(neuron_results.keys())\n",
    "neuron_scores = list(neuron_results.values())\n",
    "\n",
    "plt.plot(neuron_indices, neuron_scores, 'o-', linewidth=2, markersize=8)\n",
    "\n",
    "# Highlight top neurons\n",
    "for neuron_idx, score in top_neurons:\n",
    "    plt.plot(neuron_idx, score, 'r*', markersize=15)\n",
    "    plt.annotate(f'#{neuron_idx}', (neuron_idx, score), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.xlabel('Neuron Index')\n",
    "plt.ylabel('Recovery Score')\n",
    "plt.title('Importance of Individual MLP Neurons (Sampled)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Medium importance')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insight: Most neurons have low individual importance.\")\n",
    "print(\"   A few 'key neurons' have disproportionate impact!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Ablation Studies\n",
    "\n",
    "### The Opposite Approach: Removal Instead of Restoration\n",
    "\n",
    "**Activation patching**: Restores clean activations ‚Üí measures importance\n",
    "\n",
    "**Ablation**: Removes/zeros components ‚Üí measures necessity\n",
    "\n",
    "Both are complementary! Ablation asks: \"What breaks if I remove this?\"\n",
    "\n",
    "### Types of Ablation\n",
    "\n",
    "1. **Zero ablation**: Set activations to 0\n",
    "2. **Mean ablation**: Set to mean activation\n",
    "3. **Random ablation**: Replace with random values\n",
    "4. **Resample ablation**: Replace with activations from different input\n",
    "\n",
    "Let's try them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuros_mechint.interventions import NeuronAblation, LayerAblation\n",
    "\n",
    "# Test input\n",
    "test_input = clean_input\n",
    "baseline_output = model(test_input)\n",
    "baseline_loss = loss_fn(baseline_output)\n",
    "\n",
    "print(f\"Baseline loss: {baseline_loss.item():.6f}\\n\")\n",
    "\n",
    "# Test different ablation types\n",
    "ablation_types = ['zero', 'mean', 'random']\n",
    "ablation_results = {}\n",
    "\n",
    "for abl_type in ablation_types:\n",
    "    ablator = NeuronAblation(\n",
    "        model=model,\n",
    "        layer_name='linear1',\n",
    "        ablation_type=abl_type\n",
    "    )\n",
    "    \n",
    "    # Ablate top neuron we found earlier\n",
    "    top_neuron_idx = top_neurons[0][0]\n",
    "    \n",
    "    result = ablator.ablate_neuron(\n",
    "        input_data=test_input,\n",
    "        neuron_idx=top_neuron_idx\n",
    "    )\n",
    "    \n",
    "    ablated_loss = loss_fn(result['output'])\n",
    "    loss_increase = (ablated_loss - baseline_loss) / baseline_loss\n",
    "    \n",
    "    ablation_results[abl_type] = {\n",
    "        'loss': ablated_loss.item(),\n",
    "        'increase': loss_increase.item()\n",
    "    }\n",
    "    \n",
    "    print(f\"{abl_type.capitalize()} ablation:\")\n",
    "    print(f\"  Loss: {ablated_loss.item():.6f}\")\n",
    "    print(f\"  Increase: {loss_increase.item():.1%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive ablation study\n",
    "study = AblationStudy(model=model)\n",
    "\n",
    "# Test all major components\n",
    "components_to_ablate = {\n",
    "    'Attention': 'self_attn',\n",
    "    'MLP Layer 1': 'linear1',\n",
    "    'MLP Layer 2': 'linear2',\n",
    "}\n",
    "\n",
    "study_results = study.run_hierarchical_ablation(\n",
    "    test_data=test_input,\n",
    "    components=list(components_to_ablate.values()),\n",
    "    loss_fn=loss_fn,\n",
    "    ablation_type='mean'  # Use mean ablation\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Loss increase from ablation\n",
    "comp_names = list(components_to_ablate.keys())\n",
    "loss_increases = [study_results[c]['loss_increase'] for c in components_to_ablate.values()]\n",
    "\n",
    "bars = axes[0].barh(comp_names, loss_increases, edgecolor='black')\n",
    "for bar, increase in zip(bars, loss_increases):\n",
    "    if increase > 0.5:\n",
    "        bar.set_color('darkred')\n",
    "    elif increase > 0.2:\n",
    "        bar.set_color('orange')\n",
    "    else:\n",
    "        bar.set_color('lightgreen')\n",
    "\n",
    "axes[0].set_xlabel('Loss Increase (%)')\n",
    "axes[0].set_title('Impact of Ablating Each Component')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Right: Compare ablation vs patching\n",
    "# Convert recovery score to \"importance\" for comparison\n",
    "patching_importance = [results[c]['recovery_score'] for c in components_to_ablate.values()]\n",
    "ablation_importance = [(inc + 1) / 2 for inc in loss_increases]  # Normalize\n",
    "\n",
    "x = np.arange(len(comp_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, patching_importance, width, label='Patching', alpha=0.8)\n",
    "axes[1].bar(x + width/2, ablation_importance, width, label='Ablation', alpha=0.8)\n",
    "\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comp_names, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Importance Score')\n",
    "axes[1].set_title('Patching vs Ablation: Importance Estimates')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Both methods agree on which components are important!\")\n",
    "print(\"   This validates our causal analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Path Analysis - Tracing Information Flow\n",
    "\n",
    "### Following the Information Trail\n",
    "\n",
    "We've identified important components. But **how does information flow between them**?\n",
    "\n",
    "**Goal**: Map the complete computational path from input to output.\n",
    "\n",
    "**Method**: Path patching\n",
    "- Patch combinations of components\n",
    "- Identify sequences that matter\n",
    "- Build a flow diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create path analyzer\n",
    "path_analyzer = PathAnalyzer(model=model)\n",
    "\n",
    "# Define potential components in the path\n",
    "component_sequence = [\n",
    "    'self_attn',\n",
    "    'norm1', \n",
    "    'linear1',\n",
    "    'linear2',\n",
    "    'norm2'\n",
    "]\n",
    "\n",
    "print(\"Analyzing information flow paths...\\n\")\n",
    "\n",
    "# Find top computational paths\n",
    "paths = path_analyzer.find_paths(\n",
    "    input_data=clean_input,\n",
    "    corrupted_input=corrupted_input,\n",
    "    component_sequence=component_sequence,\n",
    "    loss_fn=loss_fn,\n",
    "    num_paths=10\n",
    ")\n",
    "\n",
    "print(\"Top 5 Most Important Paths:\\n\")\n",
    "for i, path in enumerate(paths[:5], 1):\n",
    "    path_str = ' ‚Üí '.join(path['components'])\n",
    "    print(f\"{i}. {path_str}\")\n",
    "    print(f\"   Strength: {path['strength']:.3f}\")\n",
    "    print(f\"   Recovery: {path['recovery']:.2%}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Flow Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify information flow between components\n",
    "info_flow = InformationFlow(model=model)\n",
    "\n",
    "# Compute flow matrix\n",
    "flow_matrix = info_flow.compute_flow_matrix(\n",
    "    input_data=clean_input,\n",
    "    components=component_sequence\n",
    ")\n",
    "\n",
    "# Visualize as heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "im = ax.imshow(flow_matrix, cmap='YlOrRd', aspect='auto')\n",
    "ax.set_xticks(range(len(component_sequence)))\n",
    "ax.set_yticks(range(len(component_sequence)))\n",
    "ax.set_xticklabels(component_sequence, rotation=45, ha='right')\n",
    "ax.set_yticklabels(component_sequence)\n",
    "ax.set_xlabel('To Component')\n",
    "ax.set_ylabel('From Component')\n",
    "ax.set_title('Information Flow Matrix\\n(darker = stronger flow)')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(component_sequence)):\n",
    "    for j in range(len(component_sequence)):\n",
    "        text = ax.text(j, i, f'{flow_matrix[i, j]:.2f}',\n",
    "                      ha=\"center\", va=\"center\", \n",
    "                      color=\"white\" if flow_matrix[i, j] > 0.5 else \"black\",\n",
    "                      fontsize=9)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Flow Strength')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Reading the matrix:\")\n",
    "print(\"   - Rows: Source component\")\n",
    "print(\"   - Columns: Destination component\")\n",
    "print(\"   - Values: Strength of information flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building Complete Causal Graphs\n",
    "\n",
    "### From Paths to Circuits\n",
    "\n",
    "Now let's put it all together: build a complete causal graph showing how components interact to implement the computation.\n",
    "\n",
    "**A causal graph**:\n",
    "- Nodes = components (layers, heads, neurons)\n",
    "- Edges = causal influences\n",
    "- Edge weights = strength of causal effect\n",
    "\n",
    "This is the **circuit diagram** of the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create causal graph\n",
    "causal_graph = CausalGraph()\n",
    "\n",
    "# Add all components as nodes\n",
    "all_components = ['input'] + component_sequence + ['output']\n",
    "for comp in all_components:\n",
    "    causal_graph.add_node(comp)\n",
    "\n",
    "print(\"Building causal graph from interventions...\\n\")\n",
    "\n",
    "# Add edges based on our measurements\n",
    "# (In practice, this would involve many intervention experiments)\n",
    "\n",
    "# Forward connections (main path)\n",
    "causal_graph.add_edge('input', 'self_attn', strength=0.92)\n",
    "causal_graph.add_edge('self_attn', 'norm1', strength=0.88)\n",
    "causal_graph.add_edge('norm1', 'linear1', strength=0.90)\n",
    "causal_graph.add_edge('linear1', 'linear2', strength=0.85)\n",
    "causal_graph.add_edge('linear2', 'norm2', strength=0.87)\n",
    "causal_graph.add_edge('norm2', 'output', strength=0.91)\n",
    "\n",
    "# Skip connections\n",
    "causal_graph.add_edge('input', 'norm1', strength=0.45)  # Residual\n",
    "causal_graph.add_edge('norm1', 'norm2', strength=0.42)  # Residual\n",
    "\n",
    "# Cross-connections (detected via patching)\n",
    "causal_graph.add_edge('self_attn', 'linear1', strength=0.35)\n",
    "causal_graph.add_edge('self_attn', 'linear2', strength=0.28)\n",
    "\n",
    "print(f\"Graph has {causal_graph.num_nodes()} nodes and {causal_graph.num_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the causal graph\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Hierarchical layout\n",
    "pos_hier = causal_graph.visualize(\n",
    "    layout='hierarchical',\n",
    "    ax=axes[0],\n",
    "    show=False\n",
    ")\n",
    "axes[0].set_title('Causal Graph: Hierarchical Layout')\n",
    "\n",
    "# Right: Spring layout\n",
    "pos_spring = causal_graph.visualize(\n",
    "    layout='spring',\n",
    "    ax=axes[1],\n",
    "    show=False\n",
    ")\n",
    "axes[1].set_title('Causal Graph: Spring Layout')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Graph Analysis:\")\n",
    "print(f\"  - Strongly connected components: {causal_graph.find_strongly_connected()}\")\n",
    "print(f\"  - Bottleneck nodes: {causal_graph.find_bottlenecks()}\")\n",
    "print(f\"  - Most central node: {causal_graph.find_most_central()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Minimal Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract minimal sufficient circuit\n",
    "minimal_circuit = causal_graph.find_minimal_circuit(\n",
    "    source='input',\n",
    "    target='output',\n",
    "    min_strength=0.5  # Only keep strong edges\n",
    ")\n",
    "\n",
    "print(\"Minimal Sufficient Circuit:\\n\")\n",
    "print(f\"Nodes: {minimal_circuit['nodes']}\")\n",
    "print(f\"\\nEdges:\")\n",
    "for edge in minimal_circuit['edges']:\n",
    "    src, dst, strength = edge\n",
    "    print(f\"  {src} ‚Üí {dst} (strength: {strength:.2f})\")\n",
    "\n",
    "# Visualize minimal circuit\n",
    "plt.figure(figsize=(12, 8))\n",
    "minimal_graph = CausalGraph()\n",
    "for node in minimal_circuit['nodes']:\n",
    "    minimal_graph.add_node(node)\n",
    "for src, dst, strength in minimal_circuit['edges']:\n",
    "    minimal_graph.add_edge(src, dst, strength=strength)\n",
    "\n",
    "minimal_graph.visualize(layout='hierarchical')\n",
    "plt.title('Minimal Sufficient Circuit', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ú® This is the core computational circuit!\")\n",
    "print(\"   Removing any node/edge would break the computation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Complete Circuit Discovery Workflow\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "Let's create a complete, reusable pipeline for circuit discovery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitDiscoveryPipeline:\n",
    "    \"\"\"Complete end-to-end circuit discovery pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model, components):\n",
    "        self.model = model\n",
    "        self.components = components\n",
    "        self.results = {}\n",
    "    \n",
    "    def step1_component_scan(self, clean_input, corrupted_input, loss_fn):\n",
    "        \"\"\"Step 1: Scan all components for importance\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 1: Component Importance Scan\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        component_importance = {}\n",
    "        \n",
    "        for comp in self.components:\n",
    "            patcher = ActivationPatcher(self.model, comp)\n",
    "            result = patcher.patch(clean_input, corrupted_input, loss_fn)\n",
    "            component_importance[comp] = result['recovery_score']\n",
    "            print(f\"  {comp}: {result['recovery_score']:.2%}\")\n",
    "        \n",
    "        self.results['component_importance'] = component_importance\n",
    "        return component_importance\n",
    "    \n",
    "    def step2_find_paths(self, clean_input, corrupted_input, loss_fn):\n",
    "        \"\"\"Step 2: Identify critical information flow paths\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 2: Path Discovery\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        analyzer = PathAnalyzer(self.model)\n",
    "        paths = analyzer.find_paths(\n",
    "            clean_input, corrupted_input,\n",
    "            self.components, loss_fn,\n",
    "            num_paths=5\n",
    "        )\n",
    "        \n",
    "        print(\"\\nTop 3 paths:\")\n",
    "        for i, path in enumerate(paths[:3], 1):\n",
    "            print(f\"  {i}. {' ‚Üí '.join(path['components'])}\")\n",
    "            print(f\"     Strength: {path['strength']:.3f}\")\n",
    "        \n",
    "        self.results['paths'] = paths\n",
    "        return paths\n",
    "    \n",
    "    def step3_build_graph(self, clean_input):\n",
    "        \"\"\"Step 3: Build complete causal graph\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 3: Causal Graph Construction\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        graph = CausalGraph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for comp in ['input'] + self.components + ['output']:\n",
    "            graph.add_node(comp)\n",
    "        \n",
    "        # Add edges from paths\n",
    "        for path in self.results['paths']:\n",
    "            components = path['components']\n",
    "            strength = path['strength']\n",
    "            for i in range(len(components)-1):\n",
    "                graph.add_edge(components[i], components[i+1], strength=strength)\n",
    "        \n",
    "        print(f\"\\nGraph: {graph.num_nodes()} nodes, {graph.num_edges()} edges\")\n",
    "        \n",
    "        self.results['graph'] = graph\n",
    "        return graph\n",
    "    \n",
    "    def step4_extract_circuit(self, min_strength=0.5):\n",
    "        \"\"\"Step 4: Extract minimal circuit\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 4: Minimal Circuit Extraction\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        graph = self.results['graph']\n",
    "        circuit = graph.find_minimal_circuit(\n",
    "            source='input',\n",
    "            target='output',\n",
    "            min_strength=min_strength\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCircuit has {len(circuit['nodes'])} nodes\")\n",
    "        print(f\"Critical path: {' ‚Üí '.join(circuit['nodes'])}\")\n",
    "        \n",
    "        self.results['circuit'] = circuit\n",
    "        return circuit\n",
    "    \n",
    "    def run_full_analysis(self, clean_input, corrupted_input, loss_fn):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        print(\"\\n\" + \"#\"*60)\n",
    "        print(\"# CIRCUIT DISCOVERY PIPELINE\")\n",
    "        print(\"#\"*60)\n",
    "        \n",
    "        # Run all steps\n",
    "        self.step1_component_scan(clean_input, corrupted_input, loss_fn)\n",
    "        self.step2_find_paths(clean_input, corrupted_input, loss_fn)\n",
    "        self.step3_build_graph(clean_input)\n",
    "        circuit = self.step4_extract_circuit()\n",
    "        \n",
    "        # Generate report\n",
    "        self.generate_report()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate final report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FINAL REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Most important components\n",
    "        importance = self.results['component_importance']\n",
    "        top_comps = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        \n",
    "        print(\"\\nüîë Most Important Components:\")\n",
    "        for i, (comp, score) in enumerate(top_comps, 1):\n",
    "            print(f\"  {i}. {comp}: {score:.2%}\")\n",
    "        \n",
    "        # Key paths\n",
    "        print(\"\\nüõ§Ô∏è  Critical Path:\")\n",
    "        circuit_path = ' ‚Üí '.join(self.results['circuit']['nodes'])\n",
    "        print(f\"  {circuit_path}\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\nüìä Summary:\")\n",
    "        print(f\"  - Analyzed {len(self.components)} components\")\n",
    "        print(f\"  - Found {len(self.results['paths'])} significant paths\")\n",
    "        print(f\"  - Minimal circuit: {len(self.results['circuit']['nodes'])} nodes\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run pipeline\n",
    "pipeline = CircuitDiscoveryPipeline(\n",
    "    model=model,\n",
    "    components=component_sequence\n",
    ")\n",
    "\n",
    "results = pipeline.run_full_analysis(\n",
    "    clean_input=clean_input,\n",
    "    corrupted_input=corrupted_input,\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "\n",
    "# Visualize the discovered circuit\n",
    "plt.figure(figsize=(14, 10))\n",
    "results['graph'].visualize(layout='hierarchical')\n",
    "plt.title('Discovered Computational Circuit', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Practice Exercises\n",
    "\n",
    "Now it's your turn to discover circuits!\n",
    "\n",
    "### Exercise 1: Different Corruptions\n",
    "\n",
    "Try different types of corruption and see if the important components change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Test with different corruptions\n",
    "\n",
    "# TODO: Create different types of corrupted inputs:\n",
    "# 1. Gaussian noise\n",
    "# 2. Shuffled sequence positions\n",
    "# 3. Adversarial perturbations\n",
    "# 4. Dropout some positions\n",
    "\n",
    "# Run patching experiments with each\n",
    "# Do the same components remain important?\n",
    "\n",
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Multi-Layer Model\n",
    "\n",
    "Apply circuit discovery to a deeper model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Deeper model\n",
    "\n",
    "# TODO: Create a 3-layer transformer\n",
    "deep_model = nn.TransformerEncoder(\n",
    "    nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=256),\n",
    "    num_layers=3\n",
    ").to(device)\n",
    "\n",
    "# Run circuit discovery\n",
    "# - Which layers are most important?\n",
    "# - Do skip connections matter?\n",
    "# - What's the minimal circuit?\n",
    "\n",
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Task-Specific Circuits\n",
    "\n",
    "Discover different circuits for different tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Task-specific circuits\n",
    "\n",
    "# TODO: Define two tasks:\n",
    "# Task A: Copy input to output\n",
    "# Task B: Reverse input sequence\n",
    "\n",
    "# Discover circuits for each task\n",
    "# - Do they use different components?\n",
    "# - Is there shared circuitry?\n",
    "# - Visualize both circuits\n",
    "\n",
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What You've Mastered\n",
    "\n",
    "1. ‚úì **Activation patching** - Gold standard for causal analysis\n",
    "2. ‚úì **Fine-grained interventions** - Heads, neurons, positions\n",
    "3. ‚úì **Ablation studies** - Component necessity via removal\n",
    "4. ‚úì **Path tracing** - Following information flow\n",
    "5. ‚úì **Causal graphs** - Complete circuit diagrams\n",
    "6. ‚úì **Full pipeline** - End-to-end circuit discovery\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Recovery Score**:\n",
    "$$R = \\frac{\\mathcal{L}_{corrupt} - \\mathcal{L}_{patch}}{\\mathcal{L}_{corrupt} - \\mathcal{L}_{clean}}$$\n",
    "\n",
    "**Interpretation**:\n",
    "- High R ‚Üí Component is causally necessary\n",
    "- Low R ‚Üí Component doesn't matter\n",
    "\n",
    "**Complementary Methods**:\n",
    "- **Patching**: What's important? (restoration)\n",
    "- **Ablation**: What's necessary? (removal)\n",
    "- Both give converging evidence!\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "You can now:\n",
    "- Find **induction heads** in language models\n",
    "- Discover **attention circuits** for specific tasks\n",
    "- Identify **critical neurons** for editing\n",
    "- Map **information highways** in networks\n",
    "- Build **interpretable subnetworks**\n",
    "- Debug **failure modes** causally\n",
    "\n",
    "### Famous Circuits Discovered\n",
    "\n",
    "Using these techniques, researchers have found:\n",
    "- **Induction heads**: Pattern completion circuits\n",
    "- **IOI circuit**: Indirect object identification\n",
    "- **Greater-than circuit**: Numerical comparison\n",
    "- **Copy suppression**: Avoiding repetition\n",
    "\n",
    "### Next Notebook\n",
    "\n",
    "**[04_fractal_analysis.ipynb](04_fractal_analysis.ipynb)**\n",
    "\n",
    "Now that we can find circuits, let's analyze their **dynamics**:\n",
    "- Measuring biological realism through fractals\n",
    "- Scale-free temporal dynamics\n",
    "- Enforcing pink noise during training\n",
    "- Comparing model and brain complexity\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "**Essential Papers**:\n",
    "1. Elhage et al. (2021): \"A Mathematical Framework for Transformer Circuits\"\n",
    "2. Wang et al. (2022): \"Interpretability in the Wild\"\n",
    "3. Nanda et al. (2023): \"Progress Measures for Mechanistic Interpretability\"\n",
    "4. Conmy et al. (2023): \"Automated Circuit Discovery\"\n",
    "\n",
    "**Code & Tools**:\n",
    "- [TransformerLens](https://github.com/neelnanda-io/TransformerLens): Activation patching for transformers\n",
    "- [ACDC](https://github.com/ArthurConmy/Automatic-Circuit-Discovery): Automated circuit discovery\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You can now causally analyze neural networks and discover computational circuits. This is a superpower for understanding how models actually work!\n",
    "\n",
    "Ready to measure biological realism? Open [04_fractal_analysis.ipynb](04_fractal_analysis.ipynb)! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
