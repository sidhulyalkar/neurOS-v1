{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Foundation Models Showcase\n",
    "\n",
    "This tutorial demonstrates how to use NeurOS's foundation models for neural decoding.\n",
    "\n",
    "**Foundation Models Covered:**\n",
    "1. POYO+ (Multi-task decoding)\n",
    "2. NDT3 (Motor decoding)\n",
    "3. CEBRA (Latent embeddings)\n",
    "4. Neuroformer (Zero-shot learning)\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python knowledge\n",
    "- Familiarity with NumPy\n",
    "- Tutorial 1 completed (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NeurOS if needed\n",
    "# !pip install neuros[foundation]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "# NeurOS imports\n",
    "from neuros.foundation_models import (\n",
    "    POYOPlusModel,\n",
    "    NDT3Model,\n",
    "    CEBRAModel,\n",
    "    NeuroformerModel\n",
    ")\n",
    "from neuros.datasets import load_allen_mock_data\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Neural Data\n",
    "\n",
    "We'll use mock Allen Institute-style data for this tutorial.\n",
    "In practice, you'd load real data from NWB files or Allen SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mock neural data\n",
    "data = load_allen_mock_data(n_neurons=100, duration=120.0)\n",
    "\n",
    "print(f\"Dataset info:\")\n",
    "print(f\"  - Neurons: {data['n_neurons']}\")\n",
    "print(f\"  - Duration: {data['duration']}s\")\n",
    "print(f\"  - Spike trains: {len(data['spike_times'])}\")\n",
    "print(f\"  - Spike raster shape: {data['spike_raster'].shape}\")\n",
    "\n",
    "# Extract features for models\n",
    "X_neural = data['spike_raster']  # (n_trials, n_neurons, n_bins)\n",
    "y_behavior = np.random.randint(0, 3, size=X_neural.shape[0])  # Mock labels\n",
    "\n",
    "print(f\"\\nInput data shape: {X_neural.shape}\")\n",
    "print(f\"Labels shape: {y_behavior.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. POYO+ Model (Multi-Task Decoding)\n",
    "\n",
    "POYO+ can perform multiple decoding tasks simultaneously from the same neural data.\n",
    "It's pretrained on large-scale datasets and can be fine-tuned for your specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create POYO+ model with multiple tasks\n",
    "task_configs = [\n",
    "    {\"name\": \"behavior\", \"type\": \"classification\", \"output_dim\": 3},\n",
    "    {\"name\": \"position\", \"type\": \"regression\", \"output_dim\": 2},\n",
    "]\n",
    "\n",
    "poyo = POYOPlusModel(task_configs=task_configs)\n",
    "\n",
    "# Train on your data\n",
    "print(\"Training POYO+ model...\")\n",
    "poyo.train(X_neural[:80], y_behavior[:80])\n",
    "\n",
    "# Make predictions (returns dict with all tasks)\n",
    "predictions = poyo.predict(X_neural[80:])\n",
    "\n",
    "print(f\"\\nPOYO+ Predictions:\")\n",
    "for task_name, preds in predictions.items():\n",
    "    print(f\"  {task_name}: shape {preds.shape}\")\n",
    "\n",
    "# Evaluate\n",
    "behavior_preds = predictions['behavior']\n",
    "behavior_labels = np.argmax(behavior_preds, axis=1)\n",
    "accuracy = accuracy_score(y_behavior[80:], behavior_labels)\n",
    "print(f\"\\nBehavior classification accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Multi-Task Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Task 1: Behavior classification\n",
    "axes[0].bar(range(3), np.bincount(behavior_labels, minlength=3))\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('POYO+ Behavior Classification')\n",
    "axes[0].set_xticks(range(3))\n",
    "\n",
    "# Task 2: Position prediction\n",
    "position_preds = predictions['position']\n",
    "axes[1].scatter(position_preds[:, 0], position_preds[:, 1], \n",
    "                c=behavior_labels, cmap='viridis', alpha=0.6)\n",
    "axes[1].set_xlabel('Position X')\n",
    "axes[1].set_ylabel('Position Y')\n",
    "axes[1].set_title('POYO+ Position Decoding')\n",
    "axes[1].colorbar(label='Behavior Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NDT3 Model (Motor Decoding)\n",
    "\n",
    "NDT3 is optimized for real-time motor decoding from intracortical recordings.\n",
    "It features cross-subject transfer and rapid fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NDT3 model for motor decoding\n",
    "ndt3 = NDT3Model(\n",
    "    n_neurons=100,\n",
    "    output_dim=2,  # 2D motor output (e.g., cursor velocity)\n",
    "    use_subject_embedding=True\n",
    ")\n",
    "\n",
    "# Mock motor outputs (velocity in x, y)\n",
    "motor_outputs = np.random.randn(X_neural.shape[0], 2)\n",
    "\n",
    "# Train model\n",
    "print(\"Training NDT3 model...\")\n",
    "ndt3.train(X_neural[:80], motor_outputs[:80], subject_ids=None)\n",
    "\n",
    "# Predict motor outputs\n",
    "motor_preds = ndt3.predict(X_neural[80:])\n",
    "print(f\"\\nMotor predictions shape: {motor_preds.shape}\")\n",
    "\n",
    "# Evaluate\n",
    "r2_x = r2_score(motor_outputs[80:, 0], motor_preds[:, 0])\n",
    "r2_y = r2_score(motor_outputs[80:, 1], motor_preds[:, 1])\n",
    "print(f\"\\nMotor Decoding R² Scores:\")\n",
    "print(f\"  X-axis: {r2_x:.3f}\")\n",
    "print(f\"  Y-axis: {r2_y:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning for New Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate new subject data (only 10 samples)\n",
    "X_new_subject = X_neural[-10:]\n",
    "y_new_subject = motor_outputs[-10:]\n",
    "\n",
    "# Fine-tune on minimal data\n",
    "print(\"Fine-tuning NDT3 for new subject...\")\n",
    "ndt3.fine_tune(\n",
    "    X_new_subject,\n",
    "    y_new_subject,\n",
    "    subject_id=99,\n",
    "    n_epochs=5\n",
    ")\n",
    "\n",
    "# Predict with adapted model\n",
    "adapted_preds = ndt3.predict(X_new_subject, subject_id=99)\n",
    "\n",
    "# Compare before/after adaptation\n",
    "r2_adapted = r2_score(y_new_subject[:, 0], adapted_preds[:, 0])\n",
    "print(f\"\\nR² after fine-tuning: {r2_adapted:.3f}\")\n",
    "print(\"✅ NDT3 successfully adapted to new subject!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CEBRA Model (Latent Embeddings)\n",
    "\n",
    "CEBRA learns consistent, low-dimensional representations of neural activity\n",
    "that capture behavioral correlates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for CEBRA (expects 2D: samples x features)\n",
    "X_2d = X_neural.reshape(X_neural.shape[0], -1)\n",
    "\n",
    "# Create CEBRA model\n",
    "cebra = CEBRAModel(\n",
    "    input_dim=X_2d.shape[1],\n",
    "    output_dim=3,  # 3D latent space for visualization\n",
    "    learning_mode='time',  # Can also use 'behavior' or 'hybrid'\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Fit and transform (sklearn-style API)\n",
    "print(\"Training CEBRA model...\")\n",
    "embeddings = cebra.fit_transform(X_2d)\n",
    "\n",
    "print(f\"\\nCEBRA Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embedding space: {embeddings.shape[1]}D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 2D projection\n",
    "ax1 = fig.add_subplot(121)\n",
    "scatter = ax1.scatter(\n",
    "    embeddings[:, 0], \n",
    "    embeddings[:, 1],\n",
    "    c=y_behavior,\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "ax1.set_xlabel('CEBRA Dimension 1')\n",
    "ax1.set_ylabel('CEBRA Dimension 2')\n",
    "ax1.set_title('CEBRA Latent Space (2D)')\n",
    "plt.colorbar(scatter, ax=ax1, label='Behavior')\n",
    "\n",
    "# 3D projection\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(\n",
    "    embeddings[:, 0],\n",
    "    embeddings[:, 1],\n",
    "    embeddings[:, 2],\n",
    "    c=y_behavior,\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "ax2.set_xlabel('Dim 1')\n",
    "ax2.set_ylabel('Dim 2')\n",
    "ax2.set_zlabel('Dim 3')\n",
    "ax2.set_title('CEBRA Latent Space (3D)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Session Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate second session\n",
    "data_session2 = load_allen_mock_data(n_neurons=100, duration=120.0)\n",
    "X_session2 = data_session2['spike_raster'].reshape(data_session2['spike_raster'].shape[0], -1)\n",
    "\n",
    "# Compute cross-session consistency\n",
    "consistency = cebra.compute_consistency(\n",
    "    X_2d[:50], \n",
    "    X_session2[:50],\n",
    "    n_neighbors=5\n",
    ")\n",
    "\n",
    "print(f\"\\nCross-session consistency: {consistency:.3f}\")\n",
    "print(\"(Higher is better, range 0-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neuroformer Model (Zero-Shot & Few-Shot)\n",
    "\n",
    "Neuroformer is a multimodal foundation model that supports:\n",
    "- Zero-shot prediction (no fine-tuning!)\n",
    "- Few-shot adaptation (minimal data)\n",
    "- Generative capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neuroformer model\n",
    "neuroformer = NeuroformerModel(\n",
    "    input_dim=100,\n",
    "    output_dim=3,\n",
    "    n_modalities=1,\n",
    "    task_type='classification'\n",
    ")\n",
    "\n",
    "# Pretrain on large dataset\n",
    "print(\"Pretraining Neuroformer...\")\n",
    "neuroformer.pretrain(X_neural, n_epochs=20)\n",
    "\n",
    "print(\"✅ Pretraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot: predict without any task-specific training!\n",
    "zero_shot_preds = neuroformer.zero_shot_predict(\n",
    "    X_neural[80:85],\n",
    "    task_description=\"Classify behavioral state from neural activity\"\n",
    ")\n",
    "\n",
    "print(f\"Zero-shot predictions shape: {zero_shot_preds.shape}\")\n",
    "print(f\"Predictions:\\n{zero_shot_preds}\")\n",
    "print(\"\\n✅ Zero-shot inference successful (no fine-tuning needed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot: adapt with only 15 examples (5 per class)\n",
    "n_shots = 5\n",
    "support_indices = []\n",
    "for c in range(3):\n",
    "    class_indices = np.where(y_behavior[:80] == c)[0][:n_shots]\n",
    "    support_indices.extend(class_indices)\n",
    "\n",
    "X_support = X_neural[support_indices]\n",
    "y_support = y_behavior[support_indices]\n",
    "\n",
    "# Few-shot adaptation\n",
    "print(f\"Adapting with {len(X_support)} examples ({n_shots} per class)...\")\n",
    "few_shot_preds = neuroformer.few_shot_adapt(\n",
    "    X_support=X_support,\n",
    "    y_support=y_support,\n",
    "    X_query=X_neural[80:],\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_behavior[80:], few_shot_preds)\n",
    "print(f\"\\nFew-shot accuracy: {accuracy:.2%}\")\n",
    "print(f\"(With only {n_shots} examples per class!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic neural data\n",
    "synthetic = neuroformer.generate(\n",
    "    n_samples=10,\n",
    "    sequence_length=50\n",
    ")\n",
    "\n",
    "print(f\"Generated synthetic data: {synthetic.shape}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "# Real data\n",
    "axes[0].imshow(X_neural[0, :, :50].T, aspect='auto', cmap='viridis')\n",
    "axes[0].set_title('Real Neural Activity')\n",
    "axes[0].set_xlabel('Time bins')\n",
    "axes[0].set_ylabel('Neurons')\n",
    "\n",
    "# Synthetic data\n",
    "axes[1].imshow(synthetic[0, :, :].T, aspect='auto', cmap='viridis')\n",
    "axes[1].set_title('Neuroformer Generated Activity')\n",
    "axes[1].set_xlabel('Time bins')\n",
    "axes[1].set_ylabel('Neurons')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Foundation vs. Classical Models\n",
    "\n",
    "Let's compare foundation models with classical approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Prepare data for classical models\n",
    "X_flat = X_neural.reshape(X_neural.shape[0], -1)\n",
    "X_train_flat, X_test_flat = X_flat[:80], X_flat[80:]\n",
    "y_train, y_test = y_behavior[:80], y_behavior[80:]\n",
    "\n",
    "# Classical models\n",
    "models = {\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate classical models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_flat, y_train)\n",
    "    preds = model.predict(X_test_flat)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    results[name] = acc\n",
    "    print(f\"{name:15s}: {acc:.2%}\")\n",
    "\n",
    "# Foundation model (POYO+)\n",
    "poyo_acc = accuracy_score(y_behavior[80:], behavior_labels)\n",
    "results['POYO+'] = poyo_acc\n",
    "print(f\"{'POYO+':15s}: {poyo_acc:.2%}\")\n",
    "\n",
    "# Foundation model (Neuroformer few-shot)\n",
    "results['Neuroformer'] = accuracy\n",
    "print(f\"{'Neuroformer':15s}: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "colors = ['gray', 'gray', 'blue', 'blue']\n",
    "\n",
    "bars = ax.barh(model_names, accuracies, color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Model Comparison: Classical vs. Foundation Models')\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for i, (name, acc) in enumerate(results.items()):\n",
    "    ax.text(acc + 0.01, i, f'{acc:.1%}', va='center')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='gray', alpha=0.7, label='Classical'),\n",
    "    Patch(facecolor='blue', alpha=0.7, label='Foundation')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "### Foundation Models Advantages:\n",
    "\n",
    "1. **Transfer Learning**: Pretrained on large datasets, work well on small datasets\n",
    "2. **Multi-Task**: POYO+ handles multiple tasks simultaneously\n",
    "3. **Cross-Subject**: NDT3 adapts quickly to new subjects\n",
    "4. **Interpretable**: CEBRA produces consistent latent spaces\n",
    "5. **Zero-Shot**: Neuroformer works without task-specific training\n",
    "6. **Few-Shot**: Rapid adaptation with minimal data\n",
    "\n",
    "### When to Use Each Model:\n",
    "\n",
    "- **POYO+**: Multiple related tasks, multi-session experiments\n",
    "- **NDT3**: Real-time motor decoding, cross-subject BCIs\n",
    "- **CEBRA**: Behavioral analysis, latent space visualization\n",
    "- **Neuroformer**: New tasks, limited data, generative modeling\n",
    "\n",
    "### Classical Models Still Useful For:\n",
    "\n",
    "- Simple, well-defined tasks\n",
    "- When interpretability is critical\n",
    "- Fast training on small datasets\n",
    "- Limited computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Load Real Data**: Try with NWB files or Allen datasets\n",
    "2. **Fine-Tune Models**: Use your own pretrained weights\n",
    "3. **Hyperparameter Tuning**: Optimize for your specific task\n",
    "4. **Real-Time Inference**: Deploy in production pipeline\n",
    "5. **Multi-Modal**: Combine neural + behavioral + video data\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "- [Foundation Models User Guide](../user-guide/foundation-models.md)\n",
    "- [NWB Integration Tutorial](nwb-integration.md)\n",
    "- [API Reference: Foundation Models](../api/foundation-models.md)\n",
    "- [Benchmarking Tutorial](benchmarking.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Try different CEBRA modes**: Change `learning_mode` to 'behavior' or 'hybrid'\n",
    "2. **Experiment with latent dimensions**: Try 2D, 8D, 32D for CEBRA\n",
    "3. **Test cross-subject transfer**: Train NDT3 on subject 1, test on subject 2\n",
    "4. **Compare few-shot performance**: Try 1, 3, 5, 10 shots per class\n",
    "5. **Generate conditioned data**: Use Neuroformer with context\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've learned how to use all four foundation models in NeurOS. 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
