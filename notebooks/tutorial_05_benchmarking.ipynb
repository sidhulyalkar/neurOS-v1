{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Benchmarking & Performance Optimization\n",
    "\n",
    "**Level**: Intermediate to Advanced  \n",
    "**Time**: 30-40 minutes  \n",
    "**Prerequisites**: Tutorial 1, Tutorial 2, Tutorial 4\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, you'll learn how to:\n",
    "\n",
    "1. **Benchmark Pipeline Performance** - Measure throughput and latency\n",
    "2. **Profile Code** - Identify bottlenecks\n",
    "3. **Optimize Processing** - Improve real-time performance\n",
    "4. **Compare Models** - Systematic model evaluation\n",
    "5. **Monitor Resources** - CPU, memory, and GPU usage\n",
    "6. **Real-Time Guarantees** - Ensure low-latency inference\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Throughput**: Samples processed per second\n",
    "- **Latency**: Time from input to output\n",
    "- **Jitter**: Variability in latency\n",
    "- **Profiling**: Measuring where time is spent\n",
    "- **Optimization**: Reducing computational overhead\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Basic Pipeline Benchmarking\n",
    "\n",
    "Let's start by measuring the performance of a basic pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "\n",
    "from neuros.pipeline import Pipeline\n",
    "from neuros.drivers import MockDriver\n",
    "from neuros.models import SimpleClassifier\n",
    "from neuros.processing import BandpassFilter, BandPowerExtractor\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Benchmarking Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineBenchmark:\n",
    "    \"\"\"\n",
    "    Comprehensive benchmarking suite for neurOS pipelines.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline: Pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.results = {}\n",
    "    \n",
    "    def measure_latency(self, X: np.ndarray, n_iterations: int = 100) -> Dict:\n",
    "        \"\"\"\n",
    "        Measure prediction latency.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Statistics including mean, median, std, min, max latency\n",
    "        \"\"\"\n",
    "        latencies = []\n",
    "        \n",
    "        # Warm-up\n",
    "        for _ in range(10):\n",
    "            _ = self.pipeline.predict(X[:1])\n",
    "        \n",
    "        # Measure\n",
    "        for i in range(n_iterations):\n",
    "            start = time.perf_counter()\n",
    "            _ = self.pipeline.predict(X[i:i+1])\n",
    "            end = time.perf_counter()\n",
    "            latencies.append((end - start) * 1000)  # Convert to ms\n",
    "        \n",
    "        return {\n",
    "            'mean_ms': np.mean(latencies),\n",
    "            'median_ms': np.median(latencies),\n",
    "            'std_ms': np.std(latencies),\n",
    "            'min_ms': np.min(latencies),\n",
    "            'max_ms': np.max(latencies),\n",
    "            'p95_ms': np.percentile(latencies, 95),\n",
    "            'p99_ms': np.percentile(latencies, 99),\n",
    "            'latencies': latencies\n",
    "        }\n",
    "    \n",
    "    def measure_throughput(self, X: np.ndarray, duration: float = 5.0) -> Dict:\n",
    "        \"\"\"\n",
    "        Measure throughput (samples per second).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            Test data\n",
    "        duration : float\n",
    "            How long to run the benchmark (seconds)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Throughput statistics\n",
    "        \"\"\"\n",
    "        samples_processed = 0\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        while (time.perf_counter() - start_time) < duration:\n",
    "            batch_size = min(32, len(X))\n",
    "            _ = self.pipeline.predict(X[:batch_size])\n",
    "            samples_processed += batch_size\n",
    "        \n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        throughput = samples_processed / elapsed\n",
    "        \n",
    "        return {\n",
    "            'samples_per_second': throughput,\n",
    "            'total_samples': samples_processed,\n",
    "            'duration_s': elapsed\n",
    "        }\n",
    "    \n",
    "    def measure_batch_performance(self, X: np.ndarray, batch_sizes: List[int]) -> Dict:\n",
    "        \"\"\"\n",
    "        Measure performance across different batch sizes.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            latencies = []\n",
    "            \n",
    "            for _ in range(20):\n",
    "                batch = X[:batch_size]\n",
    "                start = time.perf_counter()\n",
    "                _ = self.pipeline.predict(batch)\n",
    "                end = time.perf_counter()\n",
    "                latencies.append((end - start) * 1000)  # ms\n",
    "            \n",
    "            results[batch_size] = {\n",
    "                'mean_latency_ms': np.mean(latencies),\n",
    "                'samples_per_second': batch_size / (np.mean(latencies) / 1000)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_full_benchmark(self, X: np.ndarray) -> Dict:\n",
    "        \"\"\"\n",
    "        Run comprehensive benchmark suite.\n",
    "        \"\"\"\n",
    "        print(\"Running comprehensive benchmark...\\n\")\n",
    "        \n",
    "        # Latency\n",
    "        print(\"1. Measuring latency...\")\n",
    "        latency_stats = self.measure_latency(X, n_iterations=100)\n",
    "        print(f\"   Mean latency: {latency_stats['mean_ms']:.2f} ms\")\n",
    "        print(f\"   P95 latency: {latency_stats['p95_ms']:.2f} ms\")\n",
    "        \n",
    "        # Throughput\n",
    "        print(\"\\n2. Measuring throughput...\")\n",
    "        throughput_stats = self.measure_throughput(X, duration=3.0)\n",
    "        print(f\"   Throughput: {throughput_stats['samples_per_second']:.0f} samples/sec\")\n",
    "        \n",
    "        # Batch performance\n",
    "        print(\"\\n3. Measuring batch performance...\")\n",
    "        batch_sizes = [1, 8, 16, 32, 64]\n",
    "        batch_stats = self.measure_batch_performance(X, batch_sizes)\n",
    "        \n",
    "        self.results = {\n",
    "            'latency': latency_stats,\n",
    "            'throughput': throughput_stats,\n",
    "            'batch': batch_stats\n",
    "        }\n",
    "        \n",
    "        print(\"\\n✓ Benchmark complete\")\n",
    "        return self.results\n",
    "\n",
    "print(\"✓ PipelineBenchmark class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Benchmark on Simple Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_channels = 64\n",
    "n_features = 32\n",
    "\n",
    "X_test = np.random.randn(n_samples, n_features)\n",
    "y_test = np.random.randint(0, 4, n_samples)\n",
    "\n",
    "# Create simple pipeline\n",
    "driver = MockDriver(n_channels=n_channels, sampling_rate=250)\n",
    "model = SimpleClassifier(model_type='logistic')\n",
    "\n",
    "# Train model\n",
    "model.train(X_test[:700], y_test[:700])\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(driver=driver, model=model)\n",
    "\n",
    "# Benchmark\n",
    "benchmark = PipelineBenchmark(pipeline)\n",
    "results = benchmark.run_full_benchmark(X_test[700:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Latency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency histogram\n",
    "latencies = results['latency']['latencies']\n",
    "ax1.hist(latencies, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(results['latency']['mean_ms'], color='red', linestyle='--', \n",
    "            linewidth=2, label=f\"Mean: {results['latency']['mean_ms']:.2f} ms\")\n",
    "ax1.axvline(results['latency']['p95_ms'], color='orange', linestyle='--', \n",
    "            linewidth=2, label=f\"P95: {results['latency']['p95_ms']:.2f} ms\")\n",
    "ax1.set_xlabel('Latency (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Latency Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Latency over time\n",
    "ax2.plot(latencies, linewidth=1, alpha=0.7, color='steelblue')\n",
    "ax2.axhline(results['latency']['mean_ms'], color='red', linestyle='--', \n",
    "            linewidth=2, alpha=0.7, label='Mean')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Latency (ms)')\n",
    "ax2.set_title('Latency Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLatency Statistics:\")\n",
    "print(f\"  Mean: {results['latency']['mean_ms']:.2f} ms\")\n",
    "print(f\"  Std:  {results['latency']['std_ms']:.2f} ms\")\n",
    "print(f\"  P95:  {results['latency']['p95_ms']:.2f} ms\")\n",
    "print(f\"  P99:  {results['latency']['p99_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Batch Size Optimization\n",
    "\n",
    "Analyze how batch size affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract batch performance data\n",
    "batch_results = results['batch']\n",
    "batch_sizes = sorted(batch_results.keys())\n",
    "mean_latencies = [batch_results[bs]['mean_latency_ms'] for bs in batch_sizes]\n",
    "throughputs = [batch_results[bs]['samples_per_second'] for bs in batch_sizes]\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency vs batch size\n",
    "ax1.plot(batch_sizes, mean_latencies, marker='o', linewidth=2, markersize=8, color='coral')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Mean Latency (ms)')\n",
    "ax1.set_title('Latency vs Batch Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log', base=2)\n",
    "\n",
    "# Throughput vs batch size\n",
    "ax2.plot(batch_sizes, throughputs, marker='s', linewidth=2, markersize=8, color='green')\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Throughput (samples/sec)')\n",
    "ax2.set_title('Throughput vs Batch Size')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal batch size\n",
    "optimal_idx = np.argmax(throughputs)\n",
    "optimal_batch_size = batch_sizes[optimal_idx]\n",
    "optimal_throughput = throughputs[optimal_idx]\n",
    "\n",
    "print(f\"\\nOptimal Configuration:\")\n",
    "print(f\"  Batch Size: {optimal_batch_size}\")\n",
    "print(f\"  Throughput: {optimal_throughput:.0f} samples/sec\")\n",
    "print(f\"  Latency: {mean_latencies[optimal_idx]:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Model Comparison Benchmark\n",
    "\n",
    "Compare performance across different model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuros.models import SimpleClassifier\n",
    "\n",
    "# Define models to compare\n",
    "models_to_test = {\n",
    "    'Logistic Regression': SimpleClassifier(model_type='logistic'),\n",
    "    'SVM (Linear)': SimpleClassifier(model_type='svm'),\n",
    "    'Random Forest': SimpleClassifier(model_type='random_forest'),\n",
    "    'k-NN (k=5)': SimpleClassifier(model_type='knn')\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "print(\"Training models...\")\n",
    "for name, model in models_to_test.items():\n",
    "    model.train(X_test[:700], y_test[:700])\n",
    "    print(f\"  ✓ {name}\")\n",
    "\n",
    "# Benchmark each model\n",
    "print(\"\\nBenchmarking models...\\n\")\n",
    "comparison_results = {}\n",
    "\n",
    "for name, model in models_to_test.items():\n",
    "    pipeline = Pipeline(driver=driver, model=model)\n",
    "    benchmark = PipelineBenchmark(pipeline)\n",
    "    \n",
    "    # Quick benchmark\n",
    "    latency = benchmark.measure_latency(X_test[700:], n_iterations=50)\n",
    "    throughput = benchmark.measure_throughput(X_test[700:], duration=2.0)\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    y_pred = model.predict(X_test[700:])\n",
    "    accuracy = np.mean(y_pred == y_test[700:])\n",
    "    \n",
    "    comparison_results[name] = {\n",
    "        'latency_ms': latency['mean_ms'],\n",
    "        'throughput': throughput['samples_per_second'],\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Latency: {latency['mean_ms']:.2f} ms\")\n",
    "    print(f\"  Throughput: {throughput['samples_per_second']:.0f} samples/sec\")\n",
    "    print(f\"  Accuracy: {accuracy:.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "df = pd.DataFrame(comparison_results).T\n",
    "df = df.sort_values('throughput', ascending=False)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Latency comparison\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.barh(df.index, df['latency_ms'], color='coral')\n",
    "ax1.set_xlabel('Latency (ms)')\n",
    "ax1.set_title('Inference Latency')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars1):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.2f}',\n",
    "             ha='left', va='center', fontsize=9)\n",
    "\n",
    "# Throughput comparison\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.barh(df.index, df['throughput'], color='green')\n",
    "ax2.set_xlabel('Throughput (samples/sec)')\n",
    "ax2.set_title('Processing Throughput')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, bar in enumerate(bars2):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.0f}',\n",
    "             ha='left', va='center', fontsize=9)\n",
    "\n",
    "# Accuracy comparison\n",
    "ax3 = axes[2]\n",
    "bars3 = ax3.barh(df.index, df['accuracy'], color='steelblue')\n",
    "ax3.set_xlabel('Accuracy')\n",
    "ax3.set_title('Prediction Accuracy')\n",
    "ax3.set_xlim([0, 1])\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, bar in enumerate(bars3):\n",
    "    width = bar.get_width()\n",
    "    ax3.text(width, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.1%}',\n",
    "             ha='left', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance-Accuracy Trade-off:\")\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Code Profiling\n",
    "\n",
    "Identify performance bottlenecks using profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "from pstats import SortKey\n",
    "\n",
    "def profile_pipeline(pipeline: Pipeline, X: np.ndarray, n_iterations: int = 100):\n",
    "    \"\"\"\n",
    "    Profile pipeline execution to find bottlenecks.\n",
    "    \"\"\"\n",
    "    profiler = cProfile.Profile()\n",
    "    \n",
    "    # Profile predictions\n",
    "    profiler.enable()\n",
    "    for i in range(n_iterations):\n",
    "        _ = pipeline.predict(X[i:i+1])\n",
    "    profiler.disable()\n",
    "    \n",
    "    # Get stats\n",
    "    s = io.StringIO()\n",
    "    stats = pstats.Stats(profiler, stream=s)\n",
    "    stats.sort_stats(SortKey.CUMULATIVE)\n",
    "    stats.print_stats(20)  # Top 20 functions\n",
    "    \n",
    "    return s.getvalue()\n",
    "\n",
    "# Profile the pipeline\n",
    "print(\"Profiling pipeline (top 20 functions by cumulative time):\\n\")\n",
    "profile_output = profile_pipeline(pipeline, X_test[700:], n_iterations=50)\n",
    "print(profile_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Memory Profiling\n",
    "\n",
    "Monitor memory usage during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def measure_memory_usage(pipeline: Pipeline, X: np.ndarray, n_iterations: int = 100):\n",
    "    \"\"\"\n",
    "    Measure memory usage during pipeline execution.\n",
    "    \"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    # Get baseline\n",
    "    baseline_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    memory_samples = []\n",
    "    \n",
    "    # Run predictions and sample memory\n",
    "    for i in range(n_iterations):\n",
    "        _ = pipeline.predict(X[i:i+1])\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            current_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "            memory_samples.append(current_memory)\n",
    "    \n",
    "    peak_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    return {\n",
    "        'baseline_mb': baseline_memory,\n",
    "        'peak_mb': peak_memory,\n",
    "        'increase_mb': peak_memory - baseline_memory,\n",
    "        'samples': memory_samples\n",
    "    }\n",
    "\n",
    "# Measure memory\n",
    "memory_stats = measure_memory_usage(pipeline, X_test[700:], n_iterations=100)\n",
    "\n",
    "print(f\"Memory Usage:\")\n",
    "print(f\"  Baseline: {memory_stats['baseline_mb']:.1f} MB\")\n",
    "print(f\"  Peak: {memory_stats['peak_mb']:.1f} MB\")\n",
    "print(f\"  Increase: {memory_stats['increase_mb']:.1f} MB\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(memory_stats['samples'], marker='o', linewidth=2, markersize=6, color='purple')\n",
    "plt.axhline(memory_stats['baseline_mb'], color='red', linestyle='--', \n",
    "            label=f\"Baseline: {memory_stats['baseline_mb']:.1f} MB\")\n",
    "plt.xlabel('Sample Point')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.title('Memory Usage During Pipeline Execution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Real-Time Performance Guarantees\n",
    "\n",
    "Verify that the pipeline can meet real-time requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_realtime_performance(\n",
    "    pipeline: Pipeline,\n",
    "    X: np.ndarray,\n",
    "    sampling_rate: int = 250,\n",
    "    target_latency_ms: float = 50.0,\n",
    "    n_iterations: int = 100\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Check if pipeline meets real-time performance requirements.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sampling_rate : int\n",
    "        Data sampling rate (Hz)\n",
    "    target_latency_ms : float\n",
    "        Maximum acceptable latency (ms)\n",
    "    \"\"\"\n",
    "    # Time budget per sample\n",
    "    sample_period_ms = 1000.0 / sampling_rate\n",
    "    \n",
    "    # Measure latencies\n",
    "    latencies = []\n",
    "    for i in range(n_iterations):\n",
    "        start = time.perf_counter()\n",
    "        _ = pipeline.predict(X[i:i+1])\n",
    "        end = time.perf_counter()\n",
    "        latencies.append((end - start) * 1000)\n",
    "    \n",
    "    latencies = np.array(latencies)\n",
    "    \n",
    "    # Check constraints\n",
    "    mean_latency = np.mean(latencies)\n",
    "    p99_latency = np.percentile(latencies, 99)\n",
    "    max_latency = np.max(latencies)\n",
    "    \n",
    "    violations = np.sum(latencies > target_latency_ms)\n",
    "    violation_rate = violations / len(latencies)\n",
    "    \n",
    "    # Compute headroom\n",
    "    headroom_percent = ((target_latency_ms - mean_latency) / target_latency_ms) * 100\n",
    "    \n",
    "    meets_requirements = (p99_latency < target_latency_ms)\n",
    "    \n",
    "    return {\n",
    "        'sample_period_ms': sample_period_ms,\n",
    "        'target_latency_ms': target_latency_ms,\n",
    "        'mean_latency_ms': mean_latency,\n",
    "        'p99_latency_ms': p99_latency,\n",
    "        'max_latency_ms': max_latency,\n",
    "        'violations': violations,\n",
    "        'violation_rate': violation_rate,\n",
    "        'headroom_percent': headroom_percent,\n",
    "        'meets_requirements': meets_requirements,\n",
    "        'latencies': latencies\n",
    "    }\n",
    "\n",
    "# Check real-time performance\n",
    "rt_stats = check_realtime_performance(\n",
    "    pipeline,\n",
    "    X_test[700:],\n",
    "    sampling_rate=250,\n",
    "    target_latency_ms=50.0,\n",
    "    n_iterations=100\n",
    ")\n",
    "\n",
    "print(\"Real-Time Performance Check:\")\n",
    "print(f\"  Sample period: {rt_stats['sample_period_ms']:.2f} ms (@ 250 Hz)\")\n",
    "print(f\"  Target latency: {rt_stats['target_latency_ms']:.2f} ms\")\n",
    "print(f\"\\nMeasured Performance:\")\n",
    "print(f\"  Mean latency: {rt_stats['mean_latency_ms']:.2f} ms\")\n",
    "print(f\"  P99 latency: {rt_stats['p99_latency_ms']:.2f} ms\")\n",
    "print(f\"  Max latency: {rt_stats['max_latency_ms']:.2f} ms\")\n",
    "print(f\"\\nRequirement Check:\")\n",
    "print(f\"  Violations: {rt_stats['violations']}/{len(rt_stats['latencies'])} ({rt_stats['violation_rate']:.1%})\")\n",
    "print(f\"  Headroom: {rt_stats['headroom_percent']:.1f}%\")\n",
    "print(f\"  Status: {'✓ PASS' if rt_stats['meets_requirements'] else '✗ FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Real-Time Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency over time with threshold\n",
    "ax1.plot(rt_stats['latencies'], linewidth=1, alpha=0.7, color='steelblue', label='Latency')\n",
    "ax1.axhline(rt_stats['target_latency_ms'], color='red', linestyle='--', \n",
    "            linewidth=2, label=f\"Target: {rt_stats['target_latency_ms']:.0f} ms\")\n",
    "ax1.axhline(rt_stats['mean_latency_ms'], color='green', linestyle='--', \n",
    "            linewidth=2, label=f\"Mean: {rt_stats['mean_latency_ms']:.2f} ms\")\n",
    "ax1.set_xlabel('Sample')\n",
    "ax1.set_ylabel('Latency (ms)')\n",
    "ax1.set_title('Real-Time Latency Compliance')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# CDF (Cumulative Distribution Function)\n",
    "sorted_latencies = np.sort(rt_stats['latencies'])\n",
    "cdf = np.arange(1, len(sorted_latencies) + 1) / len(sorted_latencies)\n",
    "ax2.plot(sorted_latencies, cdf * 100, linewidth=2, color='steelblue')\n",
    "ax2.axvline(rt_stats['target_latency_ms'], color='red', linestyle='--', \n",
    "            linewidth=2, label='Target')\n",
    "ax2.axhline(99, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='99th percentile')\n",
    "ax2.set_xlabel('Latency (ms)')\n",
    "ax2.set_ylabel('Cumulative Probability (%)')\n",
    "ax2.set_title('Latency CDF')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Optimization Strategies\n",
    "\n",
    "Practical tips for improving pipeline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_batch_vs_sequential(pipeline, X, batch_size=32):\n",
    "    \"\"\"\n",
    "    Compare batch vs sequential processing.\n",
    "    \"\"\"\n",
    "    n_samples = min(100, len(X))\n",
    "    X_subset = X[:n_samples]\n",
    "    \n",
    "    # Sequential\n",
    "    start = time.perf_counter()\n",
    "    for i in range(n_samples):\n",
    "        _ = pipeline.predict(X_subset[i:i+1])\n",
    "    sequential_time = time.perf_counter() - start\n",
    "    \n",
    "    # Batch\n",
    "    start = time.perf_counter()\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        batch = X_subset[i:i+batch_size]\n",
    "        _ = pipeline.predict(batch)\n",
    "    batch_time = time.perf_counter() - start\n",
    "    \n",
    "    speedup = sequential_time / batch_time\n",
    "    \n",
    "    return {\n",
    "        'sequential_time': sequential_time,\n",
    "        'batch_time': batch_time,\n",
    "        'speedup': speedup\n",
    "    }\n",
    "\n",
    "comparison = compare_batch_vs_sequential(pipeline, X_test[700:], batch_size=32)\n",
    "\n",
    "print(\"Batch Processing Optimization:\")\n",
    "print(f\"  Sequential time: {comparison['sequential_time']:.3f}s\")\n",
    "print(f\"  Batch time (32): {comparison['batch_time']:.3f}s\")\n",
    "print(f\"  Speedup: {comparison['speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Feature Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Model Quantization (Concept)\n",
    "\n",
    "For PyTorch models, quantization can reduce model size and improve inference speed:\n",
    "\n",
    "```python\n",
    "# Quantize a PyTorch model (pseudo-code)\n",
    "import torch.quantization as quantization\n",
    "\n",
    "# Dynamic quantization (easiest)\n",
    "quantized_model = quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Typically provides:\n",
    "# - 2-4x speedup\n",
    "# - 4x smaller model size\n",
    "# - Minimal accuracy loss (<1%)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "✅ **Benchmark Pipelines** - Measure latency, throughput, and jitter  \n",
    "✅ **Optimize Batch Size** - Find the best trade-off  \n",
    "✅ **Compare Models** - Evaluate performance-accuracy trade-offs  \n",
    "✅ **Profile Code** - Identify bottlenecks with cProfile  \n",
    "✅ **Monitor Memory** - Track resource usage  \n",
    "✅ **Real-Time Guarantees** - Verify latency requirements  \n",
    "✅ **Optimization Strategies** - Batch processing, caching, quantization  \n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Measure First** - Always profile before optimizing\n",
    "2. **Batch Wisely** - Larger batches improve throughput but increase latency\n",
    "3. **Choose Models** - Balance accuracy vs speed for your use case\n",
    "4. **Monitor Continuously** - Real-time systems need ongoing monitoring\n",
    "5. **Optimize Judiciously** - Focus on bottlenecks, not premature optimization\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- **For Real-Time BCI**: Keep P99 latency < 50ms\n",
    "- **For Batch Processing**: Maximize throughput with larger batches\n",
    "- **For Production**: Monitor latency continuously\n",
    "- **For Research**: Prioritize accuracy, optimize later\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Tutorial 6**: NWB Integration & Real-World Data\n",
    "- **Advanced**: GPU acceleration with CUDA\n",
    "- **Advanced**: Distributed processing with Ray\n",
    "- **Advanced**: Model optimization (pruning, distillation)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or feedback?** Open an issue on GitHub or check the docs at https://neuros.readthedocs.io"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
