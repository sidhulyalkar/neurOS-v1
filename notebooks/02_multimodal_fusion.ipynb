{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Fusion with Attention\n",
    "\n",
    "This notebook demonstrates how to combine multiple data modalities (EEG, video, motion sensors) using NeurOS's attention-based fusion model.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Combining EEG, video, and motion sensor data\n",
    "- Using AttentionFusionModel for intelligent modality weighting\n",
    "- Interpreting attention weights to understand model decisions\n",
    "- Comparing fusion strategies (concatenation vs. attention)\n",
    "- Running multi-modal pipelines in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from neuros.models import AttentionFusionModel, CompositeModel, ModelRegistry\n",
    "from neuros.pipeline import MultiModalPipeline\n",
    "from neuros.drivers import MockDriver, VideoDriver, MotionSensorDriver\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Multi-Modal Data\n",
    "\n",
    "We'll simulate a scenario where we're trying to detect emotional states using:\n",
    "- **EEG** (8 channels, 40 features after band-power extraction)\n",
    "- **Video** (facial features, 128-dim embedding)\n",
    "- **Motion** (accelerometer/gyroscope, 12 features)\n",
    "\n",
    "Classes:\n",
    "- 0: Neutral\n",
    "- 1: Happy  \n",
    "- 2: Stressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multimodal_data(n_samples=300):\n",
    "    \"\"\"\n",
    "    Generate synthetic multi-modal emotion recognition data.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_eeg : np.ndarray (n_samples, 40)\n",
    "        EEG band-power features\n",
    "    X_video : np.ndarray (n_samples, 128)\n",
    "        Video facial features  \n",
    "    X_motion : np.ndarray (n_samples, 12)\n",
    "        Motion sensor features\n",
    "    y : np.ndarray (n_samples,)\n",
    "        Labels (0=neutral, 1=happy, 2=stressed)\n",
    "    \"\"\"\n",
    "    X_eeg = []\n",
    "    X_video = []\n",
    "    X_motion = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        label = np.random.randint(0, 3)\n",
    "        \n",
    "        if label == 0:  # Neutral\n",
    "            # Low arousal in EEG\n",
    "            eeg = np.random.randn(40) * 0.5\n",
    "            # Neutral facial expression (low variance)\n",
    "            video = np.random.randn(128) * 0.3\n",
    "            # Minimal movement\n",
    "            motion = np.random.randn(12) * 0.2\n",
    "            \n",
    "        elif label == 1:  # Happy\n",
    "            # Increased frontal alpha in EEG\n",
    "            eeg = np.random.randn(40) * 0.7\n",
    "            eeg[:8] += 1.5  # Frontal channels boosted\n",
    "            # Smile in video (positive values in specific features)\n",
    "            video = np.random.randn(128) * 0.5\n",
    "            video[50:70] += 2.0  # \"Smile\" features\n",
    "            # Moderate movement\n",
    "            motion = np.random.randn(12) * 0.5 + 0.3\n",
    "            \n",
    "        else:  # Stressed (label == 2)\n",
    "            # High beta activity in EEG\n",
    "            eeg = np.random.randn(40) * 1.0\n",
    "            eeg[24:32] += 2.0  # Beta band boosted\n",
    "            # Tense facial features\n",
    "            video = np.random.randn(128) * 0.6\n",
    "            video[20:40] += 1.5  # \"Tension\" features\n",
    "            # Fidgeting (high variance in motion)\n",
    "            motion = np.random.randn(12) * 1.2\n",
    "        \n",
    "        X_eeg.append(eeg)\n",
    "        X_video.append(video)\n",
    "        X_motion.append(motion)\n",
    "        y.append(label)\n",
    "    \n",
    "    return (np.array(X_eeg), np.array(X_video), \n",
    "            np.array(X_motion), np.array(y))\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating synthetic multi-modal data...\")\n",
    "X_eeg, X_video, X_motion, y = generate_multimodal_data(n_samples=300)\n",
    "\n",
    "print(f\"✓ Generated {len(y)} samples\")\n",
    "print(f\"  EEG features: {X_eeg.shape[1]}\")\n",
    "print(f\"  Video features: {X_video.shape[1]}\")\n",
    "print(f\"  Motion features: {X_motion.shape[1]}\")\n",
    "print(f\"  Total features: {X_eeg.shape[1] + X_video.shape[1] + X_motion.shape[1]}\")\n",
    "print(f\"  Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Fusion Models\n",
    "\n",
    "Concatenate all modality features into a single feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Concatenate all modalities\n",
    "X_concat = np.concatenate([X_eeg, X_video, X_motion], axis=1)\n",
    "print(f\"Concatenated feature shape: {X_concat.shape}\")\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_concat, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Baseline Model (Simple Concatenation)\n",
    "\n",
    "First, let's try a baseline approach: just concatenate all features and train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuros.models import SimpleClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = SimpleClassifier()\n",
    "baseline_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"Baseline Model (Simple Concatenation)\")\n",
    "print(f\"Test Accuracy: {baseline_accuracy:.2%}\\n\")\n",
    "print(classification_report(y_test, y_pred_baseline, \n",
    "                          target_names=['Neutral', 'Happy', 'Stressed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Attention Fusion Model\n",
    "\n",
    "Now let's use the AttentionFusionModel which learns to weight each modality's contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention fusion model\n",
    "attention_model = AttentionFusionModel(\n",
    "    modality_dims=[40, 128, 12],  # EEG, Video, Motion\n",
    "    n_classes=3,\n",
    "    fusion_dim=64,\n",
    "    attention_type=\"learned\",\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "print(\"Training Attention Fusion Model...\\n\")\n",
    "attention_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_attention = attention_model.predict(X_test)\n",
    "attention_accuracy = accuracy_score(y_test, y_pred_attention)\n",
    "\n",
    "print(f\"\\nAttention Fusion Model\")\n",
    "print(f\"Test Accuracy: {attention_accuracy:.2%}\\n\")\n",
    "print(classification_report(y_test, y_pred_attention,\n",
    "                          target_names=['Neutral', 'Happy', 'Stressed']))\n",
    "\n",
    "# Compare with baseline\n",
    "improvement = (attention_accuracy - baseline_accuracy) * 100\n",
    "print(f\"\\nImprovement over baseline: {improvement:+.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpret Attention Weights\n",
    "\n",
    "One key advantage of attention-based fusion is interpretability. Let's see which modalities the model relies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights for test samples\n",
    "attention_weights = attention_model.get_attention_weights(X_test)\n",
    "\n",
    "# Average attention per class\n",
    "class_names = ['Neutral', 'Happy', 'Stressed']\n",
    "modality_names = ['EEG', 'Video', 'Motion']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for cls in range(3):\n",
    "    cls_mask = y_test == cls\n",
    "    cls_attention = attention_weights[cls_mask].mean(axis=0)\n",
    "    \n",
    "    axes[cls].bar(modality_names, cls_attention, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "    axes[cls].set_title(f\"{class_names[cls]}\\nAverage Attention Weights\")\n",
    "    axes[cls].set_ylabel('Attention Weight')\n",
    "    axes[cls].set_ylim(0, 1)\n",
    "    axes[cls].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(cls_attention):\n",
    "        axes[cls].text(i, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Higher weights indicate the model relies more on that modality\")\n",
    "print(\"- Different classes may show different modality preferences\")\n",
    "print(\"- This helps us understand what signals are most informative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Per-Sample Attention\n",
    "\n",
    "Let's look at how attention varies across individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for mod_idx, mod_name in enumerate(modality_names):\n",
    "    for cls in range(3):\n",
    "        cls_mask = y_test == cls\n",
    "        cls_attention = attention_weights[cls_mask, mod_idx]\n",
    "        axes[mod_idx].hist(cls_attention, alpha=0.5, label=class_names[cls], bins=15)\n",
    "    \n",
    "    axes[mod_idx].set_title(f\"{mod_name} Attention Distribution\")\n",
    "    axes[mod_idx].set_xlabel('Attention Weight')\n",
    "    axes[mod_idx].set_ylabel('Frequency')\n",
    "    axes[mod_idx].legend()\n",
    "    axes[mod_idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models to Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = ModelRegistry()\n",
    "\n",
    "# Save baseline model\n",
    "registry.save(\n",
    "    baseline_model,\n",
    "    name=\"emotion_baseline\",\n",
    "    version=\"1.0.0\",\n",
    "    metrics={\"accuracy\": float(baseline_accuracy)},\n",
    "    tags=[\"emotion\", \"baseline\", \"multimodal\"],\n",
    ")\n",
    "\n",
    "# Save attention model\n",
    "registry.save(\n",
    "    attention_model,\n",
    "    name=\"emotion_attention_fusion\",\n",
    "    version=\"1.0.0\",\n",
    "    metrics={\n",
    "        \"accuracy\": float(attention_accuracy),\n",
    "        \"improvement_over_baseline\": float(improvement),\n",
    "    },\n",
    "    hyperparameters={\n",
    "        \"fusion_dim\": 64,\n",
    "        \"attention_type\": \"learned\",\n",
    "        \"dropout\": 0.3,\n",
    "    },\n",
    "    tags=[\"emotion\", \"attention\", \"multimodal\", \"production\"],\n",
    ")\n",
    "\n",
    "print(\"✓ Models saved to registry\")\n",
    "\n",
    "# List all multimodal models\n",
    "multimodal_models = registry.search(tags=[\"multimodal\"])\n",
    "print(f\"\\nMulti-modal models in registry: {len(multimodal_models)}\")\n",
    "for m in multimodal_models:\n",
    "    print(f\"  {m.name} v{m.version}: {m.metrics.get('accuracy', 0):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-Time Multi-Modal Pipeline\n",
    "\n",
    "Demonstrate how to run a real-time multi-modal pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Create multi-modal pipeline\n",
    "# Note: In production, replace mock drivers with real hardware\n",
    "pipeline = MultiModalPipeline(\n",
    "    drivers=[\n",
    "        MockDriver(sampling_rate=250.0, channels=8),  # EEG\n",
    "        MockDriver(sampling_rate=30.0, channels=128), # Video (30 fps)\n",
    "        MockDriver(sampling_rate=100.0, channels=12), # Motion sensors\n",
    "    ],\n",
    "    model=attention_model,\n",
    ")\n",
    "\n",
    "print(\"Running multi-modal pipeline for 3 seconds...\")\n",
    "print(\"(Using mock drivers for demonstration)\\n\")\n",
    "\n",
    "metrics = await pipeline.run(duration=3.0)\n",
    "\n",
    "print(\"\\n✓ Multi-modal pipeline complete!\")\n",
    "print(f\"  Throughput: {metrics['throughput']:.1f} samples/sec\")\n",
    "print(f\"  Mean latency: {metrics['mean_latency']*1000:.2f} ms\")\n",
    "print(f\"  Total samples: {metrics['samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Modality Ablation Study\n",
    "\n",
    "Test what happens when we remove individual modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_results = {}\n",
    "\n",
    "# Test with individual modalities\n",
    "modality_data = {\n",
    "    'EEG only': X_eeg,\n",
    "    'Video only': X_video,\n",
    "    'Motion only': X_motion,\n",
    "}\n",
    "\n",
    "for name, X_modality in modality_data.items():\n",
    "    # Split data\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_modality, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = SimpleClassifier()\n",
    "    model.train(X_tr, y_tr)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_te)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    ablation_results[name] = acc\n",
    "\n",
    "# Add full model results\n",
    "ablation_results['All modalities (baseline)'] = baseline_accuracy\n",
    "ablation_results['All modalities (attention)'] = attention_accuracy\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(ablation_results.keys())\n",
    "accs = list(ablation_results.values())\n",
    "colors = ['#d62728', '#d62728', '#d62728', '#1f77b4', '#2ca02c']\n",
    "bars = plt.barh(names, accs, color=colors, alpha=0.7)\n",
    "\n",
    "# Highlight best result\n",
    "best_idx = np.argmax(accs)\n",
    "bars[best_idx].set_color('#2ca02c')\n",
    "bars[best_idx].set_alpha(1.0)\n",
    "\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Modality Ablation Study')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (name, acc) in enumerate(zip(names, accs)):\n",
    "    plt.text(acc + 0.01, i, f\"{acc:.2%}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"- Best single modality: {max(ablation_results.items(), key=lambda x: x[1] if 'only' in x[0] else 0)}\")\n",
    "print(f\"- Fusion improves over best single modality by: {(attention_accuracy - max([v for k, v in ablation_results.items() if 'only' in k]))*100:.1f} pp\")\n",
    "print(f\"- Attention fusion is best overall: {attention_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Now that you've mastered multi-modal fusion:\n",
    "\n",
    "1. **Add more modalities:** Try combining EEG + fNIRS + EOG + EMG\n",
    "2. **Experiment with attention types:** Try \"self\" attention for cross-modality interactions\n",
    "3. **Real-world data:** Apply to actual multi-modal BCI datasets\n",
    "4. **Online adaptation:** Update attention weights in real-time\n",
    "5. **Explainability:** Use attention weights to understand model decisions\n",
    "\n",
    "See other notebooks:\n",
    "- `03_advanced_attention.ipynb` - Cross-modal attention mechanisms\n",
    "- `04_real_time_fusion.ipynb` - Production deployment strategies\n",
    "- `05_interpretability.ipynb` - Understanding fusion decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
