{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Multi-Modal Processing\n",
    "\n",
    "Welcome to the multi-modal processing tutorial! In this notebook, you'll learn how to work with multiple data streams simultaneouslyâ€”a key capability for modern neuroscience research.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Understanding multi-modal data (EEG + video + physiological signals)\n",
    "2. Creating multi-modal pipelines\n",
    "3. Synchronizing data streams\n",
    "4. Fusion strategies for combining modalities\n",
    "5. Multi-modal classification\n",
    "6. Analyzing cross-modal relationships\n",
    "\n",
    "## Why Multi-Modal?\n",
    "\n",
    "Combining multiple data sources provides:\n",
    "- **Richer representations**: Capture complementary information\n",
    "- **Better accuracy**: Multiple evidence sources improve predictions\n",
    "- **Robustness**: Redundancy helps when one modality is noisy\n",
    "- **Deeper insights**: Understand cross-modal relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import asyncio\n",
    "\n",
    "# NeurOS imports\n",
    "from neuros.agents.multimodal_orchestrator import MultiModalOrchestrator\n",
    "from neuros.agents.fusion_agent import FusionAgent\n",
    "from neuros.drivers.mock_driver import MockDriver\n",
    "from neuros.drivers.video_driver import VideoDriver\n",
    "from neuros.drivers.gsr_driver import GSRDriver\n",
    "from neuros.models.simple_classifier import SimpleClassifier\n",
    "from neuros.datasets.allen_loader import load_simulated_allen_data\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Multi-Modal Data\n",
    "\n",
    "Let's simulate a multi-modal experiment where we're studying emotional responses:\n",
    "- **EEG**: Brain activity (64 channels, 250 Hz)\n",
    "- **Video**: Facial expressions (30 fps)\n",
    "- **GSR**: Galvanic skin response (physiological arousal, 10 Hz)\n",
    "\n",
    "**Task**: Classify emotional states (neutral, happy, sad) using all modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated multi-modal data\n",
    "n_samples = 150\n",
    "n_classes = 3  # neutral, happy, sad\n",
    "\n",
    "# EEG data: 64 channels, 5 time bins (250 Hz, 20ms window)\n",
    "eeg_data = load_simulated_allen_data(\n",
    "    n_samples=n_samples,\n",
    "    n_neurons=64,\n",
    "    n_classes=n_classes,\n",
    "    noise_level=0.3\n",
    ")\n",
    "X_eeg = eeg_data['features']\n",
    "y_labels = eeg_data['labels']\n",
    "\n",
    "# Video features: Facial action units (12 features)\n",
    "X_video = np.random.randn(n_samples, 12)\n",
    "# Add class-specific patterns\n",
    "for i in range(n_classes):\n",
    "    mask = (y_labels == i)\n",
    "    X_video[mask] += np.random.randn(12) * 2  # Class-specific shift\n",
    "\n",
    "# GSR features: Skin conductance level and response (2 features)\n",
    "X_gsr = np.random.randn(n_samples, 2)\n",
    "for i in range(n_classes):\n",
    "    mask = (y_labels == i)\n",
    "    X_gsr[mask, 0] += i * 1.5  # Higher arousal for certain emotions\n",
    "\n",
    "print(f\"EEG shape: {X_eeg.shape}\")\n",
    "print(f\"Video shape: {X_video.shape}\")\n",
    "print(f\"GSR shape: {X_gsr.shape}\")\n",
    "print(f\"Labels shape: {y_labels.shape}\")\n",
    "print(f\"\\nClass distribution: {np.bincount(y_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Multi-Modal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample from each modality for each class\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "class_names = ['Neutral', 'Happy', 'Sad']\n",
    "\n",
    "for class_idx in range(3):\n",
    "    # Get first sample of this class\n",
    "    sample_idx = np.where(y_labels == class_idx)[0][0]\n",
    "    \n",
    "    # EEG\n",
    "    ax = axes[class_idx, 0]\n",
    "    eeg_sample = X_eeg[sample_idx].reshape(64, -1)\n",
    "    im = ax.imshow(eeg_sample, aspect='auto', cmap='RdBu_r')\n",
    "    ax.set_title(f'{class_names[class_idx]} - EEG')\n",
    "    ax.set_ylabel('Channel')\n",
    "    ax.set_xlabel('Time')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Video (facial action units)\n",
    "    ax = axes[class_idx, 1]\n",
    "    ax.bar(range(12), X_video[sample_idx])\n",
    "    ax.set_title(f'{class_names[class_idx]} - Facial AUs')\n",
    "    ax.set_xlabel('Action Unit')\n",
    "    ax.set_ylabel('Activation')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # GSR\n",
    "    ax = axes[class_idx, 2]\n",
    "    ax.bar(['SCL', 'SCR'], X_gsr[sample_idx])\n",
    "    ax.set_title(f'{class_names[class_idx]} - GSR')\n",
    "    ax.set_ylabel('Conductance (Î¼S)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "indices = np.arange(n_samples)\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices, test_size=0.3, random_state=42, stratify=y_labels\n",
    ")\n",
    "\n",
    "X_eeg_train, X_eeg_test = X_eeg[train_idx], X_eeg[test_idx]\n",
    "X_video_train, X_video_test = X_video[train_idx], X_video[test_idx]\n",
    "X_gsr_train, X_gsr_test = X_gsr[train_idx], X_gsr[test_idx]\n",
    "y_train, y_test = y_labels[train_idx], y_labels[test_idx]\n",
    "\n",
    "print(f\"Training samples: {len(train_idx)}\")\n",
    "print(f\"Test samples: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Single-Modality Baselines\n",
    "\n",
    "First, let's see how well each modality performs individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuros.pipeline import Pipeline\n",
    "\n",
    "results = {}\n",
    "\n",
    "# EEG-only\n",
    "eeg_model = SimpleClassifier(model_type='svm')\n",
    "eeg_pipeline = Pipeline(driver=MockDriver(channels=64), model=eeg_model)\n",
    "eeg_pipeline.train(X_eeg_train, y_train)\n",
    "y_pred_eeg = eeg_pipeline.predict(X_eeg_test)\n",
    "results['EEG'] = accuracy_score(y_test, y_pred_eeg)\n",
    "\n",
    "# Video-only\n",
    "video_model = SimpleClassifier(model_type='random_forest')\n",
    "video_pipeline = Pipeline(driver=VideoDriver(), model=video_model)\n",
    "video_pipeline.train(X_video_train, y_train)\n",
    "y_pred_video = video_pipeline.predict(X_video_test)\n",
    "results['Video'] = accuracy_score(y_test, y_pred_video)\n",
    "\n",
    "# GSR-only\n",
    "gsr_model = SimpleClassifier(model_type='svm')\n",
    "gsr_pipeline = Pipeline(driver=GSRDriver(), model=gsr_model)\n",
    "gsr_pipeline.train(X_gsr_train, y_train)\n",
    "y_pred_gsr = gsr_pipeline.predict(X_gsr_test)\n",
    "results['GSR'] = accuracy_score(y_test, y_pred_gsr)\n",
    "\n",
    "# Display results\n",
    "print(\"Single-Modality Baselines:\")\n",
    "for modality, acc in results.items():\n",
    "    print(f\"  {modality:10s}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Modal Fusion Strategies\n",
    "\n",
    "### 6.1 Early Fusion (Feature-Level)\n",
    "\n",
    "Concatenate all features before classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all features\n",
    "X_train_early = np.concatenate([X_eeg_train, X_video_train, X_gsr_train], axis=1)\n",
    "X_test_early = np.concatenate([X_eeg_test, X_video_test, X_gsr_test], axis=1)\n",
    "\n",
    "print(f\"Early fusion feature shape: {X_train_early.shape}\")\n",
    "\n",
    "# Train on fused features\n",
    "early_fusion_model = SimpleClassifier(model_type='svm')\n",
    "early_pipeline = Pipeline(driver=MockDriver(), model=early_fusion_model)\n",
    "early_pipeline.train(X_train_early, y_train)\n",
    "y_pred_early = early_pipeline.predict(X_test_early)\n",
    "results['Early Fusion'] = accuracy_score(y_test, y_pred_early)\n",
    "\n",
    "print(f\"\\nEarly Fusion Accuracy: {results['Early Fusion']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Late Fusion (Decision-Level)\n",
    "\n",
    "Train separate models and combine predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions from each modality\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Retrain with probability predictions\n",
    "eeg_clf = SVC(probability=True, random_state=42)\n",
    "video_clf = RandomForestClassifier(random_state=42)\n",
    "gsr_clf = SVC(probability=True, random_state=42)\n",
    "\n",
    "eeg_clf.fit(X_eeg_train, y_train)\n",
    "video_clf.fit(X_video_train, y_train)\n",
    "gsr_clf.fit(X_gsr_train, y_train)\n",
    "\n",
    "# Get probability predictions\n",
    "proba_eeg = eeg_clf.predict_proba(X_eeg_test)\n",
    "proba_video = video_clf.predict_proba(X_video_test)\n",
    "proba_gsr = gsr_clf.predict_proba(X_gsr_test)\n",
    "\n",
    "# Average probabilities (simple voting)\n",
    "proba_late = (proba_eeg + proba_video + proba_gsr) / 3\n",
    "y_pred_late = np.argmax(proba_late, axis=1)\n",
    "results['Late Fusion (Voting)'] = accuracy_score(y_test, y_pred_late)\n",
    "\n",
    "print(f\"Late Fusion Accuracy: {results['Late Fusion (Voting)']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Weighted Fusion\n",
    "\n",
    "Weight each modality by its individual performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights based on validation accuracy\n",
    "weights = np.array([\n",
    "    results['EEG'],\n",
    "    results['Video'],\n",
    "    results['GSR']\n",
    "])\n",
    "weights = weights / weights.sum()  # Normalize\n",
    "\n",
    "print(f\"Modality weights: EEG={weights[0]:.3f}, Video={weights[1]:.3f}, GSR={weights[2]:.3f}\")\n",
    "\n",
    "# Weighted average of probabilities\n",
    "proba_weighted = (\n",
    "    weights[0] * proba_eeg +\n",
    "    weights[1] * proba_video +\n",
    "    weights[2] * proba_gsr\n",
    ")\n",
    "y_pred_weighted = np.argmax(proba_weighted, axis=1)\n",
    "results['Weighted Fusion'] = accuracy_score(y_test, y_pred_weighted)\n",
    "\n",
    "print(f\"\\nWeighted Fusion Accuracy: {results['Weighted Fusion']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare All Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "methods = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "bars = ax.bar(methods, accuracies, color=colors)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1%}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Multi-Modal Fusion Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.axhline(y=1/3, color='r', linestyle='--', label='Chance (33.3%)', alpha=0.5)\n",
    "ax.legend()\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Summary:\")\n",
    "print(f\"Best single modality: {max(results['EEG'], results['Video'], results['GSR']):.2%}\")\n",
    "print(f\"Best fusion method: {max(results['Early Fusion'], results['Late Fusion (Voting)'], results['Weighted Fusion']):.2%}\")\n",
    "improvement = max(results['Early Fusion'], results['Late Fusion (Voting)'], results['Weighted Fusion']) - max(results['EEG'], results['Video'], results['GSR'])\n",
    "print(f\"Improvement from fusion: +{improvement:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using NeurOS FusionAgent\n",
    "\n",
    "NeurOS provides a `FusionAgent` for sophisticated multi-modal processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuros.agents.fusion_agent import FusionAgent\n",
    "\n",
    "# Create fusion agent\n",
    "fusion_agent = FusionAgent(\n",
    "    modalities=['eeg', 'video', 'gsr'],\n",
    "    fusion_strategy='weighted'\n",
    ")\n",
    "\n",
    "# Prepare data dictionary\n",
    "train_data = {\n",
    "    'eeg': X_eeg_train,\n",
    "    'video': X_video_train,\n",
    "    'gsr': X_gsr_train\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    'eeg': X_eeg_test,\n",
    "    'video': X_video_test,\n",
    "    'gsr': X_gsr_test\n",
    "}\n",
    "\n",
    "# Train fusion agent\n",
    "fusion_agent.train(train_data, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_agent = fusion_agent.predict(test_data)\n",
    "agent_accuracy = accuracy_score(y_test, y_pred_agent)\n",
    "\n",
    "print(f\"FusionAgent Accuracy: {agent_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Cross-Modal Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Compute feature-level correlations between modalities\n",
    "# Use average features per sample\n",
    "eeg_avg = X_eeg_train.mean(axis=1)\n",
    "video_avg = X_video_train.mean(axis=1)\n",
    "gsr_avg = X_gsr_train.mean(axis=1)\n",
    "\n",
    "# Compute correlations\n",
    "corr_eeg_video, p1 = pearsonr(eeg_avg, video_avg)\n",
    "corr_eeg_gsr, p2 = pearsonr(eeg_avg, gsr_avg)\n",
    "corr_video_gsr, p3 = pearsonr(video_avg, gsr_avg)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = np.array([\n",
    "    [1.0, corr_eeg_video, corr_eeg_gsr],\n",
    "    [corr_eeg_video, 1.0, corr_video_gsr],\n",
    "    [corr_eeg_gsr, corr_video_gsr, 1.0]\n",
    "])\n",
    "\n",
    "# Plot correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', \n",
    "            xticklabels=['EEG', 'Video', 'GSR'],\n",
    "            yticklabels=['EEG', 'Video', 'GSR'],\n",
    "            cmap='coolwarm', center=0, vmin=-1, vmax=1,\n",
    "            square=True, ax=ax)\n",
    "ax.set_title('Cross-Modal Correlations', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”— Cross-Modal Correlations:\")\n",
    "print(f\"  EEG â†” Video: r={corr_eeg_video:.3f} (p={p1:.4f})\")\n",
    "print(f\"  EEG â†” GSR:   r={corr_eeg_gsr:.3f} (p={p2:.4f})\")\n",
    "print(f\"  Video â†” GSR: r={corr_video_gsr:.3f} (p={p3:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Multi-Modal Orchestrator for Real-Time\n",
    "\n",
    "For real-time multi-modal processing, use the `MultiModalOrchestrator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-modal orchestrator\n",
    "orchestrator = MultiModalOrchestrator(\n",
    "    modalities=['eeg', 'video', 'gsr'],\n",
    "    sync_window_ms=100  # 100ms synchronization window\n",
    ")\n",
    "\n",
    "# Register drivers\n",
    "orchestrator.register_driver('eeg', MockDriver(channels=64))\n",
    "orchestrator.register_driver('video', VideoDriver())\n",
    "orchestrator.register_driver('gsr', GSRDriver())\n",
    "\n",
    "# Register fusion agent\n",
    "orchestrator.register_fusion(fusion_agent)\n",
    "\n",
    "print(\"âœ“ Multi-modal orchestrator configured\")\n",
    "print(f\"  Modalities: {orchestrator.modalities}\")\n",
    "print(f\"  Sync window: {orchestrator.sync_window_ms}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned multi-modal processing with NeurOS:\n",
    "\n",
    "âœ… Understanding multi-modal data types  \n",
    "âœ… Training single-modality baselines  \n",
    "âœ… Early fusion (feature-level)  \n",
    "âœ… Late fusion (decision-level)  \n",
    "âœ… Weighted fusion strategies  \n",
    "âœ… Using NeurOS FusionAgent  \n",
    "âœ… Analyzing cross-modal correlations  \n",
    "âœ… Real-time multi-modal orchestration  \n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Multi-modal fusion improves accuracy** - Combining modalities typically beats any single modality\n",
    "2. **Different fusion strategies work for different tasks** - Try early, late, and weighted fusion\n",
    "3. **Modality weighting matters** - Weight by individual performance for best results\n",
    "4. **Cross-modal correlations provide insights** - Understand relationships between data streams\n",
    "5. **Synchronization is critical** - Align timestamps across modalities for real-time\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Tutorial 4**: Custom Models - Build your own neural decoders\n",
    "- **Tutorial 5**: Benchmarking - Compare methods systematically\n",
    "- **Advanced**: Attention-based fusion, modal dropout, hierarchical processing\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Try different fusion strategies (max pooling, attention mechanisms)\n",
    "2. Add a fourth modality (e.g., heart rate)\n",
    "3. Implement modal dropout for robustness testing\n",
    "4. Analyze which modalities contribute most to each class\n",
    "5. Build a hierarchical fusion model (early + late fusion combined)\n",
    "\n",
    "Happy multi-modal processing! ðŸ§ ðŸŽ¥ðŸ“Š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
