{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Building Custom Models and Decoders\n",
    "\n",
    "**Level**: Intermediate  \n",
    "**Time**: 30-40 minutes  \n",
    "**Prerequisites**: Tutorial 1, Tutorial 2\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, you'll learn how to:\n",
    "\n",
    "1. **Extend BaseModel** - Create custom model classes\n",
    "2. **Implement Training Logic** - Custom training loops and optimization\n",
    "3. **Build Neural Decoders** - From scratch using PyTorch\n",
    "4. **Integrate with Pipelines** - Use custom models in neurOS pipelines\n",
    "5. **Save & Load Models** - Model persistence and versioning\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **BaseModel Interface**: neurOS's model abstraction\n",
    "- **Custom Architectures**: Building domain-specific models\n",
    "- **Pipeline Integration**: Seamless model swapping\n",
    "- **Hyperparameter Tuning**: Systematic optimization\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding the BaseModel Interface\n",
    "\n",
    "All neurOS models extend `BaseModel`, which provides a consistent interface for training, prediction, and serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "from neuros.models import BaseModel\n",
    "from neuros.drivers import MockDriver\n",
    "from neuros.pipeline import Pipeline\n",
    "\n",
    "# Display the BaseModel interface\n",
    "print(\"BaseModel Methods:\")\n",
    "for method in dir(BaseModel):\n",
    "    if not method.startswith('_'):\n",
    "        print(f\"  - {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Methods to Implement\n",
    "\n",
    "When creating a custom model, you must implement:\n",
    "\n",
    "1. **`train(X, y, **kwargs)`** - Training logic\n",
    "2. **`predict(X, **kwargs)`** - Inference logic\n",
    "3. **`save(path)`** - Serialization\n",
    "4. **`load(path)`** - Deserialization (class method)\n",
    "\n",
    "Optional methods:\n",
    "- **`predict_proba(X)`** - Probability estimates\n",
    "- **`score(X, y)`** - Evaluation metric\n",
    "- **`get_params()`** - Hyperparameters\n",
    "- **`set_params(**params)`** - Update hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Example 1 - Simple Threshold Decoder\n",
    "\n",
    "Let's start with a simple threshold-based decoder for motor imagery classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdDecoder(BaseModel):\n",
    "    \"\"\"\n",
    "    Simple threshold-based decoder.\n",
    "    \n",
    "    Classifies based on whether the mean signal amplitude\n",
    "    across specified channels exceeds a learned threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: Optional[list] = None):\n",
    "        super().__init__()\n",
    "        self.channels = channels  # Which channels to use\n",
    "        self.threshold = None  # Learned threshold\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray, **kwargs) -> 'ThresholdDecoder':\n",
    "        \"\"\"\n",
    "        Learn optimal threshold from training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Training features\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Training labels (0 or 1)\n",
    "        \"\"\"\n",
    "        # Use all features if channels not specified\n",
    "        if self.channels is None:\n",
    "            self.channels = list(range(X.shape[1]))\n",
    "        \n",
    "        # Extract relevant features\n",
    "        X_subset = X[:, self.channels]\n",
    "        \n",
    "        # Compute mean amplitude for each sample\n",
    "        amplitudes = np.mean(np.abs(X_subset), axis=1)\n",
    "        \n",
    "        # Find threshold that best separates classes\n",
    "        # Simple approach: midpoint between class means\n",
    "        class_0_mean = np.mean(amplitudes[y == 0])\n",
    "        class_1_mean = np.mean(amplitudes[y == 1])\n",
    "        self.threshold = (class_0_mean + class_1_mean) / 2\n",
    "        \n",
    "        self.is_trained = True\n",
    "        \n",
    "        print(f\"Threshold learned: {self.threshold:.4f}\")\n",
    "        print(f\"  Class 0 mean: {class_0_mean:.4f}\")\n",
    "        print(f\"  Class 1 mean: {class_1_mean:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        \n",
    "        # Extract relevant features\n",
    "        X_subset = X[:, self.channels]\n",
    "        \n",
    "        # Compute amplitudes\n",
    "        amplitudes = np.mean(np.abs(X_subset), axis=1)\n",
    "        \n",
    "        # Threshold classification\n",
    "        predictions = (amplitudes > self.threshold).astype(int)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save model to disk.\"\"\"\n",
    "        import pickle\n",
    "        model_data = {\n",
    "            'channels': self.channels,\n",
    "            'threshold': self.threshold,\n",
    "            'is_trained': self.is_trained\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'ThresholdDecoder':\n",
    "        \"\"\"Load model from disk.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model = cls(channels=model_data['channels'])\n",
    "        model.threshold = model_data['threshold']\n",
    "        model.is_trained = model_data['is_trained']\n",
    "        return model\n",
    "\n",
    "print(\"✓ ThresholdDecoder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Threshold Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "n_features = 10\n",
    "\n",
    "# Class 0: Low amplitude\n",
    "X_class0 = np.random.randn(n_samples // 2, n_features) * 0.5\n",
    "y_class0 = np.zeros(n_samples // 2)\n",
    "\n",
    "# Class 1: High amplitude\n",
    "X_class1 = np.random.randn(n_samples // 2, n_features) * 1.5 + 2.0\n",
    "y_class1 = np.ones(n_samples // 2)\n",
    "\n",
    "# Combine\n",
    "X = np.vstack([X_class0, X_class1])\n",
    "y = np.hstack([y_class0, y_class1])\n",
    "\n",
    "# Shuffle\n",
    "indices = np.random.permutation(n_samples)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Split train/test\n",
    "split = int(0.7 * n_samples)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Train model\n",
    "model = ThresholdDecoder()\n",
    "model.train(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\n✓ Test Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Example 2 - Custom PyTorch Decoder\n",
    "\n",
    "Now let's build a more sophisticated decoder using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class CustomNeuralDecoder(BaseModel):\n",
    "    \"\"\"\n",
    "    Custom neural network decoder with flexible architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: list = [64, 32],\n",
    "        n_classes: int = 2,\n",
    "        dropout: float = 0.3,\n",
    "        learning_rate: float = 0.001\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Build network\n",
    "        self.network = self._build_network()\n",
    "        self.optimizer = None\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def _build_network(self) -> nn.Module:\n",
    "        \"\"\"Build the neural network architecture.\"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        prev_dim = self.input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in self.hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.Dropout(self.dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, self.n_classes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        epochs: int = 50,\n",
    "        batch_size: int = 32,\n",
    "        validation_split: float = 0.2,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> 'CustomNeuralDecoder':\n",
    "        \"\"\"\n",
    "        Train the neural decoder.\n",
    "        \"\"\"\n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "        \n",
    "        # Create validation split\n",
    "        n_val = int(len(X) * validation_split)\n",
    "        indices = torch.randperm(len(X))\n",
    "        \n",
    "        val_indices = indices[:n_val]\n",
    "        train_indices = indices[n_val:]\n",
    "        \n",
    "        X_train, y_train = X_tensor[train_indices], y_tensor[train_indices]\n",
    "        X_val, y_val = X_tensor[val_indices], y_tensor[val_indices]\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training history\n",
    "        history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.network.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.network(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            self.network.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = self.network(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val).item()\n",
    "                val_pred = torch.argmax(val_outputs, dim=1)\n",
    "                val_acc = (val_pred == y_val).float().mean().item()\n",
    "            \n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, \"\n",
    "                      f\"Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        self.training_history = history\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        \n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X)\n",
    "            outputs = self.network(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1).numpy()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        \n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X)\n",
    "            outputs = self.network(X_tensor)\n",
    "            probas = torch.softmax(outputs, dim=1).numpy()\n",
    "        \n",
    "        return probas\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save model to disk.\"\"\"\n",
    "        torch.save({\n",
    "            'network_state': self.network.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict() if self.optimizer else None,\n",
    "            'config': {\n",
    "                'input_dim': self.input_dim,\n",
    "                'hidden_dims': self.hidden_dims,\n",
    "                'n_classes': self.n_classes,\n",
    "                'dropout': self.dropout,\n",
    "                'learning_rate': self.learning_rate\n",
    "            },\n",
    "            'is_trained': self.is_trained\n",
    "        }, path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'CustomNeuralDecoder':\n",
    "        \"\"\"Load model from disk.\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        config = checkpoint['config']\n",
    "        \n",
    "        model = cls(**config)\n",
    "        model.network.load_state_dict(checkpoint['network_state'])\n",
    "        \n",
    "        if checkpoint['optimizer_state']:\n",
    "            model.optimizer = optim.Adam(model.network.parameters())\n",
    "            model.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        \n",
    "        model.is_trained = checkpoint['is_trained']\n",
    "        return model\n",
    "\n",
    "print(\"✓ CustomNeuralDecoder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Neural Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "neural_model = CustomNeuralDecoder(\n",
    "    input_dim=n_features,\n",
    "    hidden_dims=[32, 16],\n",
    "    n_classes=2,\n",
    "    dropout=0.2,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Training neural decoder...\\n\")\n",
    "neural_model.train(X_train, y_train, epochs=50, batch_size=16, verbose=True)\n",
    "\n",
    "# Predict\n",
    "y_pred_neural = neural_model.predict(X_test)\n",
    "y_proba_neural = neural_model.predict_proba(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_neural = np.mean(y_pred_neural == y_test)\n",
    "print(f\"\\n✓ Neural Decoder Test Accuracy: {accuracy_neural:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(neural_model.training_history['train_loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(neural_model.training_history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(neural_model.training_history['val_acc'], label='Val Accuracy', linewidth=2, color='green')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Pipeline Integration\n",
    "\n",
    "Let's integrate our custom models into neurOS pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with custom model\n",
    "driver = MockDriver(n_channels=n_features, sampling_rate=250)\n",
    "\n",
    "# Option 1: Use threshold decoder\n",
    "pipeline_threshold = Pipeline(driver=driver, model=model)\n",
    "\n",
    "# Option 2: Use neural decoder\n",
    "pipeline_neural = Pipeline(driver=driver, model=neural_model)\n",
    "\n",
    "print(\"✓ Pipelines created with custom models\")\n",
    "print(f\"  - Threshold Decoder Pipeline: {pipeline_threshold}\")\n",
    "print(f\"  - Neural Decoder Pipeline: {pipeline_neural}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "models = {\n",
    "    'Threshold Decoder': model,\n",
    "    'Neural Decoder': neural_model\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, mdl in models.items():\n",
    "    y_pred = mdl.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    results[name] = accuracy\n",
    "    print(f\"{name}: {accuracy:.2%} accuracy\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(results.keys(), results.values(), color=['steelblue', 'coral'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Comparison')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2%}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Model Persistence\n",
    "\n",
    "Save and load models for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create temporary directory for models\n",
    "model_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Save models\n",
    "threshold_path = os.path.join(model_dir, 'threshold_decoder.pkl')\n",
    "neural_path = os.path.join(model_dir, 'neural_decoder.pth')\n",
    "\n",
    "model.save(threshold_path)\n",
    "neural_model.save(neural_path)\n",
    "\n",
    "print(f\"✓ Models saved to {model_dir}\")\n",
    "print(f\"  - Threshold: {threshold_path}\")\n",
    "print(f\"  - Neural: {neural_path}\")\n",
    "\n",
    "# Load models\n",
    "loaded_threshold = ThresholdDecoder.load(threshold_path)\n",
    "loaded_neural = CustomNeuralDecoder.load(neural_path)\n",
    "\n",
    "# Verify loaded models work\n",
    "y_pred_loaded_threshold = loaded_threshold.predict(X_test)\n",
    "y_pred_loaded_neural = loaded_neural.predict(X_test)\n",
    "\n",
    "# Check they produce same results\n",
    "assert np.array_equal(y_pred, y_pred_loaded_threshold), \"Threshold model changed after loading!\"\n",
    "assert np.array_equal(y_pred_neural, y_pred_loaded_neural), \"Neural model changed after loading!\"\n",
    "\n",
    "print(\"\\n✓ Models loaded successfully and produce identical predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Advanced Example - Time-Series Decoder\n",
    "\n",
    "Build a decoder that handles sequential/temporal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDecoder(BaseModel):\n",
    "    \"\"\"\n",
    "    LSTM-based decoder for temporal sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        n_classes: int = 2,\n",
    "        n_layers: int = 2,\n",
    "        dropout: float = 0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Build LSTM network\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, n_classes)\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        epochs: int = 30,\n",
    "        batch_size: int = 16,\n",
    "        learning_rate: float = 0.001,\n",
    "        **kwargs\n",
    "    ) -> 'TemporalDecoder':\n",
    "        \"\"\"\n",
    "        Train the temporal decoder.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, sequence_length, n_features)\n",
    "            Sequential input data\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Labels\n",
    "        \"\"\"\n",
    "        # Ensure 3D input\n",
    "        if X.ndim == 2:\n",
    "            X = X[:, np.newaxis, :]  # Add sequence dimension\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.LongTensor(y)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                lstm_out, _ = self.lstm(batch_X)\n",
    "                # Use last time step\n",
    "                out = self.fc(lstm_out[:, -1, :])\n",
    "                \n",
    "                loss = criterion(out, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                avg_loss = total_loss / len(loader)\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \"\"\"Predict on sequential data.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        if X.ndim == 2:\n",
    "            X = X[:, np.newaxis, :]\n",
    "        \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X)\n",
    "            lstm_out, _ = self.lstm(X_tensor)\n",
    "            out = self.fc(lstm_out[:, -1, :])\n",
    "            predictions = torch.argmax(out, dim=1).numpy()\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Return model parameters.\"\"\"\n",
    "        return list(self.lstm.parameters()) + list(self.fc.parameters())\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save model.\"\"\"\n",
    "        torch.save({\n",
    "            'lstm_state': self.lstm.state_dict(),\n",
    "            'fc_state': self.fc.state_dict(),\n",
    "            'config': {\n",
    "                'input_dim': self.input_dim,\n",
    "                'hidden_dim': self.hidden_dim,\n",
    "                'n_classes': self.n_classes,\n",
    "                'n_layers': self.n_layers,\n",
    "                'dropout': self.dropout\n",
    "            }\n",
    "        }, path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'TemporalDecoder':\n",
    "        \"\"\"Load model.\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        model = cls(**checkpoint['config'])\n",
    "        model.lstm.load_state_dict(checkpoint['lstm_state'])\n",
    "        model.fc.load_state_dict(checkpoint['fc_state'])\n",
    "        model.is_trained = True\n",
    "        return model\n",
    "\n",
    "print(\"✓ TemporalDecoder class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "✅ **BaseModel Interface** - The foundation for all neurOS models  \n",
    "✅ **Custom Decoders** - Built threshold and neural decoders from scratch  \n",
    "✅ **PyTorch Integration** - Created sophisticated neural architectures  \n",
    "✅ **Pipeline Integration** - Seamlessly swapped models in pipelines  \n",
    "✅ **Model Persistence** - Saved and loaded models  \n",
    "✅ **Temporal Models** - Handled sequential data with LSTMs  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Tutorial 5**: Benchmarking & Performance Optimization\n",
    "- **Tutorial 6**: Real-World NWB Data Integration\n",
    "- **Advanced**: Hyperparameter tuning with Optuna\n",
    "- **Advanced**: Multi-modal fusion architectures\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Add regularization** - Implement L1/L2 regularization in CustomNeuralDecoder\n",
    "2. **Early stopping** - Add early stopping to prevent overfitting\n",
    "3. **Ensemble models** - Combine multiple decoders\n",
    "4. **Custom loss functions** - Implement focal loss or class-weighted loss\n",
    "5. **Attention mechanism** - Add attention to TemporalDecoder\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or feedback?** Open an issue on GitHub or check the docs at https://neuros.readthedocs.io\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
