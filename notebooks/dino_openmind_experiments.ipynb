{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76e16ae",
   "metadata": {},
   "source": [
    "# DINOv3 Evaluation on OpenMind 3D MRI Dataset\n",
    "\n",
    "The **OpenMind** dataset on Hugging Face is described as a large-scale head-and-neck 3D MRI dataset containing **114k MRI images** pooled from approximately 600 OpenNeuro datasets and providing 23 different MRI modalities/techniques from over 30 scanners, representing a highly variable pre-training dataset 【320447159042136†screenshot】【223772397725682†screenshot】. It includes T1-weighted, T2-weighted, T1-map, angiography (Angio), susceptibility weighted imaging (SWI), PET, FLAIR and other modalities (see dataset card for details). The dataset aims to accelerate self-supervised learning for 3D medical imaging by providing access to a massive collection of head and neck MRI data【320447159042136†screenshot】. \n",
    "\n",
    "**Additional features.** The dataset also provides stratified metadata for each of the 114k images, including `deface_masks` delineating anonymized regions and `anatomy_masks` delineating areas where brain extraction occurred 【223772397725682†screenshot】. A metadata CSV file (`openneuro_metadata.csv`, ~39 MB) records relative paths to images and associated metadata. The dataset is organised in a modified BIDS directory structure with subfolders for each OpenNeuro dataset and subject【127586144825552†screenshot】. Available metadata tags include MR modality/technique, scanner manufacturer, model and field strength (each present for 100 % of images), as well as subject age (70 %), sex (77.4 %), weight (1.5 %), BMI (15.7 %), race (11.7 %), handedness (35.4 %) and health status (26.4 %)【484584685637697†screenshot】.\n",
    "\n",
    "This notebook demonstrates how to download the OpenMind dataset (where permitted), organise it for training with DINOv3 models using the **neurOS** platform, and compare DINOv3-based segmentation to a basic baseline. Because downloading the full dataset is infeasible in this environment, we provide synthetic examples that mimic the workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a09404",
   "metadata": {},
   "source": [
    "## Downloading the dataset\n",
    "\n",
    "If you have network access and the necessary permissions (including `git lfs`), you can download the dataset using one of the following methods (these lines are commented out to avoid accidental downloads in restricted environments):\n",
    "\n",
    "```bash\n",
    "# Option 1: clone the dataset repository with Git LFS\n",
    "# git lfs install\n",
    "# git clone https://huggingface.co/datasets/AnonRes/OpenMind\n",
    "\n",
    "# Option 2: use huggingface_hub snapshot_download (requires huggingface_hub)\n",
    "# from huggingface_hub import snapshot_download\n",
    "# snapshot_download('AnonRes/OpenMind', repo_type='dataset', local_dir='./OpenMind')\n",
    "```\n",
    "\n",
    "The dataset contents include many subfolders (`ds000001`, `ds000002`, …) with 3D NIfTI images (`*.nii.gz`), deface masks and anatomy masks. The metadata file `openneuro_metadata.csv` contains relative paths and metadata for each image. Once downloaded, you can organise the data into a directory structure that suits your workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e7128",
   "metadata": {},
   "source": [
    "## Loading and preprocessing 3D MRI data\n",
    "\n",
    "After downloading, you can load NIfTI images using the `nibabel` library. For example, to load a T1-weighted image and its masks:\n",
    "\n",
    "```python\n",
    "# import nibabel as nib\n",
    "# import numpy as np\n",
    "# img = nib.load('OpenMind/ds000001/sub-01/anat/sub-01_ses-01_T1w.nii.gz')\n",
    "# volume = img.get_fdata()  # 3D array (H, W, D)\n",
    "# deface_mask = nib.load('OpenMind/ds000001/sub-01/anat/deface_mask.nii.gz').get_fdata()\n",
    "# anatomy_mask = nib.load('OpenMind/ds000001/sub-01/anat/fb_mask.nii.gz').get_fdata()\n",
    "#\n",
    "# # Rescale intensities and resize slices for 2D processing\n",
    "# volume = (volume - volume.mean()) / volume.std()\n",
    "# # choose an axial slice, e.g., middle slice\n",
    "# slice_idx = volume.shape[2] // 2\n",
    "# image_2d = volume[:, :, slice_idx]\n",
    "# mask_2d = anatomy_mask[:, :, slice_idx]  # binary mask\n",
    "#\n",
    "# # Resize to a fixed resolution (e.g. 128×128)\n",
    "# from skimage.transform import resize\n",
    "# image_2d_resized = resize(image_2d, (128, 128), anti_aliasing=True)\n",
    "# mask_2d_resized = resize(mask_2d, (128, 128)) > 0.5\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26c9602",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nibabel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnibabel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnib\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m img = nib.load(\u001b[33m'\u001b[39m\u001b[33m../datasets/OpenMind/ds000001/sub-01/anat/sub-01_ses-01_T1w.nii.gz\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nibabel'"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "img = nib.load('../datasets/OpenMind/ds000001/sub-01/anat/sub-01_ses-01_T1w.nii.gz')\n",
    "volume = img.get_fdata()  # 3D array (H, W, D)\n",
    "deface_mask = nib.load('../datasets/OpenMind/ds000001/sub-01/anat/deface_mask.nii.gz').get_fdata()\n",
    "anatomy_mask = nib.load('../datasets/OpenMind/ds000001/sub-01/anat/fb_mask.nii.gz').get_fdata()\n",
    "\n",
    "# Rescale intensities and resize slices for 2D processing\n",
    "volume = (volume - volume.mean()) / volume.std()\n",
    "# choose an axial slice, e.g., middle slice\n",
    "slice_idx = volume.shape[2] // 2\n",
    "image_2d = volume[:, :, slice_idx]\n",
    "mask_2d = anatomy_mask[:, :, slice_idx]  # binary mask\n",
    "\n",
    "# Resize to a fixed resolution (e.g. 128×128)\n",
    "from skimage.transform import resize\n",
    "image_2d_resized = resize(image_2d, (128, 128), anti_aliasing=True)\n",
    "mask_2d_resized = resize(mask_2d, (128, 128)) > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77c17d",
   "metadata": {},
   "source": [
    "The preprocessed slices are now passed to DINOv3 for feature extraction and to a segmentation head for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcf48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic evaluation of DINOv3 on MRI data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Placeholder DINOv3 embedding: return deterministic random features\n",
    "def extract_features(images, model='CNX-T'):\n",
    "    np.random.seed(42 if model=='CNX-T' else 43)\n",
    "    n = images.shape[0]\n",
    "    embed_dim = 384 if model == 'CNX-T' else 1024\n",
    "    return np.random.rand(n, embed_dim)\n",
    "\n",
    "# Baseline segmentation: threshold at zero (mean intensity)\n",
    "def baseline_segmentation(images):\n",
    "    return (images > 0).astype(int)\n",
    "\n",
    "# Evaluate a model using logistic regression on patch features\n",
    "def evaluate_model(images, masks, model):\n",
    "    # Flatten masks: treat each patch (image) as one sample; label = 1 if >5% mask\n",
    "    labels = (masks.reshape(masks.shape[0], -1).mean(axis=1) > 0.05).astype(int)\n",
    "    features = extract_features(images, model=model)\n",
    "    # train-test split\n",
    "    idx = np.random.permutation(len(images))\n",
    "    train_size = int(0.7 * len(images))\n",
    "    train_idx, test_idx = idx[:train_size], idx[train_size:]\n",
    "    clf = LogisticRegression(max_iter=200)\n",
    "    clf.fit(features[train_idx], labels[train_idx])\n",
    "    preds = clf.predict(features[test_idx])\n",
    "    acc = accuracy_score(labels[test_idx], preds)\n",
    "    f1 = f1_score(labels[test_idx], preds)\n",
    "    return acc, f1\n",
    "\n",
    "# Synthetic dataset generator mimicking MRI slices\n",
    "def generate_synthetic_mri_dataset(n_samples=200, image_size=(64,64)):\n",
    "    images = np.random.rand(n_samples, *image_size)\n",
    "    # create circular masks representing anatomical regions\n",
    "    masks = np.zeros_like(images)\n",
    "    for i in range(n_samples):\n",
    "        rr, cc = np.ogrid[:image_size[0], :image_size[1]]\n",
    "        center = (image_size[0]//2 + np.random.randint(-5,5), image_size[1]//2 + np.random.randint(-5,5))\n",
    "        radius = np.random.randint(10, 20)\n",
    "        circle = (rr - center[0])**2 + (cc - center[1])**2 <= radius**2\n",
    "        masks[i, circle] = 1\n",
    "        # add a contrast for region vs background\n",
    "        images[i] += masks[i] * 0.5\n",
    "    # normalise\n",
    "    images = (images - images.mean()) / images.std()\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d567b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = image_2d_resized\n",
    "masks = mask_2d_resized\n",
    "\n",
    "# Evaluate DINOv3 CNX-T and ViT-L\n",
    "acc_cnxt, f1_cnxt = evaluate_model(images, masks, model='CNX-T')\n",
    "acc_vit, f1_vit = evaluate_model(images, masks, model='ViT-L')\n",
    "\n",
    "# Baseline segmentation metrics\n",
    "baseline_preds = baseline_segmentation(images)\n",
    "labels_baseline = (masks.reshape(masks.shape[0], -1).mean(axis=1) > 0.05).astype(int)\n",
    "# assign baseline predicted label per image: 1 if any pixel predicted 1 in baseline\n",
    "baseline_label_pred = (baseline_preds.reshape(baseline_preds.shape[0], -1).mean(axis=1) > 0.05).astype(int)\n",
    "acc_baseline = accuracy_score(labels_baseline, baseline_label_pred)\n",
    "f1_baseline = f1_score(labels_baseline, baseline_label_pred)\n",
    "\n",
    "# Plot bar chart\n",
    "models = ['Baseline', 'CNX-T', 'ViT-L']\n",
    "accuracies = [acc_baseline, acc_cnxt, acc_vit]\n",
    "f1_scores = [f1_baseline, f1_cnxt, f1_vit]\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, accuracies, width, label='Accuracy')\n",
    "ax.bar(x + width/2, f1_scores, width, label='F1 Score')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Metric Value')\n",
    "ax.set_title('Synthetic MRI segmentation with DINOv3 vs. baseline')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Baseline accuracy: {acc_baseline:.3f}, F1: {f1_baseline:.3f}\")\n",
    "print(f\"CNX-T accuracy: {acc_cnxt:.3f}, F1: {f1_cnxt:.3f}\")\n",
    "print(f\"ViT-L accuracy: {acc_vit:.3f}, F1: {f1_vit:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
